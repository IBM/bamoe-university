{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IBM Business Automation Open Edition 9.2.x Enablement Your Journey Begins Here Get the tools First Kogito Project Setup Labs at IBM TechXChange 2024 Guided exercises for TechXchange Session 1108 TechXchange 2023 Guided exercises for TechXchange Session 1124 IBM Business Automation Open Edition 9.2.x Decision Manager Exercises Learn DMN Implement a On-push CI/CD IBM Business Automation Open Edition 8.0 Process Automation Exercises Getting started with Proces Event-driven processes More topics Extra Learning BAMOE on OpenShift About the guides Getting Started with IBAMOE: Order Management A great guide for users who are trying BAMOE for the first time. Recommended getting started guide. Learn DMN In this hands-on workshop, users can learn about the DMN specification, author decisions, and deploy it using IBM Decision Manager Open Edition with basic, intermediate and advanced exercises. Event-driven processes This hands-on workshop allows the user to validate the experience of creating loan approval workflow with decision automation. The user will also have an introduction to Fuse, develop an endpoint and consume it using the workflow. Learn the tools In this hands-on workshop, create new decision services using Kogito tooling and the VSCode extension , create unit tests and deploy them on KIE Server. Learn how to deploy IBAMOE on OpenShift using Operators Try out the business automation operator and learn how to manage your KIE Apps on OpenShift. Check the Learn more section for more guides and references about business automation with the projects under the KIE umbrella.","title":"IBM Business Automation Open Edition 9.2.x Enablement"},{"location":"#ibm-business-automation-open-edition-92x-enablement","text":"","title":"IBM Business Automation Open Edition 9.2.x Enablement"},{"location":"#your-journey-begins-here","text":"Get the tools First Kogito Project Setup","title":"Your Journey Begins Here"},{"location":"#labs-at-ibm-techxchange-2024","text":"Guided exercises for TechXchange Session 1108","title":"Labs at IBM TechXChange 2024"},{"location":"#techxchange-2023","text":"Guided exercises for TechXchange Session 1124","title":"TechXchange 2023"},{"location":"#ibm-business-automation-open-edition-92x-decision-manager-exercises","text":"Learn DMN Implement a On-push CI/CD","title":"IBM Business Automation Open Edition 9.2.x Decision Manager Exercises"},{"location":"#ibm-business-automation-open-edition-80-process-automation-exercises","text":"Getting started with Proces Event-driven processes","title":"IBM Business Automation Open Edition 8.0 Process Automation Exercises"},{"location":"#more-topics","text":"Extra Learning BAMOE on OpenShift","title":"More topics"},{"location":"#about-the-guides","text":"","title":"About the guides"},{"location":"#getting-started-with-ibamoe-order-management","text":"A great guide for users who are trying BAMOE for the first time. Recommended getting started guide.","title":"Getting Started with IBAMOE: Order Management"},{"location":"#learn-dmn","text":"In this hands-on workshop, users can learn about the DMN specification, author decisions, and deploy it using IBM Decision Manager Open Edition with basic, intermediate and advanced exercises.","title":"Learn DMN"},{"location":"#event-driven-processes","text":"This hands-on workshop allows the user to validate the experience of creating loan approval workflow with decision automation. The user will also have an introduction to Fuse, develop an endpoint and consume it using the workflow.","title":"Event-driven processes"},{"location":"#learn-the-tools","text":"In this hands-on workshop, create new decision services using Kogito tooling and the VSCode extension , create unit tests and deploy them on KIE Server.","title":"Learn the tools"},{"location":"#learn-how-to-deploy-ibamoe-on-openshift-using-operators","text":"Try out the business automation operator and learn how to manage your KIE Apps on OpenShift. Check the Learn more section for more guides and references about business automation with the projects under the KIE umbrella.","title":"Learn how to deploy IBAMOE on OpenShift using Operators"},{"location":"more/","text":"Learn more Videos KIE Live events are live streams designed to facilitate knowledge sharing about the Business Automation topic, including business rules, decisions, processes, resource planning, tooling, and AI. They're community events and anyone is welcome to attend. Check out all the KIE Lives at: https://red.ht/kielives Red Hat Scholars You can refer to Red Hat Scholars to find several up-to-date guides about business automation topics, including Kogito. Check out all the great guides available at: https://redhat-scholars.github.io/cloud-native-business-automation/ Guides There are several guided exercises and workshops that are not yet part of this guide. You can find them below: Loan Approval Workshop with DMN This workshop is aimed at providing hands on experience creating DMN assets. This lab will implement a Loan Approval workflow. Check out this guide here . Loan Approval Workshop This workshop is aimed at providing hands-on experience creating Decision and Process Assets. This lab will implement a Loan Approval workflow. Check out this guide here . KIE Learning: BAMOE and RHDM This repository is a set of explanation and hands-on labs which you can try on your environment and follow at your own pace. The step-by-step guides covers different topics, since basics to more advanced and specific features. The content is not sequential and it covers Decision Manager, Business Optimizer, Process Automation Manager and Kogito. DMN Handbook This handbook is a vademecum for the FEEL expression language from the DMN specification, as implemented by the Drools DMN open source engine.","title":"Learn more"},{"location":"more/#learn-more","text":"","title":"Learn more"},{"location":"more/#videos","text":"KIE Live events are live streams designed to facilitate knowledge sharing about the Business Automation topic, including business rules, decisions, processes, resource planning, tooling, and AI. They're community events and anyone is welcome to attend. Check out all the KIE Lives at: https://red.ht/kielives","title":"Videos"},{"location":"more/#red-hat-scholars","text":"You can refer to Red Hat Scholars to find several up-to-date guides about business automation topics, including Kogito. Check out all the great guides available at: https://redhat-scholars.github.io/cloud-native-business-automation/","title":"Red Hat Scholars"},{"location":"more/#guides","text":"There are several guided exercises and workshops that are not yet part of this guide. You can find them below: Loan Approval Workshop with DMN This workshop is aimed at providing hands on experience creating DMN assets. This lab will implement a Loan Approval workflow. Check out this guide here . Loan Approval Workshop This workshop is aimed at providing hands-on experience creating Decision and Process Assets. This lab will implement a Loan Approval workflow. Check out this guide here . KIE Learning: BAMOE and RHDM This repository is a set of explanation and hands-on labs which you can try on your environment and follow at your own pace. The step-by-step guides covers different topics, since basics to more advanced and specific features. The content is not sequential and it covers Decision Manager, Business Optimizer, Process Automation Manager and Kogito. DMN Handbook This handbook is a vademecum for the FEEL expression language from the DMN specification, as implemented by the Drools DMN open source engine.","title":"Guides"},{"location":"guided_exercises/00_get_tools/env-setup/","text":"Environment Setup This section will cover the deployment of some of the tools locally that you will use in these exercises. IBM Business Automation Open Edition 9.2.x In order to follow these guided labs, you should have BAMOE 8.0+ in your local environment. BAMOE is flexible in how you design. The developer runtime tools are predominantly focused in VSCode (download here ) with Maven builds targeted at a more cloud native strategy. There are also runtime options available through the utilization of the KIE Server (Knowledge is Everything) to execute. This enablement session will go through both, but the evolution of the jBPM/Drools projects are definitely more towards the Kogito runtimes versus the Java EE-based KIE Server. If you need to setup BAMOE locally, you can use this repository to help you get up and running quickly: IBM Business Automation Open Edition 9.2.x Environment Setup . This will provide the files as two different forms, a locally built environment that will persist, or the option to have an ephemeral container that will lose the data stored between sessions. Both forms will provide a Business Central and KIE Server environment for use. VS Code For running through the exercises, it is highly recommended that you get Visual Studio Code as this is the IDE that IBM Business Automation Open Edition 9.2.x most supports for activities regarding both decisions and processes. To get Visual Studio Code go to the Visual Studio Code download page and download for your platform. Git, Maven and Java These labs are going to assume you are already running , a running version of git, Maven 3.6.2+ and OpenJDK 11+. To get these added to your environment, you can follow the steps in this section. IBM Business Automation Open Edition 9.2.x is built around git source code control and Maven archetypes. You will need these tools to do most of the content in these quick walk throughs. Git Git is the open-source solution for source code management. To get git, go to here to install it if you don't have it already. All of the major source code management systems work with the git command line and is ultimately platform-agnostic to those. Maven With IBM Business Automation Open Edition 9.2.x, the components are built around a Maven architecture predominantly. What this ultimately means is your workstation needs to be able to communicate with one to many different Maven Repositories. These labs will use two in particular, the Red Hat General Availability repository and Maven Central . You could easily replace the two repositories with a local environment one hosting the Maven dependencies as a mirror or based in a disconnected installation, but this is the easiest developer workflow for acquiring new dependencies. The reason we are pointing to the Red Hat Maven repository, at least in the short term, is that the builds for IBM Business Automation Open Edition 9.2.x are being deployed there as they are the same binaries used within both the IBM and Red Hat products during the transition of Red Hat Process Automation Manager (RHPAM)/Red Hat Decision Manager (RHDM) from Red Hat into IBM Automation under the name of IBM Business Automation Open Edition 9.2.x ( BAMOE). The settings.xml file included below will use the local Maven repository at your USER_HOME /.m2/repository, which when configuring Maven would be the default M2_HOME that's created. Example settings.xml file <?xml version=\"1.0\" encoding=\"UTF-8\"?> <settings xmlns= \"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\" > <localRepository> ${user.home}/.m2/repository </localRepository> <interactiveMode> true </interactiveMode> <usePluginRegistry> false </usePluginRegistry> <offline> false </offline> <profiles> <!-- Profile with online repositories required by IBAMOE --> <profile> <id> brms-bpms-online-profile </id> <repositories> <repository> <!-- Red Hat Maven Repository--> <id> jboss-ga-repository </id> <url> https://maven.repository.redhat.com/ga/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> false </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> jboss-ga-plugin-repository </id> <url> https://maven.repository.redhat.com/ga/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> <profile> <id> maven-https </id> <activation> <activeByDefault> true </activeByDefault> </activation> <repositories> <repository> <!--Maven Central Repository--> <id> central </id> <url> https://repo1.maven.org/maven2 </url> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> central </id> <url> https://repo1.maven.org/maven2 </url> <snapshots> <enabled> false </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> </profiles> <activeProfiles> <!-- Activation of the BRMS/BPMS profile --> <activeProfile> brms-bpms-online-profile </activeProfile> <activeProfile> maven-https </activeProfile> </activeProfiles> </settings> Linux/Windows/Mac Follow the instructions at the Maven Community to download and update your path variables to incorporate it into your builds. Example Linux installation Download Maven 3.8.6 from here If using the Skytap image, change to the downloads folder. cd /home/pamadmin/Downloads Now you need to extract the tar tar xzvf apache-maven-3.8.6-bin.tar.gz Now if there's an existing Maven in your stack, you can move it out with this command. sudo mv /usr/share/maven /usr/share/maven-old Now move the new download into your /usr/share with the following command. sudo mv apache-maven-3.8.6 /usr/share/maven With this, Maven should be updated, validate with running a Maven version command. mvn -v Your console should return a log similar to below [pamadmin@host-1 maven]$ mvn -version Apache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63) Maven home: /usr/share/maven Java version: 11.0.16.1, vendor: Red Hat, Inc., runtime: /usr/lib/jvm/java-11-openjdk-11.0.16.1.1-1.el8_6.x86_64 Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"4.18.0-372.26.1.el8_6.x86_64\", arch: \"amd64\", family: \"unix\" Mac Alternative The easiest way to acquire Maven is to use homebrew and run the command brew install maven , which at the time of writing will install Maven 3.8.6. Java The assumption in these exercises are that you will utilize a supported JDK. These were tested with openjdk 11.0.11 2021-04-20 , but other ones are supported. You should be using at least Java 11 with IBM Business Automation Open Edition 9.2.x to ensure your best experience with the tooling. Kafka The Kafka setup requires docker-compose to run locally, to get this capability, you can use either Docker or Podman for this capability. To see which is right for you, read this article . Coming later in 2022 will be the labs shifted to using an OpenShift deployment of AMQ Streams to minimize the local installations required. Event-driven processes can react to the events that happens in the ecosystem. Kafka is an open-source even streaming platform, and currently, one of the most popular tools. In this type of architecture, we have the Kafka topics used as the communication layer in between the services. Each service can now be considered a consumer or a producer , in other words, each service can publish or consume events to/from the topics . IBM supports the integration between BAMOE and AMQ Streams (Kafka). To follow the labs, you should have an accessible Kafka server. The KIE Server (process engine) will communicate with the topics that we will create in the Kafka server. If you don't have an environment available you can get a Kafka (Strimzi) server quickly running by using Docker. Let's clone the project to the enablement folder. Create a new folder named enablement and access it: mkdir ~/enablement && cd ~/enablement Clone this repository to your local machine. git clone https://github.com/hguerrero/amq-examples The docker-compose file available in this quickstart should bootstrap everything you need to have your Strimzi up and running: Zookeeper, Kafka server v2.5.0, Apicurio Registry and a Kafka Bridge. Access the amq-examples/strimzi-all-in-one/ folder: cd amq-examples/strimzi-all-in-one/ Start the Kafka environment: docker-compose up Docker will download the images and start the services for you. You now have a Kafka server running on localhost port 9092. Creating the Kafka topics In the upcoming labs we will need three topics: incoming-requests , requests-approved and requests-denied . Next, you need to create these topics in your Kafka server. If you are using the containerized option mentioned in this lab, you can create the topics using the kafka-topics.sh available in the kafka container. Open a new terminal and let's using the kafka container we have just started. Enter the Kafka folder: cd ~/enablement/amq-examples/strimzi-all-in-one / Create the following three topics: docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic incoming-requests docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic requests-approved docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic requests-denied Next steps At this point you should have BAMOE, and Kafka on your environment. Before proceeding to the next steps, make sure you have both BAMOE and Kafka server up and running.","title":"Environment Setup"},{"location":"guided_exercises/00_get_tools/env-setup/#environment-setup","text":"This section will cover the deployment of some of the tools locally that you will use in these exercises. IBM Business Automation Open Edition 9.2.x In order to follow these guided labs, you should have BAMOE 8.0+ in your local environment. BAMOE is flexible in how you design. The developer runtime tools are predominantly focused in VSCode (download here ) with Maven builds targeted at a more cloud native strategy. There are also runtime options available through the utilization of the KIE Server (Knowledge is Everything) to execute. This enablement session will go through both, but the evolution of the jBPM/Drools projects are definitely more towards the Kogito runtimes versus the Java EE-based KIE Server. If you need to setup BAMOE locally, you can use this repository to help you get up and running quickly: IBM Business Automation Open Edition 9.2.x Environment Setup . This will provide the files as two different forms, a locally built environment that will persist, or the option to have an ephemeral container that will lose the data stored between sessions. Both forms will provide a Business Central and KIE Server environment for use.","title":"Environment Setup"},{"location":"guided_exercises/00_get_tools/env-setup/#vs-code","text":"For running through the exercises, it is highly recommended that you get Visual Studio Code as this is the IDE that IBM Business Automation Open Edition 9.2.x most supports for activities regarding both decisions and processes. To get Visual Studio Code go to the Visual Studio Code download page and download for your platform.","title":"VS Code"},{"location":"guided_exercises/00_get_tools/env-setup/#git-maven-and-java","text":"These labs are going to assume you are already running , a running version of git, Maven 3.6.2+ and OpenJDK 11+. To get these added to your environment, you can follow the steps in this section. IBM Business Automation Open Edition 9.2.x is built around git source code control and Maven archetypes. You will need these tools to do most of the content in these quick walk throughs.","title":"Git, Maven and Java"},{"location":"guided_exercises/00_get_tools/env-setup/#git","text":"Git is the open-source solution for source code management. To get git, go to here to install it if you don't have it already. All of the major source code management systems work with the git command line and is ultimately platform-agnostic to those.","title":"Git"},{"location":"guided_exercises/00_get_tools/env-setup/#maven","text":"With IBM Business Automation Open Edition 9.2.x, the components are built around a Maven architecture predominantly. What this ultimately means is your workstation needs to be able to communicate with one to many different Maven Repositories. These labs will use two in particular, the Red Hat General Availability repository and Maven Central . You could easily replace the two repositories with a local environment one hosting the Maven dependencies as a mirror or based in a disconnected installation, but this is the easiest developer workflow for acquiring new dependencies. The reason we are pointing to the Red Hat Maven repository, at least in the short term, is that the builds for IBM Business Automation Open Edition 9.2.x are being deployed there as they are the same binaries used within both the IBM and Red Hat products during the transition of Red Hat Process Automation Manager (RHPAM)/Red Hat Decision Manager (RHDM) from Red Hat into IBM Automation under the name of IBM Business Automation Open Edition 9.2.x ( BAMOE). The settings.xml file included below will use the local Maven repository at your USER_HOME /.m2/repository, which when configuring Maven would be the default M2_HOME that's created. Example settings.xml file <?xml version=\"1.0\" encoding=\"UTF-8\"?> <settings xmlns= \"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\" > <localRepository> ${user.home}/.m2/repository </localRepository> <interactiveMode> true </interactiveMode> <usePluginRegistry> false </usePluginRegistry> <offline> false </offline> <profiles> <!-- Profile with online repositories required by IBAMOE --> <profile> <id> brms-bpms-online-profile </id> <repositories> <repository> <!-- Red Hat Maven Repository--> <id> jboss-ga-repository </id> <url> https://maven.repository.redhat.com/ga/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> false </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> jboss-ga-plugin-repository </id> <url> https://maven.repository.redhat.com/ga/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> <profile> <id> maven-https </id> <activation> <activeByDefault> true </activeByDefault> </activation> <repositories> <repository> <!--Maven Central Repository--> <id> central </id> <url> https://repo1.maven.org/maven2 </url> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> central </id> <url> https://repo1.maven.org/maven2 </url> <snapshots> <enabled> false </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> </profiles> <activeProfiles> <!-- Activation of the BRMS/BPMS profile --> <activeProfile> brms-bpms-online-profile </activeProfile> <activeProfile> maven-https </activeProfile> </activeProfiles> </settings>","title":"Maven"},{"location":"guided_exercises/00_get_tools/env-setup/#linuxwindowsmac","text":"Follow the instructions at the Maven Community to download and update your path variables to incorporate it into your builds.","title":"Linux/Windows/Mac"},{"location":"guided_exercises/00_get_tools/env-setup/#example-linux-installation","text":"Download Maven 3.8.6 from here If using the Skytap image, change to the downloads folder. cd /home/pamadmin/Downloads Now you need to extract the tar tar xzvf apache-maven-3.8.6-bin.tar.gz Now if there's an existing Maven in your stack, you can move it out with this command. sudo mv /usr/share/maven /usr/share/maven-old Now move the new download into your /usr/share with the following command. sudo mv apache-maven-3.8.6 /usr/share/maven With this, Maven should be updated, validate with running a Maven version command. mvn -v Your console should return a log similar to below [pamadmin@host-1 maven]$ mvn -version Apache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63) Maven home: /usr/share/maven Java version: 11.0.16.1, vendor: Red Hat, Inc., runtime: /usr/lib/jvm/java-11-openjdk-11.0.16.1.1-1.el8_6.x86_64 Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"4.18.0-372.26.1.el8_6.x86_64\", arch: \"amd64\", family: \"unix\"","title":"Example Linux installation"},{"location":"guided_exercises/00_get_tools/env-setup/#mac-alternative","text":"The easiest way to acquire Maven is to use homebrew and run the command brew install maven , which at the time of writing will install Maven 3.8.6.","title":"Mac Alternative"},{"location":"guided_exercises/00_get_tools/env-setup/#java","text":"The assumption in these exercises are that you will utilize a supported JDK. These were tested with openjdk 11.0.11 2021-04-20 , but other ones are supported. You should be using at least Java 11 with IBM Business Automation Open Edition 9.2.x to ensure your best experience with the tooling.","title":"Java"},{"location":"guided_exercises/00_get_tools/env-setup/#kafka","text":"The Kafka setup requires docker-compose to run locally, to get this capability, you can use either Docker or Podman for this capability. To see which is right for you, read this article . Coming later in 2022 will be the labs shifted to using an OpenShift deployment of AMQ Streams to minimize the local installations required. Event-driven processes can react to the events that happens in the ecosystem. Kafka is an open-source even streaming platform, and currently, one of the most popular tools. In this type of architecture, we have the Kafka topics used as the communication layer in between the services. Each service can now be considered a consumer or a producer , in other words, each service can publish or consume events to/from the topics . IBM supports the integration between BAMOE and AMQ Streams (Kafka). To follow the labs, you should have an accessible Kafka server. The KIE Server (process engine) will communicate with the topics that we will create in the Kafka server. If you don't have an environment available you can get a Kafka (Strimzi) server quickly running by using Docker. Let's clone the project to the enablement folder. Create a new folder named enablement and access it: mkdir ~/enablement && cd ~/enablement Clone this repository to your local machine. git clone https://github.com/hguerrero/amq-examples The docker-compose file available in this quickstart should bootstrap everything you need to have your Strimzi up and running: Zookeeper, Kafka server v2.5.0, Apicurio Registry and a Kafka Bridge. Access the amq-examples/strimzi-all-in-one/ folder: cd amq-examples/strimzi-all-in-one/ Start the Kafka environment: docker-compose up Docker will download the images and start the services for you. You now have a Kafka server running on localhost port 9092.","title":"Kafka"},{"location":"guided_exercises/00_get_tools/env-setup/#creating-the-kafka-topics","text":"In the upcoming labs we will need three topics: incoming-requests , requests-approved and requests-denied . Next, you need to create these topics in your Kafka server. If you are using the containerized option mentioned in this lab, you can create the topics using the kafka-topics.sh available in the kafka container. Open a new terminal and let's using the kafka container we have just started. Enter the Kafka folder: cd ~/enablement/amq-examples/strimzi-all-in-one / Create the following three topics: docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic incoming-requests docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic requests-approved docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic requests-denied","title":"Creating the Kafka topics"},{"location":"guided_exercises/00_get_tools/env-setup/#next-steps","text":"At this point you should have BAMOE, and Kafka on your environment. Before proceeding to the next steps, make sure you have both BAMOE and Kafka server up and running.","title":"Next steps"},{"location":"guided_exercises/00_get_tools/git-token/","text":"Link IBM Business Automation Manager Canvas to your GitHub account and create a project in GitHub using Extended Services When using IBM Business Automation Manager Canvas you can start from the decision or process model creation and work top-down to deliver your business automation work. This is a feature that will enable you to create your models and publish a sample project to your Git provider. From here the project can be deployed in your traditional means .To do this, you need to make sure your connection to GitHub is active with the token and from there you can jump right in! Link GitHub to your IBM Business Automation Manager Canvas In this section we will link your GitHub account to the IBM Business Automation Manager Canvas so we can easily synchronize changes in DMN with GitHub and our tooling, in this case IBM Business Automation Manager Canvas. First click the User icon to connect your public GitHub account to the IBM Business Automation Manager Canvas. Click the Connect to an account button to add a new Git provider Select the card for GitHub Click Generate new token to create a new token that will be used by IBM Business Automation Manager Canvas You can use similar properties to the token created below in the screenshot, but the main 2 to have right now are repo and gist - the others can be beneficial if you reuse this token for other purposes too, but are not required nor needed. You can change the date to never expiring or be as short as you want. Once the token is generated though, that is the only time you will see the actual token value. Name : Name your token a unique name from any previously created Expiration : This can either be a set time period, up to 1 year or never expiring The checkboxes you need are repo and gist to get the full benefit of IBM Business Automation Manager Canvas Use the copy button that's created with the Token to use in IBM Business Automation Manager Canvas. Return to IBM Business Automation Manager Canvas and insert the Token into the wizard. When your token is pasted, the IBM Business Automation Manager Canvas will return a similar screen to below towards your GitHub account signifying you've connected, your GitHub user ID and some extra details.","title":"Git Token Setup"},{"location":"guided_exercises/00_get_tools/git-token/#link-ibm-business-automation-manager-canvas-to-your-github-account-and-create-a-project-in-github-using-extended-services","text":"When using IBM Business Automation Manager Canvas you can start from the decision or process model creation and work top-down to deliver your business automation work. This is a feature that will enable you to create your models and publish a sample project to your Git provider. From here the project can be deployed in your traditional means .To do this, you need to make sure your connection to GitHub is active with the token and from there you can jump right in!","title":"Link IBM Business Automation Manager Canvas to your GitHub account and create a project in GitHub using Extended Services"},{"location":"guided_exercises/00_get_tools/git-token/#link-github-to-your-ibm-business-automation-manager-canvas","text":"In this section we will link your GitHub account to the IBM Business Automation Manager Canvas so we can easily synchronize changes in DMN with GitHub and our tooling, in this case IBM Business Automation Manager Canvas. First click the User icon to connect your public GitHub account to the IBM Business Automation Manager Canvas. Click the Connect to an account button to add a new Git provider Select the card for GitHub Click Generate new token to create a new token that will be used by IBM Business Automation Manager Canvas You can use similar properties to the token created below in the screenshot, but the main 2 to have right now are repo and gist - the others can be beneficial if you reuse this token for other purposes too, but are not required nor needed. You can change the date to never expiring or be as short as you want. Once the token is generated though, that is the only time you will see the actual token value. Name : Name your token a unique name from any previously created Expiration : This can either be a set time period, up to 1 year or never expiring The checkboxes you need are repo and gist to get the full benefit of IBM Business Automation Manager Canvas Use the copy button that's created with the Token to use in IBM Business Automation Manager Canvas. Return to IBM Business Automation Manager Canvas and insert the Token into the wizard. When your token is pasted, the IBM Business Automation Manager Canvas will return a similar screen to below towards your GitHub account signifying you've connected, your GitHub user ID and some extra details.","title":"Link GitHub to your IBM Business Automation Manager Canvas"},{"location":"guided_exercises/00_get_tools/introduction/","text":"Introduction Before getting started with IBM Business Automation Open Edition 9.2.x , we must first explore and prepare the different tools we'll need in order to go beyond the basics on this Business Automation journey. We'll be using different tools as we move through the guided exercises: - During development, we'll use IBM Business Automation Open Editions IBM Business Automation Manager Canvas , which is known in the open-source as KIE Sandbox. - To set up the local workstation, we'll use VSCode Plugins, Maven, GraalVM and more. - Since our goal with this enablement is to move towards Kogito adoption and its cloud-native approach, we'll mostly use IBM Business Automation Manager Canvas and VSCode. Although, you can also find extra exercises covering the usage of Business Central, supported on the 8.x version.","title":"Introduction"},{"location":"guided_exercises/00_get_tools/introduction/#introduction","text":"Before getting started with IBM Business Automation Open Edition 9.2.x , we must first explore and prepare the different tools we'll need in order to go beyond the basics on this Business Automation journey. We'll be using different tools as we move through the guided exercises: - During development, we'll use IBM Business Automation Open Editions IBM Business Automation Manager Canvas , which is known in the open-source as KIE Sandbox. - To set up the local workstation, we'll use VSCode Plugins, Maven, GraalVM and more. - Since our goal with this enablement is to move towards Kogito adoption and its cloud-native approach, we'll mostly use IBM Business Automation Manager Canvas and VSCode. Although, you can also find extra exercises covering the usage of Business Central, supported on the 8.x version.","title":"Introduction"},{"location":"guided_exercises/00_intro/","text":"Getting started In this section you will become familiar with foundational concepts of the product and use the main features of IBM Business Automation Manager Canvas with decisions based on DMN. Goals: Know the key components of IBM BAMOE. Set up your development environment. Become familiar with the use case. Explore the key features of IBM Business Automation Manager Canvas through a Decision Automation perspective. Let's move forward and get a quick overview of IBM BAMOE and its components.","title":"Getting started"},{"location":"guided_exercises/00_intro/#getting-started","text":"In this section you will become familiar with foundational concepts of the product and use the main features of IBM Business Automation Manager Canvas with decisions based on DMN. Goals: Know the key components of IBM BAMOE. Set up your development environment. Become familiar with the use case. Explore the key features of IBM Business Automation Manager Canvas through a Decision Automation perspective. Let's move forward and get a quick overview of IBM BAMOE and its components.","title":"Getting started"},{"location":"guided_exercises/00_intro/02_bamoe_basics/","text":"Overview of IBM Business Automation Manager Open Editions Components IBM Business Automation Manager Open Editions (IBM BAMOE) is a comprehensive platform for automating business processes and decision services. It provides a range of tools and components that enable you to design, implement, and manage business automation solutions. Key Components IBM BAMOE consists of several core components essential for development, execution, and management of business automation processes. Development Tools IBM Business Automation Manager Canvas : A web-based environment for designing and modeling business processes and decisions. IBM BAMOE Developer Tools for VS Code : Tools for developing business processes and decisions within the Visual Studio Code environment. Forms CLI : A command-line tool for generating web forms required for process execution. Execution Decision and Rules Engine : Executes decision models and business rules. Process Engine : Executes business process models. Management Management Console : Provides an interface for managing and monitoring process instances and tasks. Task Inbox : A user interface for viewing and interacting with user tasks in BAMOE process services. With a solid understanding of IBM BAMOE's main components, we will introduce the primary use case that will guide our hands-on exercises.","title":"Overview of IBM Business Automation Manager Open Editions"},{"location":"guided_exercises/00_intro/02_bamoe_basics/#overview-of-ibm-business-automation-manager-open-editions","text":"","title":"Overview of IBM Business Automation Manager Open Editions"},{"location":"guided_exercises/00_intro/02_bamoe_basics/#components","text":"IBM Business Automation Manager Open Editions (IBM BAMOE) is a comprehensive platform for automating business processes and decision services. It provides a range of tools and components that enable you to design, implement, and manage business automation solutions.","title":"Components"},{"location":"guided_exercises/00_intro/02_bamoe_basics/#key-components","text":"IBM BAMOE consists of several core components essential for development, execution, and management of business automation processes. Development Tools IBM Business Automation Manager Canvas : A web-based environment for designing and modeling business processes and decisions. IBM BAMOE Developer Tools for VS Code : Tools for developing business processes and decisions within the Visual Studio Code environment. Forms CLI : A command-line tool for generating web forms required for process execution. Execution Decision and Rules Engine : Executes decision models and business rules. Process Engine : Executes business process models. Management Management Console : Provides an interface for managing and monitoring process instances and tasks. Task Inbox : A user interface for viewing and interacting with user tasks in BAMOE process services. With a solid understanding of IBM BAMOE's main components, we will introduce the primary use case that will guide our hands-on exercises.","title":"Key Components"},{"location":"guided_exercises/00_intro/03_use_case_overview/","text":"Use Case Overview The Credit Card Application Process is a comprehensive workflow that automates the evaluation and approval of credit card applications. This process ensures that applications are thoroughly evaluated and processed accurately. Objectives of the Business Process The primary objective of this process is to evaluate applicant data and decide whether to approve or reject the application. This ensures that only qualified applicants receive a credit card while minimizing risk for the financial institution. Key Steps in the Process Receive Application : Receive and parse the application. Validate initial data and assign a unique application ID. Evaluate Application : Assess the applicant's credit score and financial information. Applications with a credit score below 550 are automatically rejected. Applications with scores between 550 and 649 are sent for manual review. High scores (650 and above) are considered for further processing. Manual Review : Further scrutiny by a financial officer. Detailed analysis of the applicant's financial history and relevant factors. Generate Card Details : Once approved, generate necessary card details such as card number, CVV, expiration date, credit limit, and APR. Approval and Notification : Approved applicants receive their card details, while rejected applicants are informed of the reasons for rejection. Business Rules and External Validations The process incorporates several business rules to ensure accurate decision-making: - Credit Score Assessment : Determines initial eligibility based on predefined thresholds. - Debt-to-Income Ratio Check : Ensures applicants have a manageable level of debt relative to their income. - Fraud Risk Assessment : Uses external AI services to detect potential fraud, adding an extra layer of security. Approvals and Validations Automated Approvals : Based on clear-cut criteria such as credit score and debt-to-income ratio. Manual Approvals : Required for borderline cases where human judgment is necessary. External Service Validations : Involving AI-based fraud detection and other external checks to ensure data authenticity and reliability. Next, let's delve into the environment setup required to begin our journey with BAMOE.","title":"Use Case Overview"},{"location":"guided_exercises/00_intro/03_use_case_overview/#use-case-overview","text":"The Credit Card Application Process is a comprehensive workflow that automates the evaluation and approval of credit card applications. This process ensures that applications are thoroughly evaluated and processed accurately.","title":"Use Case Overview"},{"location":"guided_exercises/00_intro/03_use_case_overview/#objectives-of-the-business-process","text":"The primary objective of this process is to evaluate applicant data and decide whether to approve or reject the application. This ensures that only qualified applicants receive a credit card while minimizing risk for the financial institution.","title":"Objectives of the Business Process"},{"location":"guided_exercises/00_intro/03_use_case_overview/#key-steps-in-the-process","text":"Receive Application : Receive and parse the application. Validate initial data and assign a unique application ID. Evaluate Application : Assess the applicant's credit score and financial information. Applications with a credit score below 550 are automatically rejected. Applications with scores between 550 and 649 are sent for manual review. High scores (650 and above) are considered for further processing. Manual Review : Further scrutiny by a financial officer. Detailed analysis of the applicant's financial history and relevant factors. Generate Card Details : Once approved, generate necessary card details such as card number, CVV, expiration date, credit limit, and APR. Approval and Notification : Approved applicants receive their card details, while rejected applicants are informed of the reasons for rejection.","title":"Key Steps in the Process"},{"location":"guided_exercises/00_intro/03_use_case_overview/#business-rules-and-external-validations","text":"The process incorporates several business rules to ensure accurate decision-making: - Credit Score Assessment : Determines initial eligibility based on predefined thresholds. - Debt-to-Income Ratio Check : Ensures applicants have a manageable level of debt relative to their income. - Fraud Risk Assessment : Uses external AI services to detect potential fraud, adding an extra layer of security.","title":"Business Rules and External Validations"},{"location":"guided_exercises/00_intro/03_use_case_overview/#approvals-and-validations","text":"Automated Approvals : Based on clear-cut criteria such as credit score and debt-to-income ratio. Manual Approvals : Required for borderline cases where human judgment is necessary. External Service Validations : Involving AI-based fraud detection and other external checks to ensure data authenticity and reliability. Next, let's delve into the environment setup required to begin our journey with BAMOE.","title":"Approvals and Validations"},{"location":"guided_exercises/00_intro/04_env_setup/","text":"Environment Setup In this section, we will set up the necessary environment to start working with IBM Business Automation Manager Open Editions (BAMOE). Prerequisites Below is the list of technologies you will need. Java JDK 17 Maven 3.9.6 OpenShift 4.15 ( Need an OpenShift? Try the Dev Sandbox for Red Hat OpenShift for free. ) Git CLI Microsoft VSCode (latest version) (Need the IDE? Get it here: VSCode download page .) Tip For more information about the supported environments of IBM BAMOE, refer to the official documentation You may need an account on these websites in order to rely on easy to use cloud services in some of the exercises: GitHub - for versioning projects on GitHub Red Hat - For those using Dev Sandbox for OpenShift IBM - For using IBM's managed Kafka service Verifying Installed Software & Version To verify if you have the correct versions installed, you can use the following commands: java -version mvn -version oc version git --version Preparing VSCode and OpenShift Dev Sandbox If you have an OpenShift instance ready to use, and if the IBM BAMOE Dev Tools extension is already installed your VSCode IDE, you can jump to installing canvas Setting Up Local Development Environment with VS Code For running through the exercises, we'll use Visual Studio Code combined with IBM BAMOE Developer Tools Extension for Microsoft VSCode. Setting Up IBM BAMOE Developer Tools for VS Code To install BAMOE Dev Tools, open your IDE and go through the following steps: Go to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window or by pressing Ctrl+Shift+X . In the search box, type IBM BAMOE Developer Tools . Click the Install button for the IBM BAMOE Developer Tools extension. Need an OpenShift Environment? Developer Sandbox for Red Hat OpenShift To go through the OpenShift exercises, you'll need an OpenShift environment up and running. You can use any instance where you have access within a project. If you need a free, easy-to-setup OpenShift for our learning purposes, you can use the Developer Sandbox for Red Hat OpenShift . As stated by Red Hat: \"The Developer Sandbox for Red Hat OpenShift provides you with 30 days of no-cost access to a shared cluster on OpenShift, an enterprise-grade Kubernetes-based platform. Get instant access to your own minimal, preconfigured OpenShift environment for development and testing, hosted and managed by Red Hat.\" After you log-in and create a sandbox on the page above, you'll get to access your OpenShift: In this environment, you have access to a namespace named with your-username-dev (e.g. karina-varela-dev ). Login to access OpenShift from your Terminal To deploy IBM Business Automation Manager Canvas to OpenShift, you will need your OpenShift login token. Follow these steps to retrieve it using the OpenShift web console: Log in to the OpenShift Web Console : Open your web browser and navigate to your OpenShift cluster's web console URL. Enter your username and password to log in. Accessing the Token : Click on your username in the top-right corner of the console, and select \" Copy Login Command \" from the dropdown menu. A new page will open with your login token. Click on the \" Display Token \" button to view your token. Copy the login token provided. Using the Token to log-in in your terminal : Open your terminal. Log in to the OpenShift cluster using the oc CLI with the following command: oc login --token = <your-token> --server = <your-openshift-api-url> Next, let's install IBM Business Automation Manager Canvas on OpenShift. Installing IBM Business Automation Manager Canvas on Red Hat OpenShift Container Platform (OCP) IBM Business Automation Manager Canvas is a web application that provides authoring tools for standards based business assets, directly in the browser. It allows users to create, edit, and manage decisions and processes, integrate with git for syncing repositories, and during development phase, deploy files to OpenShift and Kubernetes. To install IBM Business Automation Manager Canvas on a container platform, we need to install three resources: Extended Services CORS Proxy IBM Business Automation Manager Canvas Let's go through the installation and recall the purpose of these resources. Installation Steps In this section, we will install IBM Business Automation Manager Canvas on an OpenShift cluster. By using a manual installation process, you can have a better view of the resources created on OCP during the installation. Note Users who can install helm locally, can refer to the official documentation on how to Install IBM Business Automation Manager Canvas with Helm . Installing IBM Business Automation Manager Canvas on OpenShift Let's install IBM Business Automation Manager Canvas , the environment we'll explore on the next lab for experimenting with automation services development with IBM BAMOE. IBM Business Automation Manager Canvas is a powerful web application that provides tools for authoring decisions and workflows directly in the browser. It integrates seamlessly with Git for version control and with OpenShift for deploying your models for development validation purposes. Deploy Extended Services Extended Services are back-end services that provide additional features to IBM Business Automation Manager Canvas, such as the DMN Runner (execution and validation of decision models) and a proxy (enables communication with OpenShift and Kubernetes clusters). export APP_PART_OF = bamoe-canvas-app export APP_NAME_EXTENDED_SERVICES = bamoe-extended-services oc new-app quay.io/bamoe/extended-services:9.1.0-ibm-0001 --name = $APP_NAME_EXTENDED_SERVICES oc create route edge --service = $APP_NAME_EXTENDED_SERVICES oc label services/ $APP_NAME_EXTENDED_SERVICES app.kubernetes.io/part-of = $APP_PART_OF oc label routes/ $APP_NAME_EXTENDED_SERVICES app.kubernetes.io/part-of = $APP_PART_OF oc label deployments/ $APP_NAME_EXTENDED_SERVICES app.kubernetes.io/part-of = $APP_PART_OF oc label deployments/ $APP_NAME_EXTENDED_SERVICES app.openshift.io/runtime = golang If you check your OpenShift console, you should be able to see the new pod is up and running. You can do the same observation for the next two deployments as well. Deploy CORS Proxy The CORS Proxy allows IBM Business Automation Manager Canvas to communicate with Git providers like GitHub, Gitlab and Bitbucket. export APP_NAME_CORS_PROXY = bamoe-cors-proxy oc new-app quay.io/bamoe/cors-proxy:9.1.0-ibm-0001 --name = $APP_NAME_CORS_PROXY oc create route edge --service = $APP_NAME_CORS_PROXY oc label services/ $APP_NAME_CORS_PROXY app.kubernetes.io/part-of = $APP_PART_OF oc label routes/ $APP_NAME_CORS_PROXY app.kubernetes.io/part-of = $APP_PART_OF oc label deployments/ $APP_NAME_CORS_PROXY app.kubernetes.io/part-of = $APP_PART_OF oc label deployments/ $APP_NAME_CORS_PROXY app.openshift.io/runtime = nodejs Deploy IBM Business Automation Manager Canvas Finally, deploy the IBM Business Automation Manager Canvas image, setting the environment variables required to connect it to the Extended Services and CORS Proxy backends. sh export APP_NAME_BAMOE_CANVAS=bamoe-canvas oc new-app quay.io/bamoe/canvas:9.1.0-ibm-0001 --name=$APP_NAME_BAMOE_CANVAS \\ --env=KIE_SANDBOX_EXTENDED_SERVICES_URL=https://$(oc get route $APP_NAME_EXTENDED_SERVICES --output jsonpath={.spec.host}) \\ --env=KIE_SANDBOX_CORS_PROXY_URL=https://$(oc get route $APP_NAME_CORS_PROXY --output jsonpath={.spec.host}) oc create route edge --service=$APP_NAME_BAMOE_CANVAS oc label services/$APP_NAME_BAMOE_CANVAS app.kubernetes.io/part-of=$APP_PART_OF oc label routes=$APP_NAME_BAMOE_CANVAS app.kubernetes.io/part-of=$APP_PART_OF oc label deployments/$APP_NAME_BAMOE_CANVAS app.kubernetes.io/part-of=$APP_PART_OF oc label deployments/$APP_NAME_BAMOE_CANVAS app.openshift.io/runtime=js Access IBM Business Automation Manager Canvas If all went well, you should be able to see three pods - either on the OCP web console or using the cli to retrieve the pods of the current namespace: oc get pods . You should see something like: oc get pods NAME READY STATUS RESTARTS bamoe-canvas-54f87f584c-dpjr6 1 /1 Running 0 bamoe-cors-proxy-578bf787cb-rc4t8 1 /1 Running 0 bamoe-extended-services-6fdfd85b7b-xg82x 1 /1 Running 0 Your IBM Business Automation Manager Canvas instance should be up and accessible. To get IBM Business Automation Manager Canvas' URL, run this command: oc get route $APP_NAME_BAMOE_CANVAS --output jsonpath ={ .spec.host } ; echo The retrieved URL would look something like bamoe-canvas-username-dev.apps.sandbox-id.p1.openshiftapps.com/#/ . The URL should lead to your new installation of IBM Business Automation Manager Canvas: With your development tools prepared, you're now ready to start exploring the solution in more detail. Let's move on to exploring Canvas with Decision Automation. Preparing IBM Business Automation Manager Canvas for Integrating with Git and OpenShift To be able to use the maximum potential of IBM Business Automation Manager Canvas, let's set up the integration with Git and OpenShift. In IBM Business Automation Manager Canvas, execute the following procedures to set up the integration: Connect IBM Business Automation Manager Canvas to OpenShift for Deployment and Integration In IBM Business Automation Manager Canvas, click on the \"User\" icon on the top-right corner, and click on \"Connect to an account\". Select OpenShift. Enter the OpenShift API URL and your project name. You can obtain them through the web console or using the CLI commands below to retrieve the namespace, API URL, and authentication token respectively: oc project -q oc whoami --show-server oc whoami --show-token Configuration sample: Integration Successful: Save the settings. Your IBM Business Automation Manager Canvas can now run dev deployments of your decision services on OpenShift! To wrap up, let's configure github integration. Connect IBM Business Automation Manager Canvas to GitHub for Version Control and Collaboration In IBM Business Automation Manager Canvas, go to the settings menu and select \"GitHub Integration\".! Click on Generate new token . You'll be redirected to the GitHub page where you can create a token for IBM Business Automation Manager Canvas to authenticate on GitHub with the given permissions. In GitHub, create a new classic token Choose a note (any description for this token) and the permissions for 'repo' and 'gist' scopes. On the bottom of the page, click on \" Generate Token \". Copy and save the generated token, you'll need it in IBM Business Automation Manager Canvas. Now back in IBM Business Automation Manager Canvas, enter your GitHub repository URL and personal access token. You should see a green message informing you have Successfully connected . Awesome!! You're all set! With your development tools prepared, you're now ready to start exploring the solution in more detail. Let's move on to exploring IBM Business Automation Manager Canvas with Decision Automation.","title":"Environment Setup"},{"location":"guided_exercises/00_intro/04_env_setup/#environment-setup","text":"In this section, we will set up the necessary environment to start working with IBM Business Automation Manager Open Editions (BAMOE).","title":"Environment Setup"},{"location":"guided_exercises/00_intro/04_env_setup/#prerequisites","text":"Below is the list of technologies you will need. Java JDK 17 Maven 3.9.6 OpenShift 4.15 ( Need an OpenShift? Try the Dev Sandbox for Red Hat OpenShift for free. ) Git CLI Microsoft VSCode (latest version) (Need the IDE? Get it here: VSCode download page .) Tip For more information about the supported environments of IBM BAMOE, refer to the official documentation You may need an account on these websites in order to rely on easy to use cloud services in some of the exercises: GitHub - for versioning projects on GitHub Red Hat - For those using Dev Sandbox for OpenShift IBM - For using IBM's managed Kafka service Verifying Installed Software & Version To verify if you have the correct versions installed, you can use the following commands: java -version mvn -version oc version git --version","title":"Prerequisites"},{"location":"guided_exercises/00_intro/04_env_setup/#preparing-vscode-and-openshift-dev-sandbox","text":"If you have an OpenShift instance ready to use, and if the IBM BAMOE Dev Tools extension is already installed your VSCode IDE, you can jump to installing canvas","title":"Preparing VSCode and OpenShift Dev Sandbox"},{"location":"guided_exercises/00_intro/04_env_setup/#setting-up-local-development-environment-with-vs-code","text":"For running through the exercises, we'll use Visual Studio Code combined with IBM BAMOE Developer Tools Extension for Microsoft VSCode.","title":"Setting Up Local Development Environment with VS Code"},{"location":"guided_exercises/00_intro/04_env_setup/#setting-up-ibm-bamoe-developer-tools-for-vs-code","text":"To install BAMOE Dev Tools, open your IDE and go through the following steps: Go to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window or by pressing Ctrl+Shift+X . In the search box, type IBM BAMOE Developer Tools . Click the Install button for the IBM BAMOE Developer Tools extension.","title":"Setting Up IBM BAMOE Developer Tools for VS Code"},{"location":"guided_exercises/00_intro/04_env_setup/#need-an-openshift-environment-developer-sandbox-for-red-hat-openshift","text":"To go through the OpenShift exercises, you'll need an OpenShift environment up and running. You can use any instance where you have access within a project. If you need a free, easy-to-setup OpenShift for our learning purposes, you can use the Developer Sandbox for Red Hat OpenShift . As stated by Red Hat: \"The Developer Sandbox for Red Hat OpenShift provides you with 30 days of no-cost access to a shared cluster on OpenShift, an enterprise-grade Kubernetes-based platform. Get instant access to your own minimal, preconfigured OpenShift environment for development and testing, hosted and managed by Red Hat.\" After you log-in and create a sandbox on the page above, you'll get to access your OpenShift: In this environment, you have access to a namespace named with your-username-dev (e.g. karina-varela-dev ).","title":"Need an OpenShift Environment? Developer Sandbox for Red Hat OpenShift"},{"location":"guided_exercises/00_intro/04_env_setup/#login-to-access-openshift-from-your-terminal","text":"To deploy IBM Business Automation Manager Canvas to OpenShift, you will need your OpenShift login token. Follow these steps to retrieve it using the OpenShift web console: Log in to the OpenShift Web Console : Open your web browser and navigate to your OpenShift cluster's web console URL. Enter your username and password to log in. Accessing the Token : Click on your username in the top-right corner of the console, and select \" Copy Login Command \" from the dropdown menu. A new page will open with your login token. Click on the \" Display Token \" button to view your token. Copy the login token provided. Using the Token to log-in in your terminal : Open your terminal. Log in to the OpenShift cluster using the oc CLI with the following command: oc login --token = <your-token> --server = <your-openshift-api-url> Next, let's install IBM Business Automation Manager Canvas on OpenShift.","title":"Login to access OpenShift from your Terminal"},{"location":"guided_exercises/00_intro/04_env_setup/#installing-ibm-business-automation-manager-canvas-on-red-hat-openshift-container-platform-ocp","text":"IBM Business Automation Manager Canvas is a web application that provides authoring tools for standards based business assets, directly in the browser. It allows users to create, edit, and manage decisions and processes, integrate with git for syncing repositories, and during development phase, deploy files to OpenShift and Kubernetes. To install IBM Business Automation Manager Canvas on a container platform, we need to install three resources: Extended Services CORS Proxy IBM Business Automation Manager Canvas Let's go through the installation and recall the purpose of these resources.","title":"Installing IBM Business Automation Manager Canvas on Red Hat OpenShift Container Platform (OCP)"},{"location":"guided_exercises/00_intro/04_env_setup/#installation-steps","text":"In this section, we will install IBM Business Automation Manager Canvas on an OpenShift cluster. By using a manual installation process, you can have a better view of the resources created on OCP during the installation. Note Users who can install helm locally, can refer to the official documentation on how to Install IBM Business Automation Manager Canvas with Helm .","title":"Installation Steps"},{"location":"guided_exercises/00_intro/04_env_setup/#installing-ibm-business-automation-manager-canvas-on-openshift","text":"Let's install IBM Business Automation Manager Canvas , the environment we'll explore on the next lab for experimenting with automation services development with IBM BAMOE. IBM Business Automation Manager Canvas is a powerful web application that provides tools for authoring decisions and workflows directly in the browser. It integrates seamlessly with Git for version control and with OpenShift for deploying your models for development validation purposes. Deploy Extended Services Extended Services are back-end services that provide additional features to IBM Business Automation Manager Canvas, such as the DMN Runner (execution and validation of decision models) and a proxy (enables communication with OpenShift and Kubernetes clusters). export APP_PART_OF = bamoe-canvas-app export APP_NAME_EXTENDED_SERVICES = bamoe-extended-services oc new-app quay.io/bamoe/extended-services:9.1.0-ibm-0001 --name = $APP_NAME_EXTENDED_SERVICES oc create route edge --service = $APP_NAME_EXTENDED_SERVICES oc label services/ $APP_NAME_EXTENDED_SERVICES app.kubernetes.io/part-of = $APP_PART_OF oc label routes/ $APP_NAME_EXTENDED_SERVICES app.kubernetes.io/part-of = $APP_PART_OF oc label deployments/ $APP_NAME_EXTENDED_SERVICES app.kubernetes.io/part-of = $APP_PART_OF oc label deployments/ $APP_NAME_EXTENDED_SERVICES app.openshift.io/runtime = golang If you check your OpenShift console, you should be able to see the new pod is up and running. You can do the same observation for the next two deployments as well. Deploy CORS Proxy The CORS Proxy allows IBM Business Automation Manager Canvas to communicate with Git providers like GitHub, Gitlab and Bitbucket. export APP_NAME_CORS_PROXY = bamoe-cors-proxy oc new-app quay.io/bamoe/cors-proxy:9.1.0-ibm-0001 --name = $APP_NAME_CORS_PROXY oc create route edge --service = $APP_NAME_CORS_PROXY oc label services/ $APP_NAME_CORS_PROXY app.kubernetes.io/part-of = $APP_PART_OF oc label routes/ $APP_NAME_CORS_PROXY app.kubernetes.io/part-of = $APP_PART_OF oc label deployments/ $APP_NAME_CORS_PROXY app.kubernetes.io/part-of = $APP_PART_OF oc label deployments/ $APP_NAME_CORS_PROXY app.openshift.io/runtime = nodejs Deploy IBM Business Automation Manager Canvas Finally, deploy the IBM Business Automation Manager Canvas image, setting the environment variables required to connect it to the Extended Services and CORS Proxy backends. sh export APP_NAME_BAMOE_CANVAS=bamoe-canvas oc new-app quay.io/bamoe/canvas:9.1.0-ibm-0001 --name=$APP_NAME_BAMOE_CANVAS \\ --env=KIE_SANDBOX_EXTENDED_SERVICES_URL=https://$(oc get route $APP_NAME_EXTENDED_SERVICES --output jsonpath={.spec.host}) \\ --env=KIE_SANDBOX_CORS_PROXY_URL=https://$(oc get route $APP_NAME_CORS_PROXY --output jsonpath={.spec.host}) oc create route edge --service=$APP_NAME_BAMOE_CANVAS oc label services/$APP_NAME_BAMOE_CANVAS app.kubernetes.io/part-of=$APP_PART_OF oc label routes=$APP_NAME_BAMOE_CANVAS app.kubernetes.io/part-of=$APP_PART_OF oc label deployments/$APP_NAME_BAMOE_CANVAS app.kubernetes.io/part-of=$APP_PART_OF oc label deployments/$APP_NAME_BAMOE_CANVAS app.openshift.io/runtime=js Access IBM Business Automation Manager Canvas If all went well, you should be able to see three pods - either on the OCP web console or using the cli to retrieve the pods of the current namespace: oc get pods . You should see something like: oc get pods NAME READY STATUS RESTARTS bamoe-canvas-54f87f584c-dpjr6 1 /1 Running 0 bamoe-cors-proxy-578bf787cb-rc4t8 1 /1 Running 0 bamoe-extended-services-6fdfd85b7b-xg82x 1 /1 Running 0 Your IBM Business Automation Manager Canvas instance should be up and accessible. To get IBM Business Automation Manager Canvas' URL, run this command: oc get route $APP_NAME_BAMOE_CANVAS --output jsonpath ={ .spec.host } ; echo The retrieved URL would look something like bamoe-canvas-username-dev.apps.sandbox-id.p1.openshiftapps.com/#/ . The URL should lead to your new installation of IBM Business Automation Manager Canvas: With your development tools prepared, you're now ready to start exploring the solution in more detail. Let's move on to exploring Canvas with Decision Automation.","title":"Installing IBM Business Automation Manager Canvas on OpenShift"},{"location":"guided_exercises/00_intro/04_env_setup/#preparing-ibm-business-automation-manager-canvas-for-integrating-with-git-and-openshift","text":"To be able to use the maximum potential of IBM Business Automation Manager Canvas, let's set up the integration with Git and OpenShift. In IBM Business Automation Manager Canvas, execute the following procedures to set up the integration:","title":"Preparing IBM Business Automation Manager Canvas for Integrating with Git and OpenShift"},{"location":"guided_exercises/00_intro/04_env_setup/#connect-ibm-business-automation-manager-canvas-to-openshift-for-deployment-and-integration","text":"In IBM Business Automation Manager Canvas, click on the \"User\" icon on the top-right corner, and click on \"Connect to an account\". Select OpenShift. Enter the OpenShift API URL and your project name. You can obtain them through the web console or using the CLI commands below to retrieve the namespace, API URL, and authentication token respectively: oc project -q oc whoami --show-server oc whoami --show-token Configuration sample: Integration Successful: Save the settings. Your IBM Business Automation Manager Canvas can now run dev deployments of your decision services on OpenShift! To wrap up, let's configure github integration.","title":"Connect IBM Business Automation Manager Canvas to OpenShift for Deployment and Integration"},{"location":"guided_exercises/00_intro/04_env_setup/#connect-ibm-business-automation-manager-canvas-to-github-for-version-control-and-collaboration","text":"In IBM Business Automation Manager Canvas, go to the settings menu and select \"GitHub Integration\".! Click on Generate new token . You'll be redirected to the GitHub page where you can create a token for IBM Business Automation Manager Canvas to authenticate on GitHub with the given permissions. In GitHub, create a new classic token Choose a note (any description for this token) and the permissions for 'repo' and 'gist' scopes. On the bottom of the page, click on \" Generate Token \". Copy and save the generated token, you'll need it in IBM Business Automation Manager Canvas. Now back in IBM Business Automation Manager Canvas, enter your GitHub repository URL and personal access token. You should see a green message informing you have Successfully connected . Awesome!! You're all set! With your development tools prepared, you're now ready to start exploring the solution in more detail. Let's move on to exploring IBM Business Automation Manager Canvas with Decision Automation.","title":"Connect IBM Business Automation Manager Canvas to GitHub for Version Control and Collaboration"},{"location":"guided_exercises/01_getting_started/01_deploy_local/","text":"Deploying the Project locally as a Quarkus runtime Now that we have seen a little of the DMN Decision, let's use the combined power of Quarkus ( Learn more here about the Kubernetes-native Java Stack that is the best place for Kogito to run! ) and Kogito to quickly build and deploy the service using Maven commands and also having the option to deploy it to OpenShift as well. The first thing we're going to do is open a terminal (either in VSCode or whatever method you prefer) and validate that you have Maven and Java installed. java -version Expected output should be similar to openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment AdoptOpenJDK-11.0.11+9 (build 11.0.11+9) OpenJDK 64-Bit Server VM AdoptOpenJDK-11.0.11+9 (build 11.0.11+9, mixed mode) mvn -version Expected output should be similar to: Apache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63) Java version: 18.0.2, vendor: Homebrew, runtime: /opt/homebrew/Cellar/openjdk/18.0.2/libexec/openjdk.jdk/Contents/Home Maven home: /opt/homebrew/Cellar/maven/3.8.6/libexec Default locale: en_US, platform encoding: UTF-8 OS name: \"mac os x\", version: \"12.6\", arch: \"aarch64\", family: \"mac\" Now that our environment has the appropriate resources, let's build and run the service locally. Quarkus does this very simply with the command mvn quarkus:dev which will operate the container in development mode. By default it will run on 2 ports when running this command, 8080 and 5005 . These ports can be maintained in the application.properties file in src/main/resources/ or you can modify the mvn quarkus:dev command to load those properties from the command line. If you want to do the command line, you can do mvn quarkus:dev -Dquarkus.http.port=8085 -Ddebug=7007 which would setup your application to run off of port 8085 and be able to attach a remote debugger at 7007 . To change the default host in application.properties add the following line. quarkus . http . port = 8085 After the project's application.properties has been updated, let's start the Quarkus service by running the following command. This command will run the remote debugger (which we are not using in these labs) on port 6006 so that if you already have another application running on 5005 (the default port) it won't give an error. mvn quarkus:dev -Ddebug = 6006 When the service is ready, it will produce a log similar to the below: $ quick-kogito % mvn quarkus:dev -Ddebug=6006 [INFO] Scanning for projects... [INFO] [INFO] --------------------< com.ibm.sample:quick-kogito >--------------------- [INFO] Building quick-kogito 1.0.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- ... 2022-09-29 20:48:34,404 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) Initializing DMN DT Validator... 2022-09-29 20:48:34,405 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) DMN DT Validator initialized. 2022-09-29 20:48:34,405 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) Analysing decision tables in DMN Model 'pricing' ... 2022-09-29 20:48:34,410 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) analysis for decision table 'Base price': 2022-09-29 20:48:34,412 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) Decision Table Analysis of table 'Base price' finished with no messages to be reported. 2022-09-29 20:48:34,494 INFO [org.kie.kog.qua.com.dep.KogitoAssetsProcessor] (build-38) reflectiveEfestoGeneratedClassBuildItem org.kie.kogito.quarkus.common.deployment.KogitoGeneratedSourcesBuildItem@415815e1 __ ____ __ _____ ___ __ ____ ______ --/ __ \\/ / / / _ | / _ \\/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,< / /_/ /\\ \\ --\\___\\_\\____/_/ |_/_/|_/_/|_|\\____/___/ 2022-09-29 20:48:35,728 INFO quick-kogito 1.0.0-SNAPSHOT on JVM started in 2.432s. 2022-09-29 20:48:35,728 INFO Listening on: http://localhost:8085 2022-09-29 20:48:35,728 INFO Profile dev activated. Live Coding activated. 2022-09-29 20:48:35,728 INFO Installed features: [cdi, kogito-decisions, kogito-predictions, kogito-processes, kogito-rules, resteasy-reactive, resteasy-reactive-jackson, smallrye-context-propagation, smallrye-openapi, swagger-ui, vertx] Using the project that is created Now that your application is created, you can start using it more for live development and change as you go. To access the Quarkus Development console, in the logs, you can Command/Ctrl+click the link in VSCode or go to ()[http://localhost:8085], directly. The development console will show various aspects of your deployment. A quick highlight will be: The list of end points that are available The produced Maven artifact name (in this case quick-kogito 1.0.0-SNAPSHOT) You can modify configurations of your running application from here The Swagger UI and OpenAPI pages to get more information and test out your end points. We will be clicking the Swagger UI link to open the Swagger page to try out our sample DMN. Here you can see your generated Quarkus endpoints that deal with the Decision model. When building a DMN Decision Service with IBM Business Automation Open Edition 9.2.x in Kogito, it will be able to use the DMN Model, coupled with the data model created in it to generate the required end points and inputs. This creates a Domain Specific Decision Service , which is different than how KIE Server worked before in that you would interact with a generic API and insert the objects for execution. By going away from the classpath loading strategy in Kogito, Decision Services are ready to be deployed into a cloud native environment. Your payloads are simpler and easy to reuse across any services as required without having to know exactly how the service is deployed, the objects associated with the deployment and more. The service that was created for the Decision Service is called Pricing Resource and if you click any of the end points you will get more information on them. The first end point to look at is the Post /pricing . This endpoint can be used to execute a decision and return just the data associated with the decison (inputs and outputs). After clicking, you can edit the Domain Specific payload in input that matches your decision input nodes ( Age and Previous incidents ) with values you want to test. Notice that the endpoints use faces in the object names, this matches the expected values that were tied to the inputs in the model as opposed to camelCase or snake_case or PascalCase. This helps with a consistent view across the Kogito stack - from design to runtime. Your result should look similar to the one below. As you can see, the Swagger-UI page produces a few things of note, the first being a Curl command to reproduce locally, the URL you called, and then ultimately the response from the service. The next endpoint to evaluate is the /pricing/dmnresult endpoint which will tell you more about the decision that executed. You can use the same payload from the previous steps to show the differences of what is presented. That payload can be similar to the code below provided in JSON. Click Try it out and repeat the steps from above. The input for the Try it out can be used from below. { \"Age\" : 35 , \"Previous incidents?\" : true } Click Execute with the updated payload. The result will go into more detail about which Decision Nodes were executed against and more. When there is more than one Decision Node available, you can see the results from each node. Now you can play with the DMN model more and see how those changes impact your Quarkus service. Since you're in Dev mode, as you make changes, those changes should be available to you each time you save. Try it out!","title":"Deploy Locally"},{"location":"guided_exercises/01_getting_started/01_deploy_local/#deploying-the-project-locally-as-a-quarkus-runtime","text":"Now that we have seen a little of the DMN Decision, let's use the combined power of Quarkus ( Learn more here about the Kubernetes-native Java Stack that is the best place for Kogito to run! ) and Kogito to quickly build and deploy the service using Maven commands and also having the option to deploy it to OpenShift as well. The first thing we're going to do is open a terminal (either in VSCode or whatever method you prefer) and validate that you have Maven and Java installed. java -version Expected output should be similar to openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment AdoptOpenJDK-11.0.11+9 (build 11.0.11+9) OpenJDK 64-Bit Server VM AdoptOpenJDK-11.0.11+9 (build 11.0.11+9, mixed mode) mvn -version Expected output should be similar to: Apache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63) Java version: 18.0.2, vendor: Homebrew, runtime: /opt/homebrew/Cellar/openjdk/18.0.2/libexec/openjdk.jdk/Contents/Home Maven home: /opt/homebrew/Cellar/maven/3.8.6/libexec Default locale: en_US, platform encoding: UTF-8 OS name: \"mac os x\", version: \"12.6\", arch: \"aarch64\", family: \"mac\" Now that our environment has the appropriate resources, let's build and run the service locally. Quarkus does this very simply with the command mvn quarkus:dev which will operate the container in development mode. By default it will run on 2 ports when running this command, 8080 and 5005 . These ports can be maintained in the application.properties file in src/main/resources/ or you can modify the mvn quarkus:dev command to load those properties from the command line. If you want to do the command line, you can do mvn quarkus:dev -Dquarkus.http.port=8085 -Ddebug=7007 which would setup your application to run off of port 8085 and be able to attach a remote debugger at 7007 . To change the default host in application.properties add the following line. quarkus . http . port = 8085 After the project's application.properties has been updated, let's start the Quarkus service by running the following command. This command will run the remote debugger (which we are not using in these labs) on port 6006 so that if you already have another application running on 5005 (the default port) it won't give an error. mvn quarkus:dev -Ddebug = 6006 When the service is ready, it will produce a log similar to the below: $ quick-kogito % mvn quarkus:dev -Ddebug=6006 [INFO] Scanning for projects... [INFO] [INFO] --------------------< com.ibm.sample:quick-kogito >--------------------- [INFO] Building quick-kogito 1.0.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- ... 2022-09-29 20:48:34,404 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) Initializing DMN DT Validator... 2022-09-29 20:48:34,405 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) DMN DT Validator initialized. 2022-09-29 20:48:34,405 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) Analysing decision tables in DMN Model 'pricing' ... 2022-09-29 20:48:34,410 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) analysis for decision table 'Base price': 2022-09-29 20:48:34,412 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) Decision Table Analysis of table 'Base price' finished with no messages to be reported. 2022-09-29 20:48:34,494 INFO [org.kie.kog.qua.com.dep.KogitoAssetsProcessor] (build-38) reflectiveEfestoGeneratedClassBuildItem org.kie.kogito.quarkus.common.deployment.KogitoGeneratedSourcesBuildItem@415815e1 __ ____ __ _____ ___ __ ____ ______ --/ __ \\/ / / / _ | / _ \\/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,< / /_/ /\\ \\ --\\___\\_\\____/_/ |_/_/|_/_/|_|\\____/___/ 2022-09-29 20:48:35,728 INFO quick-kogito 1.0.0-SNAPSHOT on JVM started in 2.432s. 2022-09-29 20:48:35,728 INFO Listening on: http://localhost:8085 2022-09-29 20:48:35,728 INFO Profile dev activated. Live Coding activated. 2022-09-29 20:48:35,728 INFO Installed features: [cdi, kogito-decisions, kogito-predictions, kogito-processes, kogito-rules, resteasy-reactive, resteasy-reactive-jackson, smallrye-context-propagation, smallrye-openapi, swagger-ui, vertx]","title":"Deploying the Project locally as a Quarkus runtime"},{"location":"guided_exercises/01_getting_started/01_deploy_local/#using-the-project-that-is-created","text":"Now that your application is created, you can start using it more for live development and change as you go. To access the Quarkus Development console, in the logs, you can Command/Ctrl+click the link in VSCode or go to ()[http://localhost:8085], directly. The development console will show various aspects of your deployment. A quick highlight will be: The list of end points that are available The produced Maven artifact name (in this case quick-kogito 1.0.0-SNAPSHOT) You can modify configurations of your running application from here The Swagger UI and OpenAPI pages to get more information and test out your end points. We will be clicking the Swagger UI link to open the Swagger page to try out our sample DMN. Here you can see your generated Quarkus endpoints that deal with the Decision model. When building a DMN Decision Service with IBM Business Automation Open Edition 9.2.x in Kogito, it will be able to use the DMN Model, coupled with the data model created in it to generate the required end points and inputs. This creates a Domain Specific Decision Service , which is different than how KIE Server worked before in that you would interact with a generic API and insert the objects for execution. By going away from the classpath loading strategy in Kogito, Decision Services are ready to be deployed into a cloud native environment. Your payloads are simpler and easy to reuse across any services as required without having to know exactly how the service is deployed, the objects associated with the deployment and more. The service that was created for the Decision Service is called Pricing Resource and if you click any of the end points you will get more information on them. The first end point to look at is the Post /pricing . This endpoint can be used to execute a decision and return just the data associated with the decison (inputs and outputs). After clicking, you can edit the Domain Specific payload in input that matches your decision input nodes ( Age and Previous incidents ) with values you want to test. Notice that the endpoints use faces in the object names, this matches the expected values that were tied to the inputs in the model as opposed to camelCase or snake_case or PascalCase. This helps with a consistent view across the Kogito stack - from design to runtime. Your result should look similar to the one below. As you can see, the Swagger-UI page produces a few things of note, the first being a Curl command to reproduce locally, the URL you called, and then ultimately the response from the service. The next endpoint to evaluate is the /pricing/dmnresult endpoint which will tell you more about the decision that executed. You can use the same payload from the previous steps to show the differences of what is presented. That payload can be similar to the code below provided in JSON. Click Try it out and repeat the steps from above. The input for the Try it out can be used from below. { \"Age\" : 35 , \"Previous incidents?\" : true } Click Execute with the updated payload. The result will go into more detail about which Decision Nodes were executed against and more. When there is more than one Decision Node available, you can see the results from each node. Now you can play with the DMN model more and see how those changes impact your Quarkus service. Since you're in Dev mode, as you make changes, those changes should be available to you each time you save. Try it out!","title":"Using the project that is created"},{"location":"guided_exercises/01_getting_started/01_deploy_openshift/","text":"Under construction This content will be completed soon! Deploying your service to OpenShift as a Kogito Quarkus Service Now that we have seen a little of the DMN Decision, let's use the combined power of Quarkus ( Learn more here about the Kubernetes-native Java Stack that is the best place for Kogito to run! ) and Kogito to quickly build and deploy the service to OpenShift. If you have already done the deploy locally, you will be able to skip some of these steps and start at Getting Ready for OpenShift Deployment . When deploying to OpenShift you will need to take the following things into account: The application with your Kogito microservices is in a Git repository that is reachable from your OpenShift environment. You have access to the OpenShift web console with the necessary permissions to create and edit KogitoBuild and KogitoRuntime. If implementing Quarkus, update the pom.xml file of your project contains the following dependency for the Quarkus smallrye-health extension. This extension enables the liveness and readiness probes that are required for Quarkus-based projects on OpenShift. If you created the project like in 01_walk_through.md , you will already have this extension added to your deployment. Deploying the Kogito Operator First create a namespace for this to deployed into from the OpenShift console. For example tim-kogito-on-openshift . Next, we need to install the Kogito Operator from the OperatorHub. To do this, login to the OpenShift Cluster console and login as an admin. From here, go to Operator ==> OperatorHub to open the OperatorHub. Search for Kogito and click the IBM BAMOE Kogito Operator tile. From the Operator Form, click Install at the top left to install the Operator. Install it to your namespace you created earlier. This way if there's a version already installed, you're not colliding with it and it is contained to just your namespace. The drawback is that it is limited to this namespace. The installation may take a few minutes, but once it is completed, you will be ready to deploy your service. Click View Operator Success Deploying your first Kogito Application on OpenShift Now that we've got the Operator installed, we're going to see how it works. This lab will walk through the deployment of a Quarkus deployment of the Kogito Service. There will be a collapsed section to refer to if the service was a Spring Boot service instead so there's a reference to it to show how easy it both types of deployments are utilizing the operator. Once the operator is installed, you will click the Kebab icon and Create KogitoBuild to create the build. Since we're developing in Quarkus, our implemetation details handled in yaml are fairly minimal. We will point to a git repository where the service is deployed. In the previous section you created a repository, e.g. github.com/timwuthenow/quick-openshift-kogito .","title":"Deploy in OpenShift"},{"location":"guided_exercises/01_getting_started/01_deploy_openshift/#under-construction","text":"This content will be completed soon!","title":"Under construction"},{"location":"guided_exercises/01_getting_started/01_deploy_openshift/#deploying-your-service-to-openshift-as-a-kogito-quarkus-service","text":"Now that we have seen a little of the DMN Decision, let's use the combined power of Quarkus ( Learn more here about the Kubernetes-native Java Stack that is the best place for Kogito to run! ) and Kogito to quickly build and deploy the service to OpenShift. If you have already done the deploy locally, you will be able to skip some of these steps and start at Getting Ready for OpenShift Deployment . When deploying to OpenShift you will need to take the following things into account: The application with your Kogito microservices is in a Git repository that is reachable from your OpenShift environment. You have access to the OpenShift web console with the necessary permissions to create and edit KogitoBuild and KogitoRuntime. If implementing Quarkus, update the pom.xml file of your project contains the following dependency for the Quarkus smallrye-health extension. This extension enables the liveness and readiness probes that are required for Quarkus-based projects on OpenShift. If you created the project like in 01_walk_through.md , you will already have this extension added to your deployment.","title":"Deploying your service to OpenShift as a Kogito Quarkus Service"},{"location":"guided_exercises/01_getting_started/01_deploy_openshift/#deploying-the-kogito-operator","text":"First create a namespace for this to deployed into from the OpenShift console. For example tim-kogito-on-openshift . Next, we need to install the Kogito Operator from the OperatorHub. To do this, login to the OpenShift Cluster console and login as an admin. From here, go to Operator ==> OperatorHub to open the OperatorHub. Search for Kogito and click the IBM BAMOE Kogito Operator tile. From the Operator Form, click Install at the top left to install the Operator. Install it to your namespace you created earlier. This way if there's a version already installed, you're not colliding with it and it is contained to just your namespace. The drawback is that it is limited to this namespace. The installation may take a few minutes, but once it is completed, you will be ready to deploy your service. Click View Operator Success","title":"Deploying the Kogito Operator"},{"location":"guided_exercises/01_getting_started/01_deploy_openshift/#deploying-your-first-kogito-application-on-openshift","text":"Now that we've got the Operator installed, we're going to see how it works. This lab will walk through the deployment of a Quarkus deployment of the Kogito Service. There will be a collapsed section to refer to if the service was a Spring Boot service instead so there's a reference to it to show how easy it both types of deployments are utilizing the operator. Once the operator is installed, you will click the Kebab icon and Create KogitoBuild to create the build. Since we're developing in Quarkus, our implemetation details handled in yaml are fairly minimal. We will point to a git repository where the service is deployed. In the previous section you created a repository, e.g. github.com/timwuthenow/quick-openshift-kogito .","title":"Deploying your first Kogito Application on OpenShift"},{"location":"guided_exercises/01_getting_started/01_walk_through/","text":"Using Maven to create the project workspace In the previous section, you setup Maven locally in your environment, so you should now have access to all of the mvn commands that are associated with running it. The first thing we're going to create is a project using the proceeding steps: We're going to create the service in Quarkus with the Maven commands below, this will create a Quarkus project called quick-kogito that will be versioned 1.0.0-SNAPSHOT including the extensions kogito-quarkus, dmn, resteasy-reactive-jackson, quarkus-smallrye-openapi, quarkus-smallrye-health which will create a Quarkus DMN project with the openapi components to get the OpenAPI end points easily with health checks when deploying to OpenShift. mvn io.quarkus:quarkus-maven-plugin:2.16.7.Final:create \\ -DprojectGroupId = com.ibm.sample \\ -DprojectArtifactId = quick-kogito \\ -DprojectVersion = 1 .0.0-SNAPSHOT \\ -DplatformVersion = 2 .16.7.Final \\ -Dextensions = kogito-quarkus,dmn,resteasy-reactive-jackson,quarkus-smallrye-openapi,quarkus-smallrye-health When you create this project you should get a bunch of Maven artifacts start to stream in your console that are being pulled and ultimately are left with a console message like the below: [INFO] [INFO] ======================================================================================== [INFO] Your new application has been created in /Users/timwuthenow/techxchange/quick-kogito [INFO] Navigate into this directory and launch your application with mvn quarkus:dev [INFO] Your application will be accessible on http://localhost:8080 [INFO] ======================================================================================== [INFO] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 8.555 s [INFO] Finished at: 2023-07-17T13:35:03-04:00 [INFO] ------------------------------------------------------------------------ NOTE: If you are using an image provided by IBM for labs, the 8080 port location that is labeled here will not work immediately because another service is utilizing this port. We will show how to change this in the next steps. With VSCode installed to your PATH variables, you can open the workspace by doing the below command, otherwise open VSCode and navigate to where you ran the command for quick-kogito to be created at: cd quick-kogito code . From here we can see the workspace's contents and if we expand the contents of /src/main you will see the creation of several artifacts. Within java you will have a GreetingResource.java and within resources you will have an application.properties and pricing.dmn file. These are sample files that can be later modified or deleted, but we will be explore them first in this section, but will do more in later labs around the various end points. Let's first click the pricing.dmn file to open it. When you do so you may be greeted with a message similar to This diagram does not have layout information. Click 'Yes' to compute optimal layout, it takes time according to the diagram size. Click 'No' to proceed without layout. Please save the layout changes once diagram is opened. - if so click Yes to automap the DMN locations. When the diagram opens you will see something similar to below, so we will start exploring it. The DMN is made up of two inputs Age and Previous incidents? , which are used to make the decision, Base price . If you click Age and then click the Properties icon on the right, you will open a pane for the input. Within this pane, you can see information about the input Age , this includes that it is a number and what the input name is. More can be changed around this object, including changing the color of the node, font size, etc. To view the Decision, click the square decision node and select the Edit button to enter the decision for Base Price . From here you will see the Decision Table that is associated with the Base Price decision. From here you will see two (2) input columns ( Age and Previous Incidents ), as well as one output column ( Base price ) all with their types below them. These types are controlled from the properties panel similarly to how they were opened when looking at Age a few steps ago. This decision has 4 different rows that could fire, with a Hit Policy of UNIQUE signified by the U in the top left corner of the table. A decision writer could make any comments they want to the table and have them saved towards the decision here Create a GitHub repository for the project One last thing we're going to do is to create a GitHub repository for this service, so you have somewhere to store our changes and also take advantage of building it into a cloud service running on OpenShift. You can create a repository on GitHub . To do this login to your GitHub username (or create one if you don't have one!) and from your home page and click the green New icon near the top left of the page (or you can navigate directly to here ) Fill out the form with your values Repository Name : quick-openshift-kogito Select Public for now Add a Description if you want Click Create Repository Copy the command for ...or create a new repository on the command line as we're going to take exactly what's in our repository add a mostly empty README.md and push those changes to GitHub. The below command is an example and will not be the exact same as what you have on your repository . Copy yours . When this is done, you are finished with this section. Proceed to either deploying locally or deploying on OpenShift","title":"Project Creation and Walk through"},{"location":"guided_exercises/01_getting_started/01_walk_through/#using-maven-to-create-the-project-workspace","text":"In the previous section, you setup Maven locally in your environment, so you should now have access to all of the mvn commands that are associated with running it. The first thing we're going to create is a project using the proceeding steps: We're going to create the service in Quarkus with the Maven commands below, this will create a Quarkus project called quick-kogito that will be versioned 1.0.0-SNAPSHOT including the extensions kogito-quarkus, dmn, resteasy-reactive-jackson, quarkus-smallrye-openapi, quarkus-smallrye-health which will create a Quarkus DMN project with the openapi components to get the OpenAPI end points easily with health checks when deploying to OpenShift. mvn io.quarkus:quarkus-maven-plugin:2.16.7.Final:create \\ -DprojectGroupId = com.ibm.sample \\ -DprojectArtifactId = quick-kogito \\ -DprojectVersion = 1 .0.0-SNAPSHOT \\ -DplatformVersion = 2 .16.7.Final \\ -Dextensions = kogito-quarkus,dmn,resteasy-reactive-jackson,quarkus-smallrye-openapi,quarkus-smallrye-health When you create this project you should get a bunch of Maven artifacts start to stream in your console that are being pulled and ultimately are left with a console message like the below: [INFO] [INFO] ======================================================================================== [INFO] Your new application has been created in /Users/timwuthenow/techxchange/quick-kogito [INFO] Navigate into this directory and launch your application with mvn quarkus:dev [INFO] Your application will be accessible on http://localhost:8080 [INFO] ======================================================================================== [INFO] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 8.555 s [INFO] Finished at: 2023-07-17T13:35:03-04:00 [INFO] ------------------------------------------------------------------------ NOTE: If you are using an image provided by IBM for labs, the 8080 port location that is labeled here will not work immediately because another service is utilizing this port. We will show how to change this in the next steps. With VSCode installed to your PATH variables, you can open the workspace by doing the below command, otherwise open VSCode and navigate to where you ran the command for quick-kogito to be created at: cd quick-kogito code . From here we can see the workspace's contents and if we expand the contents of /src/main you will see the creation of several artifacts. Within java you will have a GreetingResource.java and within resources you will have an application.properties and pricing.dmn file. These are sample files that can be later modified or deleted, but we will be explore them first in this section, but will do more in later labs around the various end points. Let's first click the pricing.dmn file to open it. When you do so you may be greeted with a message similar to This diagram does not have layout information. Click 'Yes' to compute optimal layout, it takes time according to the diagram size. Click 'No' to proceed without layout. Please save the layout changes once diagram is opened. - if so click Yes to automap the DMN locations. When the diagram opens you will see something similar to below, so we will start exploring it. The DMN is made up of two inputs Age and Previous incidents? , which are used to make the decision, Base price . If you click Age and then click the Properties icon on the right, you will open a pane for the input. Within this pane, you can see information about the input Age , this includes that it is a number and what the input name is. More can be changed around this object, including changing the color of the node, font size, etc. To view the Decision, click the square decision node and select the Edit button to enter the decision for Base Price . From here you will see the Decision Table that is associated with the Base Price decision. From here you will see two (2) input columns ( Age and Previous Incidents ), as well as one output column ( Base price ) all with their types below them. These types are controlled from the properties panel similarly to how they were opened when looking at Age a few steps ago. This decision has 4 different rows that could fire, with a Hit Policy of UNIQUE signified by the U in the top left corner of the table. A decision writer could make any comments they want to the table and have them saved towards the decision here","title":"Using Maven to create the project workspace"},{"location":"guided_exercises/01_getting_started/01_walk_through/#create-a-github-repository-for-the-project","text":"One last thing we're going to do is to create a GitHub repository for this service, so you have somewhere to store our changes and also take advantage of building it into a cloud service running on OpenShift. You can create a repository on GitHub . To do this login to your GitHub username (or create one if you don't have one!) and from your home page and click the green New icon near the top left of the page (or you can navigate directly to here ) Fill out the form with your values Repository Name : quick-openshift-kogito Select Public for now Add a Description if you want Click Create Repository Copy the command for ...or create a new repository on the command line as we're going to take exactly what's in our repository add a mostly empty README.md and push those changes to GitHub. The below command is an example and will not be the exact same as what you have on your repository . Copy yours . When this is done, you are finished with this section. Proceed to either deploying locally or deploying on OpenShift","title":"Create a GitHub repository for the project"},{"location":"guided_exercises/01_getting_started/introduction/","text":"Introduction This section will focus on how to setup your first Kogito projects from scratch. The method we're going to use for this is through the use of Maven to create it from an empty workspace, produce a DMN model and make it available both locally and how to use deploy it to OpenShift. Initial project setup and walk through Deploy locally Deploy to OpenShift","title":"Introduction"},{"location":"guided_exercises/01_getting_started/introduction/#introduction","text":"This section will focus on how to setup your first Kogito projects from scratch. The method we're going to use for this is through the use of Maven to create it from an empty workspace, produce a DMN model and make it available both locally and how to use deploy it to OpenShift. Initial project setup and walk through Deploy locally Deploy to OpenShift","title":"Introduction"},{"location":"guided_exercises/02_techXchange/02_exploring_canvas_features-new/","text":"Exploring the Features of BAMOE Canvas In this section, you will explore the base samples of both a DMN model and a BPMN model and some of the features found within them. BAMOE Canvas is a lightweight client-side tool that can be used to maintain business process and decision models for our Business Automation projects. Getting Started with BAMOE Canvas On your workstation, open Chrome. Click the bookmark for BAMOE Canvas or navigate to http://localhost:9090 to open without the bookmark. All instances of BAMOE Canvas for this lab are deployed locally using a docker compose that builds the local environment. At the welcome screen, you will see several areas: The top two tiles are for creation and importing of projects. The top bar has 4 areas: Dev deployments, Extended Services connection, Settings, and Profile configuration. The lower section displays your locally imported projects/models. The top bar has 4 areas: Dev deployments \u2013 any sample deployment done by this instance of Canvas to a connected Kubernetes or OpenShift cluster The radar logo is the connection to Extended Services which provides the DMN model runner for sample execution The gear \u2699\ufe0f logo provides settings for your Canvas instance, including the version of DMN modeling you use (Classic in DMN 1.2 for releases 9.0 and before, DMN 1.5 is the default for 9.1 going forward). The location of the CORS proxy so that your Canvas instance can interact with your Git provider. Lastly, the location of the Extended Services service. Lastly, the human logo is for configuring your connected profiles for the Canvas environment. This will be for both Git providers and Kubernetes/OpenShift environments. We will work with this more in the lab later. The lower section is where your locally imported projects/models will be displayed, not your neighbor in the lab or co-workers. Everything is isolated to your instance. This makes it so that the only projects present are the ones you import. If skeptical, click Try Sample for Workflow and then open an incognito window and go to Canvas and you will see no models in the Recent model section. Exploring Canvas with Decisions In this section, we are briefly going to touch on some of the features of the Canvas editors for Workflow and DMN. The first thing we're going to do is look at the DMN sample and get a quick introduction to it. NOTE: Last year's TechXchange lab focused on DMN, refer to the https://bamoe.university website, guided exercises for TechXchange 1717, to learn more about decision automation with DMN. Click Try sample to open the sample DMN model. When you open the Sample, you will see a DMN 1.5 model for Loan Pre-qualification. Explore the different elements of the DMN diagram: Input Nodes: \"Applicant Data\" and \"Credit Score\" Decision Nodes: \"Loan Pre-Qualification\" Business Knowledge Model Nodes (BKM) Click on the \"Credit Score Rating\" square and then click the Edit button to visualize the logic in the Credit Score Rating decision. In the decision node, observe the inputs and outputs, as well as the logic required to define the Credit Score and the rating. Let us see this decision in action. On the menu bar, at the right-hand side of the screen, click on the button Run. An automatically generated form allows you to try out the DMN model's inputs (Applicant Data, Requested Product and Credit Score) and do a live evaluation the result of the decision. Let us try evaluating one of the decisions in the diagram. We will check the Credit Score Rating decision, which only requires one input - the Credit Score. Fill in a credit score, having in mind the values in the decision table, and observe at the right side the output of the decision as you type. This capability allows you to work on your DMN, for example by changing the decision table and, without any deployments, being able to readily evaluate the outcomes on using the values of your preference. Let's do a change on the decision table to observe this feature in Canvas: under Credit Score's FICO input, write a number in between 700 and 750. If you change the value in the second column for the score between 700 and 750 from \"Good\" to \"Excellent\" or \"Fair\", observe that after you update the cells, the decision Credit Score Rating output will reflect the change immediately. Exploring the BPMN Sample Return to the BAMOE Canvas home screen by clicking the IBM logo in the top left corner. Click on the \"Try sample\" option under the BPMN section. Explore the sample BPMN process, which goes through a simplified hiring process. Observe the following actions: at the start an automated evaluation of the candidate (in this case, through a script task) immediately determines if a candidate is eligible; afterwards, a base offer gets created based on the inputs with the candidate's background. At this point, the offer is internal to the process and not visible to the candidate. Next, there are two user tasks: an HR interview and IT interview. Both have boundary timer events that adds an SLA for automatically denying the hiring due to interviews not occurring in time. Explore the various nodes and their settings by clicking on a node and using the properties panel. To open the properties panel, clicking on the pencil and paper icon on the right side of the screen. For example, click on HR Interview and then click the pencil/paper icon: In the panel, identify the potential owners of the task and the data assignments (input/output variables) associated with this Human Task. The HR Interview task is assigned to the actor jdoe and has 5 data inputs and 4 data outputs. At this point, this project in Canvas consists only of a BPMN file sitting in the storage of the browser, meaning that any changes can be lost if the local storage in the browser is cleared for some reason (e.g. a browser window in incognito mode does not show the same files you have in your current environment). So, it is important to work with projects and have them synchronizing it with our Git provider. Let us now create a new project using the Accelerator and assure it is versioned in GitHub.","title":"Exploring Canvas"},{"location":"guided_exercises/02_techXchange/02_exploring_canvas_features-new/#exploring-the-features-of-bamoe-canvas","text":"In this section, you will explore the base samples of both a DMN model and a BPMN model and some of the features found within them. BAMOE Canvas is a lightweight client-side tool that can be used to maintain business process and decision models for our Business Automation projects.","title":"Exploring the Features of BAMOE Canvas"},{"location":"guided_exercises/02_techXchange/02_exploring_canvas_features-new/#getting-started-with-bamoe-canvas","text":"On your workstation, open Chrome. Click the bookmark for BAMOE Canvas or navigate to http://localhost:9090 to open without the bookmark. All instances of BAMOE Canvas for this lab are deployed locally using a docker compose that builds the local environment. At the welcome screen, you will see several areas: The top two tiles are for creation and importing of projects. The top bar has 4 areas: Dev deployments, Extended Services connection, Settings, and Profile configuration. The lower section displays your locally imported projects/models. The top bar has 4 areas: Dev deployments \u2013 any sample deployment done by this instance of Canvas to a connected Kubernetes or OpenShift cluster The radar logo is the connection to Extended Services which provides the DMN model runner for sample execution The gear \u2699\ufe0f logo provides settings for your Canvas instance, including the version of DMN modeling you use (Classic in DMN 1.2 for releases 9.0 and before, DMN 1.5 is the default for 9.1 going forward). The location of the CORS proxy so that your Canvas instance can interact with your Git provider. Lastly, the location of the Extended Services service. Lastly, the human logo is for configuring your connected profiles for the Canvas environment. This will be for both Git providers and Kubernetes/OpenShift environments. We will work with this more in the lab later. The lower section is where your locally imported projects/models will be displayed, not your neighbor in the lab or co-workers. Everything is isolated to your instance. This makes it so that the only projects present are the ones you import. If skeptical, click Try Sample for Workflow and then open an incognito window and go to Canvas and you will see no models in the Recent model section.","title":"Getting Started with BAMOE Canvas"},{"location":"guided_exercises/02_techXchange/02_exploring_canvas_features-new/#exploring-canvas-with-decisions","text":"In this section, we are briefly going to touch on some of the features of the Canvas editors for Workflow and DMN. The first thing we're going to do is look at the DMN sample and get a quick introduction to it. NOTE: Last year's TechXchange lab focused on DMN, refer to the https://bamoe.university website, guided exercises for TechXchange 1717, to learn more about decision automation with DMN. Click Try sample to open the sample DMN model. When you open the Sample, you will see a DMN 1.5 model for Loan Pre-qualification. Explore the different elements of the DMN diagram: Input Nodes: \"Applicant Data\" and \"Credit Score\" Decision Nodes: \"Loan Pre-Qualification\" Business Knowledge Model Nodes (BKM) Click on the \"Credit Score Rating\" square and then click the Edit button to visualize the logic in the Credit Score Rating decision. In the decision node, observe the inputs and outputs, as well as the logic required to define the Credit Score and the rating. Let us see this decision in action. On the menu bar, at the right-hand side of the screen, click on the button Run. An automatically generated form allows you to try out the DMN model's inputs (Applicant Data, Requested Product and Credit Score) and do a live evaluation the result of the decision. Let us try evaluating one of the decisions in the diagram. We will check the Credit Score Rating decision, which only requires one input - the Credit Score. Fill in a credit score, having in mind the values in the decision table, and observe at the right side the output of the decision as you type. This capability allows you to work on your DMN, for example by changing the decision table and, without any deployments, being able to readily evaluate the outcomes on using the values of your preference. Let's do a change on the decision table to observe this feature in Canvas: under Credit Score's FICO input, write a number in between 700 and 750. If you change the value in the second column for the score between 700 and 750 from \"Good\" to \"Excellent\" or \"Fair\", observe that after you update the cells, the decision Credit Score Rating output will reflect the change immediately.","title":"Exploring Canvas with Decisions"},{"location":"guided_exercises/02_techXchange/02_exploring_canvas_features-new/#exploring-the-bpmn-sample","text":"Return to the BAMOE Canvas home screen by clicking the IBM logo in the top left corner. Click on the \"Try sample\" option under the BPMN section. Explore the sample BPMN process, which goes through a simplified hiring process. Observe the following actions: at the start an automated evaluation of the candidate (in this case, through a script task) immediately determines if a candidate is eligible; afterwards, a base offer gets created based on the inputs with the candidate's background. At this point, the offer is internal to the process and not visible to the candidate. Next, there are two user tasks: an HR interview and IT interview. Both have boundary timer events that adds an SLA for automatically denying the hiring due to interviews not occurring in time. Explore the various nodes and their settings by clicking on a node and using the properties panel. To open the properties panel, clicking on the pencil and paper icon on the right side of the screen. For example, click on HR Interview and then click the pencil/paper icon: In the panel, identify the potential owners of the task and the data assignments (input/output variables) associated with this Human Task. The HR Interview task is assigned to the actor jdoe and has 5 data inputs and 4 data outputs. At this point, this project in Canvas consists only of a BPMN file sitting in the storage of the browser, meaning that any changes can be lost if the local storage in the browser is cleared for some reason (e.g. a browser window in incognito mode does not show the same files you have in your current environment). So, it is important to work with projects and have them synchronizing it with our Git provider. Let us now create a new project using the Accelerator and assure it is versioned in GitHub.","title":"Exploring the BPMN Sample"},{"location":"guided_exercises/02_techXchange/02_exploring_canvas_features/","text":"Exploring the Features of BAMOE Canvas In this section, you will explore the base samples of both a DMN model and a BPMN model and some of the features found within them. BAMOE Canvas is a lightweight client-side tool that can be used to maintain business process and decision models for our Business Automation projects. Getting Started with BAMOE Canvas On your workstation, open Chrome. Click the bookmark for BAMOE Canvas or navigate to http://localhost:9090 to open without the bookmark. All instances of BAMOE Canvas for this lab are deployed locally using a docker compose that builds the local environment. At the welcome screen, you will see several areas: The top two tiles are for creation and importing of projects. The top bar has 4 areas: Dev deployments, Extended Services connection, Settings, and Profile configuration. The lower section displays your locally imported projects/models. The top bar has 4 areas: Dev deployments \u2013 any sample deployment done by this instance of Canvas to a connected Kubernetes or OpenShift cluster The radar logo is the connection to Extended Services which provides the DMN model runner for sample execution The gear \u2699\ufe0f logo provides settings for your Canvas instance, including the version of DMN modeling you use (Classic in DMN 1.2 for releases 9.0 and before, DMN 1.5 is the default for 9.1 going forward). The location of the CORS proxy so that your Canvas instance can interact with your Git provider. Lastly, the location of the Extended Services service. Lastly, the human logo is for configuring your connected profiles for the Canvas environment. This will be for both Git providers and Kubernetes/OpenShift environments. We will work with this more in the lab later. The lower section is where your locally imported projects/models will be displayed, not your neighbor in the lab or co-workers. Everything is isolated to your instance. This makes it so that the only projects present are the ones you import. If skeptical, click Try Sample for Workflow and then open an incognito window and go to Canvas and you will see no models in the Recent model section. Exploring Canvas with Decisions In this section, we are briefly going to touch on some of the features of the Canvas editors for Workflow and DMN. The first thing we\u2019re going to do is look at the DMN sample and get a quick introduction to it. NOTE: Last year\u2019s TechXchange lab focused on DMN, refer to the https://bamoe.university website, guided exercises for TechXchange 1717, to learn more about decision automation with DMN. Click Try sample to open the sample DMN model. When you open the Sample, you will see a DMN 1.5 model for Loan Pre-qualification. Explore the different elements of the DMN diagram: Input Nodes: \"Applicant Data\" and \"Credit Score\" Decision Nodes: \"Loan Pre-Qualification\" Business Knowledge Model Nodes (BKM) Click on the \"Credit Score Rating\" square and then click the Edit button to visualize the logic in the Credit Score Rating decision. In the decision node, observe the inputs and outputs, as well as the logic required to define the Credit Score and the rating. Let us see this decision in action. On the menu bar, at the right-hand side of the screen, click on the button Run. An automatically generated form allows you to try out the DMN model\u2019s inputs (Applicant Data, Requested Product and Credit Score) and do a live evaluation the result of the decision. Let us try evaluating one of the decisions in the diagram. We will check the Credit Score Rating decision, which only requires one input - the Credit Score. Fill in a credit score, having in mind the values in the decision table, and observe at the right side the output of the decision as you type. This capability allows you to work on your DMN, for example by changing the decision table and, without any deployments, being able to readily evaluate the outcomes on using the values of your preference. Let's do a change on the decision table to observe this feature in Canvas: under Credit Score\u2019s FICO input, write a number in between 700 and 750. If you change the value in the second column for the score between 700 and 750 from \u201cGood\u201d to \u201cExcellent\u201d or \u201cFair\u201d, observe that after you update the cells, the decision Credit Score Rating output will reflect the change immediately. Exploring the BPMN Sample Return to the BAMOE Canvas home screen by clicking the IBM logo in the top left corner. Click on the \"Try sample\" option under the BPMN section. Explore the sample BPMN process, which goes through a simplified hiring process. Observe the following actions: at the start an automated evaluation of the candidate (in this case, through a script task) immediately determines if a candidate is eligible; afterwards, a base offer gets created based on the inputs with the candidate's background. At this point, the offer is internal to the process and not visible to the candidate. Next, there are two user tasks: an HR interview and IT interview. Both have boundary timer events that adds an SLA for automatically denying the hiring due to interviews not occurring in time. Explore the various nodes and their settings by clicking on a node and using the properties panel. To open the properties panel, clicking on the pencil and paper icon on the right side of the screen. For example, click on HR Interview and then click the pencil/paper icon: In the panel, identify the potential owners of the task and the data assignments (input/output variables) associated with this Human Task. The HR Interview task is assigned to the actor jdoe and has 5 data inputs and 4 data outputs. At this point, this project in Canvas consists only of a BPMN file sitting in the storage of the browser, meaning that any changes can be lost if the local storage in the browser is cleared for some reason (e.g. a browser window in incognito mode does not show the same files you have in your current environment). So, it is important to work with projects and have them synchronizing it with our Git provider. Let us now create a new project using the Accelerator and assure it is versioned in GitHub.","title":"Exploring the Features of BAMOE Canvas"},{"location":"guided_exercises/02_techXchange/02_exploring_canvas_features/#exploring-the-features-of-bamoe-canvas","text":"In this section, you will explore the base samples of both a DMN model and a BPMN model and some of the features found within them. BAMOE Canvas is a lightweight client-side tool that can be used to maintain business process and decision models for our Business Automation projects.","title":"Exploring the Features of BAMOE Canvas"},{"location":"guided_exercises/02_techXchange/02_exploring_canvas_features/#getting-started-with-bamoe-canvas","text":"On your workstation, open Chrome. Click the bookmark for BAMOE Canvas or navigate to http://localhost:9090 to open without the bookmark. All instances of BAMOE Canvas for this lab are deployed locally using a docker compose that builds the local environment. At the welcome screen, you will see several areas: The top two tiles are for creation and importing of projects. The top bar has 4 areas: Dev deployments, Extended Services connection, Settings, and Profile configuration. The lower section displays your locally imported projects/models. The top bar has 4 areas: Dev deployments \u2013 any sample deployment done by this instance of Canvas to a connected Kubernetes or OpenShift cluster The radar logo is the connection to Extended Services which provides the DMN model runner for sample execution The gear \u2699\ufe0f logo provides settings for your Canvas instance, including the version of DMN modeling you use (Classic in DMN 1.2 for releases 9.0 and before, DMN 1.5 is the default for 9.1 going forward). The location of the CORS proxy so that your Canvas instance can interact with your Git provider. Lastly, the location of the Extended Services service. Lastly, the human logo is for configuring your connected profiles for the Canvas environment. This will be for both Git providers and Kubernetes/OpenShift environments. We will work with this more in the lab later. The lower section is where your locally imported projects/models will be displayed, not your neighbor in the lab or co-workers. Everything is isolated to your instance. This makes it so that the only projects present are the ones you import. If skeptical, click Try Sample for Workflow and then open an incognito window and go to Canvas and you will see no models in the Recent model section.","title":"Getting Started with BAMOE Canvas"},{"location":"guided_exercises/02_techXchange/02_exploring_canvas_features/#exploring-canvas-with-decisions","text":"In this section, we are briefly going to touch on some of the features of the Canvas editors for Workflow and DMN. The first thing we\u2019re going to do is look at the DMN sample and get a quick introduction to it. NOTE: Last year\u2019s TechXchange lab focused on DMN, refer to the https://bamoe.university website, guided exercises for TechXchange 1717, to learn more about decision automation with DMN. Click Try sample to open the sample DMN model. When you open the Sample, you will see a DMN 1.5 model for Loan Pre-qualification. Explore the different elements of the DMN diagram: Input Nodes: \"Applicant Data\" and \"Credit Score\" Decision Nodes: \"Loan Pre-Qualification\" Business Knowledge Model Nodes (BKM) Click on the \"Credit Score Rating\" square and then click the Edit button to visualize the logic in the Credit Score Rating decision. In the decision node, observe the inputs and outputs, as well as the logic required to define the Credit Score and the rating. Let us see this decision in action. On the menu bar, at the right-hand side of the screen, click on the button Run. An automatically generated form allows you to try out the DMN model\u2019s inputs (Applicant Data, Requested Product and Credit Score) and do a live evaluation the result of the decision. Let us try evaluating one of the decisions in the diagram. We will check the Credit Score Rating decision, which only requires one input - the Credit Score. Fill in a credit score, having in mind the values in the decision table, and observe at the right side the output of the decision as you type. This capability allows you to work on your DMN, for example by changing the decision table and, without any deployments, being able to readily evaluate the outcomes on using the values of your preference. Let's do a change on the decision table to observe this feature in Canvas: under Credit Score\u2019s FICO input, write a number in between 700 and 750. If you change the value in the second column for the score between 700 and 750 from \u201cGood\u201d to \u201cExcellent\u201d or \u201cFair\u201d, observe that after you update the cells, the decision Credit Score Rating output will reflect the change immediately.","title":"Exploring Canvas with Decisions"},{"location":"guided_exercises/02_techXchange/02_exploring_canvas_features/#exploring-the-bpmn-sample","text":"Return to the BAMOE Canvas home screen by clicking the IBM logo in the top left corner. Click on the \"Try sample\" option under the BPMN section. Explore the sample BPMN process, which goes through a simplified hiring process. Observe the following actions: at the start an automated evaluation of the candidate (in this case, through a script task) immediately determines if a candidate is eligible; afterwards, a base offer gets created based on the inputs with the candidate's background. At this point, the offer is internal to the process and not visible to the candidate. Next, there are two user tasks: an HR interview and IT interview. Both have boundary timer events that adds an SLA for automatically denying the hiring due to interviews not occurring in time. Explore the various nodes and their settings by clicking on a node and using the properties panel. To open the properties panel, clicking on the pencil and paper icon on the right side of the screen. For example, click on HR Interview and then click the pencil/paper icon: In the panel, identify the potential owners of the task and the data assignments (input/output variables) associated with this Human Task. The HR Interview task is assigned to the actor jdoe and has 5 data inputs and 4 data outputs. At this point, this project in Canvas consists only of a BPMN file sitting in the storage of the browser, meaning that any changes can be lost if the local storage in the browser is cleared for some reason (e.g. a browser window in incognito mode does not show the same files you have in your current environment). So, it is important to work with projects and have them synchronizing it with our Git provider. Let us now create a new project using the Accelerator and assure it is versioned in GitHub.","title":"Exploring the BPMN Sample"},{"location":"guided_exercises/02_techXchange/03_creating_versioning_business_services/","text":"Creating and Versioning Business Services: Customized Accelerators and Git Synchronization In this section, we'll create a new project using the Accelerator and synchronize it with our Git provider. Applying the Accelerator The first thing we're going to do is apply the accelerator to create an in-browser storage project that can utilize the Kogito architecture. You can apply either of the bottom accelerators to work with the process engine. This will open a modal asking you to confirm that you want to apply the accelerator. Click Apply . This will also create an initial git commit within the file system, so it will ask for an initial commit message. Change it if you want and click Commit . Afterwards, at the top banner you will see a message stating \"Successfully applied TechXchange Accelerator Kogito Powered jBPM Accelerator\" or a similar message. Synchronizing with Git To synchronize your project with GitHub, click the Share button, then at the bottom there is a pull down for Authentication Source , choose your Git user. After selecting your user, click Create GitHub repository... Change the name of the repository to something meaningful for you and then click Create . BAMOE Canvas now synchronizes your project with your Git provider, at the repository you created. This action creates a new project in the repository, following the permissions you chose for it (private or public). Note: A private repository can only be accessed by the owner or others with authorization configured in GitHub - in this case, only you would be able to access it. Access to your private repositories can be controlled using the possibilities of the git provider being used. It's important to note that all operations are being tracked by git as being made by your user (the token user, retrieved from GitHub); This means that all changes on the project in this browser session from now on are tracked by the commits executed by your user. No custom security is needed - we are just relying on the solid authorization capabilities that are already there for us to take advantage of!","title":"Versioning Business Services"},{"location":"guided_exercises/02_techXchange/03_creating_versioning_business_services/#creating-and-versioning-business-services-customized-accelerators-and-git-synchronization","text":"In this section, we'll create a new project using the Accelerator and synchronize it with our Git provider.","title":"Creating and Versioning Business Services: Customized Accelerators and Git Synchronization"},{"location":"guided_exercises/02_techXchange/03_creating_versioning_business_services/#applying-the-accelerator","text":"The first thing we're going to do is apply the accelerator to create an in-browser storage project that can utilize the Kogito architecture. You can apply either of the bottom accelerators to work with the process engine. This will open a modal asking you to confirm that you want to apply the accelerator. Click Apply . This will also create an initial git commit within the file system, so it will ask for an initial commit message. Change it if you want and click Commit . Afterwards, at the top banner you will see a message stating \"Successfully applied TechXchange Accelerator Kogito Powered jBPM Accelerator\" or a similar message.","title":"Applying the Accelerator"},{"location":"guided_exercises/02_techXchange/03_creating_versioning_business_services/#synchronizing-with-git","text":"To synchronize your project with GitHub, click the Share button, then at the bottom there is a pull down for Authentication Source , choose your Git user. After selecting your user, click Create GitHub repository... Change the name of the repository to something meaningful for you and then click Create . BAMOE Canvas now synchronizes your project with your Git provider, at the repository you created. This action creates a new project in the repository, following the permissions you chose for it (private or public). Note: A private repository can only be accessed by the owner or others with authorization configured in GitHub - in this case, only you would be able to access it. Access to your private repositories can be controlled using the possibilities of the git provider being used. It's important to note that all operations are being tracked by git as being made by your user (the token user, retrieved from GitHub); This means that all changes on the project in this browser session from now on are tracked by the commits executed by your user. No custom security is needed - we are just relying on the solid authorization capabilities that are already there for us to take advantage of!","title":"Synchronizing with Git"},{"location":"guided_exercises/02_techXchange/04_running_on_openshift/","text":"Running on OpenShift using the Canvas Dev Deployment The dev deployment is a feature in Canvas that allows developers to quickly run their business assets and business services on OpenShift, as a fast and efficient approach for using or sharing the service APIs with other team members for evaluation. Some benefits of this capabilities are: Deploy with a click: Easily deploy your business service to a local or remote Kubernetes or OpenShift environment directly from the Canvas web tool. Real-time updates: See changes to your project reflected immediately in the running application, for faster iteration and testing. Simplified development: Streamline your development process by eliminating the need for complex deployment procedures. Have in mind that this capability is not intended as a strategy for production-grade deployment. For production deployments, consider using proper deployment strategies for your Kubernetes / OpenShift environment. By default, Canvas offers options of template application and images for the Dev Deployment. Having in mind the flexibility, users who do not want to rely on the default assets for Dev Deployments, can benefit from the Canvas capability that allows using your own customized image and template projects, but is not designed to replace a proper production release and should not be treated as such. Configuring OpenShift Integration Log in to OpenShift with the command line. The easiest way is to use the OpenShift token bookmark and click keycloak and use your student## with password ibmrhocp (e.g. student01:ibmrhocp) Copy the oc login command from the requested token to login, e.g. oc login --token=sha~bd17d73178ecf020bc7138509f620cb4f6fdbd15 --server=https://api.a_real_openshift.io:6443 In Canvas, open the connection settings by clicking on the user icon (top right corner) to open the Connected accounts modal. Click the + Add link to add our OpenShift environment. For authenticating with OpenShift, use the Token and Host from Step 1, and the namespace will be either student##-namespace if using the dedicated OpenShift cluster or if using the Red Hat OpenShift Developer Sandbox will be yourRedHatUser-dev (e.g. timothywuthenow-dev). Validate a successful connection to your chosen environment. A message like the one below is similar to what you want to see. Deploying the Service Now from your model\u2019s screen, you can deploy a very developer-oriented service of the process. This would deploy a Quarkus service running the jBPM powered by Kogito engine with the rest endpoints if you use the blank image. There will be no persistence, task or management consoles, but you will be able to test how the process works a bit from a Rest/Swagger perspective. To do this, click Deploy. Select your project's name and click Deploy \"[your-project-name]\" . On the modal that opens up, click Confirm to deploy your sample service. Deployment can take about 30 seconds to over a minute depending on how many things are on-going on the console. You can monitor it from the OpenShift console. The Developer view from the console allows for the most visual representation of the pod being deployed and coming up. To access it go to https://ibm.biz/dev-ocp and select your namespace (1) and then click Topology (2) on the left hand side of the OpenShift console screen. When deployed, you will see the Topology evolve, adding a new pod to your namespace. Click on the pod and then click on View logs . Look at the logs and see if your service is deployed. You want to see a message similar to the below INFO: Profile dev activated INFO: Installed features: [cdi, kogito-decisions,kogito-processes, kogito-rules] Return back to Canvas, and if it's no longer visible, click Dev Deployments to access your service. This will take you to a Swagger-UI endpoint of your service, which as previously mentioned is quite standalone, but is great for a quick test or seeing expected behaviors. Click on Post /hiring which is the second generated endpoint \u2013 all of this was done and created from a BPMN diagram and the Kogito and jBPM projects worked together to create a domain specific end point that we can quickly try out. You will see that this has a sample input with 3 process variables (skills, candidate, experience). Click on Post /hiring which is the second generated endpoint. Put in sample data, and click the Execute button. Sample data can include, a skill of Java, a candidate name of any and experience set to an integer, like 1 or 2. An Example is below: { \"candidate\" : \"Tim\" , \"experience\" : 2 , \"skills\" : \"Java\" } When you do this, you will get a return similar to the one below in the screenshot kicking off the process. You can explore more with the Swagger-UI using the process-id that gets generated to find the tasks and use the user jdoe to do the tasks, but we will see more of that in the next section. Before closing out this section, return to Canvas to quickly explore the process model once more now that you\u2019ve interacted with the process and can learn a little more how the evolution of IBM Business Automation Manager Open Editions 9 has evolved to making process runtimes with the Kogito architecture smarter and easier to use. Make sure you are not clicked any where on the process diagram and then click the properties icon shown by the pencil and paper icon. Scroll down and click Process Data to expand where the process variables are defined. On them you will see a new feature to the BPMN editor called Tags. Tags specify how the data for the process variable will be used, is it an input or output, is it internal, is it read-only, etc. These are used for many of the features for generating the process\u2019s Swagger-UI domain specific endpoints, and also what gets returned based on selection and each variable can have multiple tags if necessary. This makes interacting with your process right away so much easier since you can start building top-down for your process design and get it deployed rapidly! Explore the tags and see if you can figure out why skills, experience and candidate were all a part of the example schema for the start process end point. In the next section we will go into more detail of using and interacting with the process.","title":"Running sample on OpenShift"},{"location":"guided_exercises/02_techXchange/04_running_on_openshift/#running-on-openshift-using-the-canvas-dev-deployment","text":"The dev deployment is a feature in Canvas that allows developers to quickly run their business assets and business services on OpenShift, as a fast and efficient approach for using or sharing the service APIs with other team members for evaluation. Some benefits of this capabilities are: Deploy with a click: Easily deploy your business service to a local or remote Kubernetes or OpenShift environment directly from the Canvas web tool. Real-time updates: See changes to your project reflected immediately in the running application, for faster iteration and testing. Simplified development: Streamline your development process by eliminating the need for complex deployment procedures. Have in mind that this capability is not intended as a strategy for production-grade deployment. For production deployments, consider using proper deployment strategies for your Kubernetes / OpenShift environment. By default, Canvas offers options of template application and images for the Dev Deployment. Having in mind the flexibility, users who do not want to rely on the default assets for Dev Deployments, can benefit from the Canvas capability that allows using your own customized image and template projects, but is not designed to replace a proper production release and should not be treated as such.","title":"Running on OpenShift using the Canvas Dev Deployment"},{"location":"guided_exercises/02_techXchange/04_running_on_openshift/#configuring-openshift-integration","text":"Log in to OpenShift with the command line. The easiest way is to use the OpenShift token bookmark and click keycloak and use your student## with password ibmrhocp (e.g. student01:ibmrhocp) Copy the oc login command from the requested token to login, e.g. oc login --token=sha~bd17d73178ecf020bc7138509f620cb4f6fdbd15 --server=https://api.a_real_openshift.io:6443 In Canvas, open the connection settings by clicking on the user icon (top right corner) to open the Connected accounts modal. Click the + Add link to add our OpenShift environment. For authenticating with OpenShift, use the Token and Host from Step 1, and the namespace will be either student##-namespace if using the dedicated OpenShift cluster or if using the Red Hat OpenShift Developer Sandbox will be yourRedHatUser-dev (e.g. timothywuthenow-dev). Validate a successful connection to your chosen environment. A message like the one below is similar to what you want to see.","title":"Configuring OpenShift Integration"},{"location":"guided_exercises/02_techXchange/04_running_on_openshift/#deploying-the-service","text":"Now from your model\u2019s screen, you can deploy a very developer-oriented service of the process. This would deploy a Quarkus service running the jBPM powered by Kogito engine with the rest endpoints if you use the blank image. There will be no persistence, task or management consoles, but you will be able to test how the process works a bit from a Rest/Swagger perspective. To do this, click Deploy. Select your project's name and click Deploy \"[your-project-name]\" . On the modal that opens up, click Confirm to deploy your sample service. Deployment can take about 30 seconds to over a minute depending on how many things are on-going on the console. You can monitor it from the OpenShift console. The Developer view from the console allows for the most visual representation of the pod being deployed and coming up. To access it go to https://ibm.biz/dev-ocp and select your namespace (1) and then click Topology (2) on the left hand side of the OpenShift console screen. When deployed, you will see the Topology evolve, adding a new pod to your namespace. Click on the pod and then click on View logs . Look at the logs and see if your service is deployed. You want to see a message similar to the below INFO: Profile dev activated INFO: Installed features: [cdi, kogito-decisions,kogito-processes, kogito-rules] Return back to Canvas, and if it's no longer visible, click Dev Deployments to access your service. This will take you to a Swagger-UI endpoint of your service, which as previously mentioned is quite standalone, but is great for a quick test or seeing expected behaviors. Click on Post /hiring which is the second generated endpoint \u2013 all of this was done and created from a BPMN diagram and the Kogito and jBPM projects worked together to create a domain specific end point that we can quickly try out. You will see that this has a sample input with 3 process variables (skills, candidate, experience). Click on Post /hiring which is the second generated endpoint. Put in sample data, and click the Execute button. Sample data can include, a skill of Java, a candidate name of any and experience set to an integer, like 1 or 2. An Example is below: { \"candidate\" : \"Tim\" , \"experience\" : 2 , \"skills\" : \"Java\" } When you do this, you will get a return similar to the one below in the screenshot kicking off the process. You can explore more with the Swagger-UI using the process-id that gets generated to find the tasks and use the user jdoe to do the tasks, but we will see more of that in the next section. Before closing out this section, return to Canvas to quickly explore the process model once more now that you\u2019ve interacted with the process and can learn a little more how the evolution of IBM Business Automation Manager Open Editions 9 has evolved to making process runtimes with the Kogito architecture smarter and easier to use. Make sure you are not clicked any where on the process diagram and then click the properties icon shown by the pencil and paper icon. Scroll down and click Process Data to expand where the process variables are defined. On them you will see a new feature to the BPMN editor called Tags. Tags specify how the data for the process variable will be used, is it an input or output, is it internal, is it read-only, etc. These are used for many of the features for generating the process\u2019s Swagger-UI domain specific endpoints, and also what gets returned based on selection and each variable can have multiple tags if necessary. This makes interacting with your process right away so much easier since you can start building top-down for your process design and get it deployed rapidly! Explore the tags and see if you can figure out why skills, experience and candidate were all a part of the example schema for the start process end point. In the next section we will go into more detail of using and interacting with the process.","title":"Deploying the Service"},{"location":"guided_exercises/02_techXchange/05_exploring_dev_ui_console/","text":"Exploring the Dev-UI Console in More Detail The biggest change from previous versions of the jBPM runtime is that everything used to have to be built in KIE Server and Business Central required a build, deploy and multiple user changes to validate a process. Now with the assistance of Kogito and Quarkus, we can combine a more developer-focused improvement to the pattern of delivering and testing processes while we are building them. Setting Up the Development Environment Open a terminal from the desktop. Clone the following repository using the terminal: git clone https://github.com/timwuthenow/cc-application-approval-starter Navigate to the folder and open Visual Studio Code: cd cc-application-approval-starter code . When Visual Studio Code opens, you will have the workspace and a terminal. In the terminal within VS Code, type: mvn quarkus:dev Open the Browser and navigate to http://localhost:8080/q/dev-ui. This will feature all of the extensions and capabilities that are on the current process project. Exploring the Dev-UI Locate and click on Process Instances in the jBPM Quarkus Dev UI add-on. To start a new process to test, click the Process Definitions tab and then click the cc_application_approval's play button. A form is autogenerated. Fill in sample data and click Start . Tag Value Age 20 Annual Income 20000 Credit Score 300 Is student? Checked Name Angelito After submitting, a green banner will be opened allowing you to go to the Process Details . Open the process details, and check the process instance progress, and the variable values. Here you will be able to see what was done in the process, a very simple straight through process that ran a bunch of script tasks. In the next section we\u2019re going to improve this! For now, close the browser that you've navigated to the dev-ui as we're going to do more work on it in VS Code and then look at it again when we finish. This is very important for now if you leave mvn quarkus:dev up as changes and the active connection will currently break the live coding. However, reaccessing is very quick!","title":"Exploring the Dev-UI"},{"location":"guided_exercises/02_techXchange/05_exploring_dev_ui_console/#exploring-the-dev-ui-console-in-more-detail","text":"The biggest change from previous versions of the jBPM runtime is that everything used to have to be built in KIE Server and Business Central required a build, deploy and multiple user changes to validate a process. Now with the assistance of Kogito and Quarkus, we can combine a more developer-focused improvement to the pattern of delivering and testing processes while we are building them.","title":"Exploring the Dev-UI Console in More Detail"},{"location":"guided_exercises/02_techXchange/05_exploring_dev_ui_console/#setting-up-the-development-environment","text":"Open a terminal from the desktop. Clone the following repository using the terminal: git clone https://github.com/timwuthenow/cc-application-approval-starter Navigate to the folder and open Visual Studio Code: cd cc-application-approval-starter code . When Visual Studio Code opens, you will have the workspace and a terminal. In the terminal within VS Code, type: mvn quarkus:dev Open the Browser and navigate to http://localhost:8080/q/dev-ui. This will feature all of the extensions and capabilities that are on the current process project.","title":"Setting Up the Development Environment"},{"location":"guided_exercises/02_techXchange/05_exploring_dev_ui_console/#exploring-the-dev-ui","text":"Locate and click on Process Instances in the jBPM Quarkus Dev UI add-on. To start a new process to test, click the Process Definitions tab and then click the cc_application_approval's play button. A form is autogenerated. Fill in sample data and click Start . Tag Value Age 20 Annual Income 20000 Credit Score 300 Is student? Checked Name Angelito After submitting, a green banner will be opened allowing you to go to the Process Details . Open the process details, and check the process instance progress, and the variable values. Here you will be able to see what was done in the process, a very simple straight through process that ran a bunch of script tasks. In the next section we\u2019re going to improve this! For now, close the browser that you've navigated to the dev-ui as we're going to do more work on it in VS Code and then look at it again when we finish. This is very important for now if you leave mvn quarkus:dev up as changes and the active connection will currently break the live coding. However, reaccessing is very quick!","title":"Exploring the Dev-UI"},{"location":"guided_exercises/02_techXchange/06_using_dmn_in_processes/","text":"Using DMN in Processes In this section, we'll modify our process to incorporate a decision that we have in the project. Modifying the Process In VSCode, open the process approval.bpmn, which is located at src/main/resources /approval.bpmn. Find the Check Card Eligibility scipt task. This one we are going to modify to incorporate a decision that we have. You can delete the task by using the trashcan icon (or pressing backspace/delete key on your keyboard). and delete the script task named Check Card Eligibility. This can be done by clicking on it and clicking the backspace/delete key on your keyboard or clicking the trashcan icon. To save a step though, you can also convert this task to a Business Rule task by clicking the icon and clicking the gears icon below the task and clicking the one that looks like a spreadsheet to add a decision node. If you don\u2019t convert it, you can use the BPMN panel on the left where you can right click the empty square icon and selecting Business Rule and drag from the name onto the line between Log application received and Generate CC Details. Add a new business rules task to the process diagram, named Is Eligible . In the next section we will fully change the settings to work with the DMN model from the workspace. Since the DMN plugin for the VSCode workspace at authoring was not yet updated to do the DMN 1.5 specification, if you want to view the DMN model. Import the project into BAMOE Canvas. To do this, go to your browser and navigate to the BAMOE Canvas bookmark. In the section titled Import under the From URL, paste the URL that you cloned earlier, https://github.com/kmacedovarela/cc-application-approval-starter and click Clone. Click the arrow on the workspace that opens up that is next to cc-application-approval and click on CreditCardEligibility to open the DMN model for the Eligibility rule. Click the circle icon with the \u201ci\u201d in it to open the properties of the model. These will be auto-populated in VS-Code from the decision since they\u2019re in the same workspace, but this provides a way of understanding how the decisions are mapped. On the properties panel you will see the following that will be used to help populate the BPMN diagram to connect the DMN decision to the process model. Now that you have a basic understanding of what the model looks like and some of the properties, we will see in the next section how to tie it in. Configuring the Business Rules Task Return back to Visual Studio Code and configure the business rules task to consume the decision model. You can find and double check the information below, in the DMN file CreditCardEligibility.dmn available in your project. Click the Is Eligible node and go to the properties. Change the values in the properties window to match the table below. Most values will be autopopulated as you select the model/decision. When you change the Rule Language to DMN, the filename will be selectable and the other data will start getting populated automatically: Value Input Rule Language DMN Filename CreditCardEligibility.dmn Namespace https://kie.org/dmn/_639D6115-E08E-439D-8D29-45750C32DB28 Decision Name IsEligible (select IsEligible from the pulldown) DMN Model Name Credit Card Eligibility Validation Configure the inputs and outputs of the task by clicking on Assignments and using the following info. Note the I is capitalized in the IsEligible. Input Name Data Type Source Applicant Applicant[org.acme.cc_approval.model] applicant Output : - Applicant: Applicant[org.acme.cc_approval.model] Output : | Name | Data Type | Source | |-------|-------|------| | IsEligible | String | approval | Your BPMN should now look like the one in the screenshot below. As long as it does, Press Control + S to save your diagram changes (you may have to do it two or three times, make sure the dot next to approval.bpmn disappears. Running the Process with DMN Automation As long as you left the http://localhost:8080/q/dev-ui closed from the previous lab, re-open it . If mvn quarkus:dev is running, it will hot reload as we change our process and project as long as the browser currently isn't engaged with the service. After this is reloaded from the saves, open the Dev UI, navigate to http://localhost:8080/q/dev-ui and go to Process Instances. From here, click on Process Definitions and click the arrow again on the approval process to start a new process instance from the process definition. Test with different data that will result in different outcomes in the decision. Scenario Is Student Annual Income Credit Score Age Automatic Approval false 15000 750 25 Automatic Rejection false 15000 750 17 Manual Review false 30000 600 20 After you submit any of the test scenarios, you can navigate to the green banner's to check the process details. Check Process Variables: Verify the variables of the completed instance to ensure the DMN decision was executed correctly. You can stop Quarkus, but at minimum, you must close the Dev-UI in your browser for further updates in the next sections. This is very important in the current release of BAMOE 9.1 with the live application trying to maintain contact with the mvn quarkus:dev and can cause issues. Configuring the process to handle different outcomes Now, with the process working, let's add the gateways to handle the three possible scenarios: Automatic approval, Automatic rejection, Manual Approval. To do this we will continue to modify the approval process. We need to add an exclusive gateway after the decision node by changing the diagram a little. You will be moving some the last task and end node first to create some room. Click and draw a square around the two nodes and drag to the right to make them further away from the Is Eligible decision. Now let\u2019s configure the process to have an exclusive gateway. To do this, click the gateway icon (orange diamond on the palette) and click and drag Exclusive Gateway onto the line separating Is Eligible and Generate CC details. If placed correctly, the gateway will turn the line blue and you will have new arrows form. Otherwise, just reconnect the arrows between the two existing nodes. From the gateway you can now click it and click the Red End Node twice to create two extra pathways. You can adjust the lines how you want, to put right angles on them double click the line to create the bend. Now let\u2019s configure the various pathways. To do this you will click each pathway and use the properties window to specify the condition. Let's configure the possible outcomes: Automatic Approval: - For the sequence flow leading to the approval end event, click the arrow going to Generate CC Details and type the name as Automatic Approval and expand Implementation/Execution and use the condition: return approval . toLowerCase (). equals ( \"approved\" ); Repeat the process for Manual Approval: - For the sequence flow leading to the manual review end event, now using the condition: return approval . toLowerCase (). equals ( \"manual\" ); Lastly, set Automatic Rejection: - For the sequence flow leading to the rejection end event, use the condition: return approval . toLowerCase (). equals ( \"rejected\" ); Add three different paths from the gateway, each leading to an end event. Save your diagram and then repeat the process from earlier to generate the SVG diagram in VSCode to visualize the process. This can be done by clicking the SVG icon above the BPMN diagram editor. Make sure there are no active connections to the local Quarkus service and then return to the terminal and run: mvn quarkus:dev if you have stopped it to restart Quarkus in development mode. Return to the http://localhost:8080/q/dev-ui environment and go back to the jBPM Quarkus Dev UI and click Process Instances. Start a new process instance and test the three different scenarios (approved, manual, rejected). Using the data from earlier: Scenario Is Student Annual Income Credit Score Age Automatic Approval false 15000 750 25 Automatic Rejection false 15000 750 17 Manual Review false 30000 600 20 Notice if you do the Automatic Rejection scenario, you get the path that exits the bottom of the exclusive gateway as seen in the instance details. Close the browser for Dev-UI window to continue to the next section.","title":"Adding DMN to Process"},{"location":"guided_exercises/02_techXchange/06_using_dmn_in_processes/#using-dmn-in-processes","text":"In this section, we'll modify our process to incorporate a decision that we have in the project.","title":"Using DMN in Processes"},{"location":"guided_exercises/02_techXchange/06_using_dmn_in_processes/#modifying-the-process","text":"In VSCode, open the process approval.bpmn, which is located at src/main/resources /approval.bpmn. Find the Check Card Eligibility scipt task. This one we are going to modify to incorporate a decision that we have. You can delete the task by using the trashcan icon (or pressing backspace/delete key on your keyboard). and delete the script task named Check Card Eligibility. This can be done by clicking on it and clicking the backspace/delete key on your keyboard or clicking the trashcan icon. To save a step though, you can also convert this task to a Business Rule task by clicking the icon and clicking the gears icon below the task and clicking the one that looks like a spreadsheet to add a decision node. If you don\u2019t convert it, you can use the BPMN panel on the left where you can right click the empty square icon and selecting Business Rule and drag from the name onto the line between Log application received and Generate CC Details. Add a new business rules task to the process diagram, named Is Eligible . In the next section we will fully change the settings to work with the DMN model from the workspace. Since the DMN plugin for the VSCode workspace at authoring was not yet updated to do the DMN 1.5 specification, if you want to view the DMN model. Import the project into BAMOE Canvas. To do this, go to your browser and navigate to the BAMOE Canvas bookmark. In the section titled Import under the From URL, paste the URL that you cloned earlier, https://github.com/kmacedovarela/cc-application-approval-starter and click Clone. Click the arrow on the workspace that opens up that is next to cc-application-approval and click on CreditCardEligibility to open the DMN model for the Eligibility rule. Click the circle icon with the \u201ci\u201d in it to open the properties of the model. These will be auto-populated in VS-Code from the decision since they\u2019re in the same workspace, but this provides a way of understanding how the decisions are mapped. On the properties panel you will see the following that will be used to help populate the BPMN diagram to connect the DMN decision to the process model. Now that you have a basic understanding of what the model looks like and some of the properties, we will see in the next section how to tie it in.","title":"Modifying the Process"},{"location":"guided_exercises/02_techXchange/06_using_dmn_in_processes/#configuring-the-business-rules-task","text":"Return back to Visual Studio Code and configure the business rules task to consume the decision model. You can find and double check the information below, in the DMN file CreditCardEligibility.dmn available in your project. Click the Is Eligible node and go to the properties. Change the values in the properties window to match the table below. Most values will be autopopulated as you select the model/decision. When you change the Rule Language to DMN, the filename will be selectable and the other data will start getting populated automatically: Value Input Rule Language DMN Filename CreditCardEligibility.dmn Namespace https://kie.org/dmn/_639D6115-E08E-439D-8D29-45750C32DB28 Decision Name IsEligible (select IsEligible from the pulldown) DMN Model Name Credit Card Eligibility Validation Configure the inputs and outputs of the task by clicking on Assignments and using the following info. Note the I is capitalized in the IsEligible. Input Name Data Type Source Applicant Applicant[org.acme.cc_approval.model] applicant Output : - Applicant: Applicant[org.acme.cc_approval.model] Output : | Name | Data Type | Source | |-------|-------|------| | IsEligible | String | approval | Your BPMN should now look like the one in the screenshot below. As long as it does, Press Control + S to save your diagram changes (you may have to do it two or three times, make sure the dot next to approval.bpmn disappears.","title":"Configuring the Business Rules Task"},{"location":"guided_exercises/02_techXchange/06_using_dmn_in_processes/#running-the-process-with-dmn-automation","text":"As long as you left the http://localhost:8080/q/dev-ui closed from the previous lab, re-open it . If mvn quarkus:dev is running, it will hot reload as we change our process and project as long as the browser currently isn't engaged with the service. After this is reloaded from the saves, open the Dev UI, navigate to http://localhost:8080/q/dev-ui and go to Process Instances. From here, click on Process Definitions and click the arrow again on the approval process to start a new process instance from the process definition. Test with different data that will result in different outcomes in the decision. Scenario Is Student Annual Income Credit Score Age Automatic Approval false 15000 750 25 Automatic Rejection false 15000 750 17 Manual Review false 30000 600 20 After you submit any of the test scenarios, you can navigate to the green banner's to check the process details. Check Process Variables: Verify the variables of the completed instance to ensure the DMN decision was executed correctly. You can stop Quarkus, but at minimum, you must close the Dev-UI in your browser for further updates in the next sections. This is very important in the current release of BAMOE 9.1 with the live application trying to maintain contact with the mvn quarkus:dev and can cause issues.","title":"Running the Process with DMN Automation"},{"location":"guided_exercises/02_techXchange/06_using_dmn_in_processes/#configuring-the-process-to-handle-different-outcomes","text":"Now, with the process working, let's add the gateways to handle the three possible scenarios: Automatic approval, Automatic rejection, Manual Approval. To do this we will continue to modify the approval process. We need to add an exclusive gateway after the decision node by changing the diagram a little. You will be moving some the last task and end node first to create some room. Click and draw a square around the two nodes and drag to the right to make them further away from the Is Eligible decision. Now let\u2019s configure the process to have an exclusive gateway. To do this, click the gateway icon (orange diamond on the palette) and click and drag Exclusive Gateway onto the line separating Is Eligible and Generate CC details. If placed correctly, the gateway will turn the line blue and you will have new arrows form. Otherwise, just reconnect the arrows between the two existing nodes. From the gateway you can now click it and click the Red End Node twice to create two extra pathways. You can adjust the lines how you want, to put right angles on them double click the line to create the bend. Now let\u2019s configure the various pathways. To do this you will click each pathway and use the properties window to specify the condition. Let's configure the possible outcomes: Automatic Approval: - For the sequence flow leading to the approval end event, click the arrow going to Generate CC Details and type the name as Automatic Approval and expand Implementation/Execution and use the condition: return approval . toLowerCase (). equals ( \"approved\" ); Repeat the process for Manual Approval: - For the sequence flow leading to the manual review end event, now using the condition: return approval . toLowerCase (). equals ( \"manual\" ); Lastly, set Automatic Rejection: - For the sequence flow leading to the rejection end event, use the condition: return approval . toLowerCase (). equals ( \"rejected\" ); Add three different paths from the gateway, each leading to an end event. Save your diagram and then repeat the process from earlier to generate the SVG diagram in VSCode to visualize the process. This can be done by clicking the SVG icon above the BPMN diagram editor. Make sure there are no active connections to the local Quarkus service and then return to the terminal and run: mvn quarkus:dev if you have stopped it to restart Quarkus in development mode. Return to the http://localhost:8080/q/dev-ui environment and go back to the jBPM Quarkus Dev UI and click Process Instances. Start a new process instance and test the three different scenarios (approved, manual, rejected). Using the data from earlier: Scenario Is Student Annual Income Credit Score Age Automatic Approval false 15000 750 25 Automatic Rejection false 15000 750 17 Manual Review false 30000 600 20 Notice if you do the Automatic Rejection scenario, you get the path that exits the bottom of the exclusive gateway as seen in the instance details. Close the browser for Dev-UI window to continue to the next section.","title":"Configuring the process to handle different outcomes"},{"location":"guided_exercises/02_techXchange/07_adding_service_task/","text":"Adding a Service Task for Credit Card Generation When a credit card application is approved, we need to generate the credit card details. This can be done a lot of different ways, for this lab we are going to start with a Service Task that will invoke a Java method, but this could easily be a call to a different existing service. This is the power of the flexibility that the jBPM runtime can provide with running Java services in a multitude of ways. Understanding the Service Class to be implemented There is a Java class that has already been created to do the credit card details for the approved account. This can be opened by navigating to src/main/java/org/acme/service/CreditCardService.java. @ApplicationScoped public class CreditCardService { public CreditCard generateCreditCardDetails ( Applicant applicant ) { double creditLimit = applicant . getAnnualIncome () * 0.3 ; return new CreditCard ( applicant . getName (), creditLimit ); } } Adding the Service Task to Your BPMN Return to the approval process in VSCode so we can change the Generate CC Details to a service task on the approved path. Click the Generate CC Details script task and click the three-gear icon followed by the two-gear icon to Convert into Service Task . Configure the service task with these attributes in the properties panel: Implementation: Java Interface: org.acme.cc_approval.service.CreditCardService Operation: generateCreditCardDetails Set the Assignments . Click the paper and pencil icon to open the editor. For the Data Inputs and Assignments , add: Name: applicant Data Type: Applicant [org.acme.cc_approval.model] Source: applicant For the Data Outputs and Assignments , add: Name: creditCard Data Type: CreditCard[org.acme.cc_approval Target: creditCard After making changes, test the newly updated process with the addition of the autogenerated credit card details. Make sure your diagram is saved and you press the generate SVG button to regenerate the image. Open the Quarkus Dev UI in your browser by going to to http://localhost:8080/q/dev-ui Start a new process instance with an approved application to see that now the response that gets returned generates the card details in the instance data. \ufffc Scenario Is Student Annual Income Credit Score Age Automatic Approval false 15000 750 25 After starting the instance go to the Process details link. Examine the process instance details, paying particular attention to the generated credit card values. Now that we have the Credit Card details implemented, it is time to add the human task to round out this process. Close the Dev UI.","title":"Adding Service Tasks"},{"location":"guided_exercises/02_techXchange/07_adding_service_task/#adding-a-service-task-for-credit-card-generation","text":"When a credit card application is approved, we need to generate the credit card details. This can be done a lot of different ways, for this lab we are going to start with a Service Task that will invoke a Java method, but this could easily be a call to a different existing service. This is the power of the flexibility that the jBPM runtime can provide with running Java services in a multitude of ways.","title":"Adding a Service Task for Credit Card Generation"},{"location":"guided_exercises/02_techXchange/07_adding_service_task/#understanding-the-service-class-to-be-implemented","text":"There is a Java class that has already been created to do the credit card details for the approved account. This can be opened by navigating to src/main/java/org/acme/service/CreditCardService.java. @ApplicationScoped public class CreditCardService { public CreditCard generateCreditCardDetails ( Applicant applicant ) { double creditLimit = applicant . getAnnualIncome () * 0.3 ; return new CreditCard ( applicant . getName (), creditLimit ); } }","title":"Understanding the Service Class to be implemented"},{"location":"guided_exercises/02_techXchange/07_adding_service_task/#adding-the-service-task-to-your-bpmn","text":"Return to the approval process in VSCode so we can change the Generate CC Details to a service task on the approved path. Click the Generate CC Details script task and click the three-gear icon followed by the two-gear icon to Convert into Service Task . Configure the service task with these attributes in the properties panel: Implementation: Java Interface: org.acme.cc_approval.service.CreditCardService Operation: generateCreditCardDetails Set the Assignments . Click the paper and pencil icon to open the editor. For the Data Inputs and Assignments , add: Name: applicant Data Type: Applicant [org.acme.cc_approval.model] Source: applicant For the Data Outputs and Assignments , add: Name: creditCard Data Type: CreditCard[org.acme.cc_approval Target: creditCard After making changes, test the newly updated process with the addition of the autogenerated credit card details. Make sure your diagram is saved and you press the generate SVG button to regenerate the image. Open the Quarkus Dev UI in your browser by going to to http://localhost:8080/q/dev-ui Start a new process instance with an approved application to see that now the response that gets returned generates the card details in the instance data. \ufffc Scenario Is Student Annual Income Credit Score Age Automatic Approval false 15000 750 25 After starting the instance go to the Process details link. Examine the process instance details, paying particular attention to the generated credit card values. Now that we have the Credit Card details implemented, it is time to add the human task to round out this process. Close the Dev UI.","title":"Adding the Service Task to Your BPMN"},{"location":"guided_exercises/02_techXchange/08_adding_human_task/","text":"Adding a Human Task The long-awaited addition of Human Tasks when using Kogito and the jBPM engine together is a part of the Technical Preview for the 9.1 release. This section will focus on adding them to the process. Modifying the manual approval to incorporate human tasks When the decision outcome indicates the need of a manual approval, the flow should move forward to a user task. Let's add this manual step in the process and see it in action! The process changes we are going to implement are going to focus on when an application requires manual review, it now goes to the \"Review Application\" task. For now, these are going to be assigned to the user, jdoe. They will review the application and decides to approve or reject it. - Based on the decision (stored in the approval variable), the process takes one of two paths: if approved, it proceeds to generate credit card details; if rejected, it ends the process immediately. Implement the Manual approval Add a new user task on your process, on the \"manual approval\" path. To do this from the palette, click the blank square and click and drag User to the manual approval path of the process. Open the properties of the Human Task, by clicking it and then the Pencil/Paper icon. Change the human task name to \"Review Application\". Expand Implementation/Execution so we can set the task name to reviewApplication. Set the task name to reviewApplication. Scroll down if necessary and now we need to set the actor to jdoe. To do this, first under Actors click Add, then click the pulldown box. Click New to create a new user. Type the actor name as jdoe and be sure to click the checkbox to save the id. After clicking the checkbox, be sure to click your new user jdoe. Set the data assignments for the task, open the Assignments on the task. Set the assignments as below: Input Assignment: - Name: Applicant - Data Type: Applicant[org.acme.cc_approval.model] - Source: applicant Output Assignment: - Name: approval - Data Type: String - Source: approval The final properties view of the human task should look like the below screenshot. Cleaning up and finalizing the process The process diagram after the Review Application Human task can be configured many different ways, below is just an example of a possible solution, you do not have to have all of the exclusive gateways coming together to simplify if you choose not to. That is ultimately your choice! Below is just a sample of how the rerouting can look like. You can configure the sequence flows after the human task such as: For the \"approved\" path: - Set the condition to: return approval . toLowerCase (). equals ( \"approved\" ); For the \"rejected\" path, you can set it to be the default path after the manual approval so that if it doesn\u2019t explicitly say approved, the process will proceed to the rejected applications path. If you create an exclusive gateway after the Review Application human task, you can select the default route under Implementation/Execution to be Rejected Requests or you can set the exact condition if you\u2019d prefer. To manually set the condition it would need to be set to : return approval . toLowerCase (). equals ( \"rejected\" ); Once completed, save the diagram, and regenerate the SVG associated with this process. Testing the Updated Process If the process is still in mvn quarkus:dev then proceed to http://localhost:8080/q/dev-ui otherwise run the command to restart the application. Start a new process instance that would route to manual review. Scenario Is Student Annual Income Credit Score Age Manual Review false 30000 600 20 After submitting the process, review the process details to see how the changes impacted the process. From here, we can see that there is a Review Application task as the process has a Red Square on the node. So, now what do we do, we\u2019re in a development UI, but we\u2019re not jdoe, right? Well in the Dev-UI, you can be who you need to be as long as the profile is configured for it. To do this, we\u2019re going to go to the Tasks Console within the Dev-UI environment. When we open the Task Inbox in Dev-UI, we notice that there are currently no tasks, what\u2019s going on? If you notice in the top right of the screen shot there is a pull down for the user and it\u2019s on admin, so the console is correct, admin has no tasks, but we assigned the task to jdoe, let\u2019s see what\u2019s going on! Now we\u2019ve morphed into jdoe and are able to do any tasks in the development environment for jdoe. In this case we have one task, Review Application, click the Review Application link. Click the Review Application link to open the task. You will see that when we open this task, a form was autogenerated that is built to handle this task. Now we know based on our process we either need to set the approval to approved (any capitalization will work here because it\u2019s converted to lowercase) to approve the request, otherwise anything else will reject the application. Let\u2019s give John, the 20 year with 12,000 annual income a credit card to start building a credit history! Return to the Processes view and check the completed instance details. By default, the Process Instances view will only show active processes, since we approved this instance, it is completed. You can modify the filter on the status to include completed instances by clicking Status and adding the checkbox for Completed. Click Apply Filter to active the filter. There\u2019s our last ran process! Click approval to review the process details. Now we can see that the process took the approval path and is completed generating a new credit card for John! Optional challenge Now this process right now is assigned to just jdoe, ideally we would do a group, but for now in development for this lab we\u2019re going to attach this to a new user. Let\u2019s say instead of jdoe, the tasks will now go to mscott. We need to make some changes for the Dev-UI to work with this. Before proceeding be sure to close the Chrome instance of the Dev-UI console! Repeat the new actor method that was used for creating jdoe to have your user mscott \u2013 once again be sure to check the checkbox! Then change the actor from jdoe to mscott. If you were to access the Dev-UI now and repeated the steps from last section, you would see a task was created, but there is not an actor mscott in the Dev-UI to be used, so how do we get that there? Very easily, a quick properties change will allow it to be added to the Dev-UI console. In VSCode open src/main/resources/application.properties If you notice on line 22 there is a property that says %dev.jbpm.devui.users.jdoe.groups=admin,HR,IT \u2013 we can copy this line, changing jdoe to mscott and now mscott will have the same groups as jdoe, but since the task is assigned to mscott now, we can complete it! % dev . bpm . devui . users . jdoe . groups = admin , HR , IT % dev . jbpm . devui . users . mscott . groups = admin , HR , IT Save the file and then reopen the Dev-UI by going to http://localhost:8080/q/dev-ui and create a manual approval task and see if it is now assigned to mscott and make your path. Now much easier to manage multiple users, groups, etc in an environment to test the workflow and it is ready in seconds! Now we have our task assigned to mscott!","title":"Adding Human Task"},{"location":"guided_exercises/02_techXchange/08_adding_human_task/#adding-a-human-task","text":"The long-awaited addition of Human Tasks when using Kogito and the jBPM engine together is a part of the Technical Preview for the 9.1 release. This section will focus on adding them to the process.","title":"Adding a Human Task"},{"location":"guided_exercises/02_techXchange/08_adding_human_task/#modifying-the-manual-approval-to-incorporate-human-tasks","text":"When the decision outcome indicates the need of a manual approval, the flow should move forward to a user task. Let's add this manual step in the process and see it in action! The process changes we are going to implement are going to focus on when an application requires manual review, it now goes to the \"Review Application\" task. For now, these are going to be assigned to the user, jdoe. They will review the application and decides to approve or reject it. - Based on the decision (stored in the approval variable), the process takes one of two paths: if approved, it proceeds to generate credit card details; if rejected, it ends the process immediately.","title":"Modifying the manual approval to incorporate human tasks"},{"location":"guided_exercises/02_techXchange/08_adding_human_task/#implement-the-manual-approval","text":"Add a new user task on your process, on the \"manual approval\" path. To do this from the palette, click the blank square and click and drag User to the manual approval path of the process. Open the properties of the Human Task, by clicking it and then the Pencil/Paper icon. Change the human task name to \"Review Application\". Expand Implementation/Execution so we can set the task name to reviewApplication. Set the task name to reviewApplication. Scroll down if necessary and now we need to set the actor to jdoe. To do this, first under Actors click Add, then click the pulldown box. Click New to create a new user. Type the actor name as jdoe and be sure to click the checkbox to save the id. After clicking the checkbox, be sure to click your new user jdoe. Set the data assignments for the task, open the Assignments on the task. Set the assignments as below: Input Assignment: - Name: Applicant - Data Type: Applicant[org.acme.cc_approval.model] - Source: applicant Output Assignment: - Name: approval - Data Type: String - Source: approval The final properties view of the human task should look like the below screenshot.","title":"Implement the Manual approval"},{"location":"guided_exercises/02_techXchange/08_adding_human_task/#cleaning-up-and-finalizing-the-process","text":"The process diagram after the Review Application Human task can be configured many different ways, below is just an example of a possible solution, you do not have to have all of the exclusive gateways coming together to simplify if you choose not to. That is ultimately your choice! Below is just a sample of how the rerouting can look like. You can configure the sequence flows after the human task such as: For the \"approved\" path: - Set the condition to: return approval . toLowerCase (). equals ( \"approved\" ); For the \"rejected\" path, you can set it to be the default path after the manual approval so that if it doesn\u2019t explicitly say approved, the process will proceed to the rejected applications path. If you create an exclusive gateway after the Review Application human task, you can select the default route under Implementation/Execution to be Rejected Requests or you can set the exact condition if you\u2019d prefer. To manually set the condition it would need to be set to : return approval . toLowerCase (). equals ( \"rejected\" ); Once completed, save the diagram, and regenerate the SVG associated with this process.","title":"Cleaning up and finalizing the process"},{"location":"guided_exercises/02_techXchange/08_adding_human_task/#testing-the-updated-process","text":"If the process is still in mvn quarkus:dev then proceed to http://localhost:8080/q/dev-ui otherwise run the command to restart the application. Start a new process instance that would route to manual review. Scenario Is Student Annual Income Credit Score Age Manual Review false 30000 600 20 After submitting the process, review the process details to see how the changes impacted the process. From here, we can see that there is a Review Application task as the process has a Red Square on the node. So, now what do we do, we\u2019re in a development UI, but we\u2019re not jdoe, right? Well in the Dev-UI, you can be who you need to be as long as the profile is configured for it. To do this, we\u2019re going to go to the Tasks Console within the Dev-UI environment. When we open the Task Inbox in Dev-UI, we notice that there are currently no tasks, what\u2019s going on? If you notice in the top right of the screen shot there is a pull down for the user and it\u2019s on admin, so the console is correct, admin has no tasks, but we assigned the task to jdoe, let\u2019s see what\u2019s going on! Now we\u2019ve morphed into jdoe and are able to do any tasks in the development environment for jdoe. In this case we have one task, Review Application, click the Review Application link. Click the Review Application link to open the task. You will see that when we open this task, a form was autogenerated that is built to handle this task. Now we know based on our process we either need to set the approval to approved (any capitalization will work here because it\u2019s converted to lowercase) to approve the request, otherwise anything else will reject the application. Let\u2019s give John, the 20 year with 12,000 annual income a credit card to start building a credit history! Return to the Processes view and check the completed instance details. By default, the Process Instances view will only show active processes, since we approved this instance, it is completed. You can modify the filter on the status to include completed instances by clicking Status and adding the checkbox for Completed. Click Apply Filter to active the filter. There\u2019s our last ran process! Click approval to review the process details. Now we can see that the process took the approval path and is completed generating a new credit card for John!","title":"Testing the Updated Process"},{"location":"guided_exercises/02_techXchange/08_adding_human_task/#optional-challenge","text":"Now this process right now is assigned to just jdoe, ideally we would do a group, but for now in development for this lab we\u2019re going to attach this to a new user. Let\u2019s say instead of jdoe, the tasks will now go to mscott. We need to make some changes for the Dev-UI to work with this. Before proceeding be sure to close the Chrome instance of the Dev-UI console! Repeat the new actor method that was used for creating jdoe to have your user mscott \u2013 once again be sure to check the checkbox! Then change the actor from jdoe to mscott. If you were to access the Dev-UI now and repeated the steps from last section, you would see a task was created, but there is not an actor mscott in the Dev-UI to be used, so how do we get that there? Very easily, a quick properties change will allow it to be added to the Dev-UI console. In VSCode open src/main/resources/application.properties If you notice on line 22 there is a property that says %dev.jbpm.devui.users.jdoe.groups=admin,HR,IT \u2013 we can copy this line, changing jdoe to mscott and now mscott will have the same groups as jdoe, but since the task is assigned to mscott now, we can complete it! % dev . bpm . devui . users . jdoe . groups = admin , HR , IT % dev . jbpm . devui . users . mscott . groups = admin , HR , IT Save the file and then reopen the Dev-UI by going to http://localhost:8080/q/dev-ui and create a manual approval task and see if it is now assigned to mscott and make your path. Now much easier to manage multiple users, groups, etc in an environment to test the workflow and it is ready in seconds! Now we have our task assigned to mscott!","title":"Optional challenge"},{"location":"guided_exercises/02_techXchange/09_running_on_openshift/","text":"Running on OpenShift 6.1 Deploying the business service on OpenShift With the base project that was applied, we have a great kick start on the environment to get it deployed. Within the technical preview of BAMOE 9.1, there is a 1 to 1 ratio of Process Microservice to Task and Process Management consoles (one of each as they are connected via the services to exchange information). To simplify this roll out, the workspace includes several scripts that can be used for after this lab to assist in the roll out of your services. The one that we are most interested in because our environments are operating right now all locally, is to see this working in OpenShift to get the true benefit of Cloud Native Process. Log in to OpenShift with the command line. The easiest way is to use https://ibm.biz/bamoe-ocp-token and get the oc login command from the browser. This will link you to the Development Sandbox token of Red Hat OpenShift. Within the project itself, we have a few scripts that will assist in the roll out of the architecture. There are some limitations within the Technical Preview of IBM Business Automation Manager Open Editions in that for security, we are utilizing Keycloak and there is a one to one ratio for roll out of services. In the full release for BAMOE 9.2, these limitations will start to be greatly reduced and can be explained within a product roadmap session with your IBM account team! The script that we\u2019re most interested in is the one that will help us roll out all of these services. This will be handled through deploy-ocp-lab.sh and deploy-ocp-lab.bat depending on how you\u2019re accessing it, if you have time you can always refer to the scripts that can be used to setup a Keycloak instance at a high level utilizing the new-keycloak.sh. 3. This script when you run it through the usage of Package and deploy the application on OpenShift using the Quarkus Extension Maven. So if you look at the script there will be a section that looks like the below if using the Windows Batch File (deploy-to-ocp.bat): : : Build and deploy the application call mvn clean package ^ -Dquarkus.container-image.build=true ^ -Dquarkus.kubernetes-client.namespace= %NAMESPACE% ^ -Dquarkus.openshift.deploy=true ^ -Dquarkus.openshift.expose=true ^ -Dquarkus.application.name= %SERVICE_NAME% ^ -Dkogito.service.url=https:// %SERVICE_NAME% - %NAMESPACE% . %BASE_URL% ^ -Dkogito.jobs-service.url=https:// %SERVICE_NAME% - %NAMESPACE% . %BASE_URL% ^ -Dkogito.dataindex.http.url=https:// %SERVICE_NAME% - %NAMESPACE% . %BASE_URL% If using the script version (deploy-to-ocp.sh): mvn clean package \\ -Dquarkus.container-image.build = true \\ -Dquarkus.kubernetes-client.namespace = $NAMESPACE \\ -Dquarkus.openshift.deploy = true \\ -Dquarkus.openshift.expose = true \\ -Dquarkus.application.name = $SERVICE_NAME \\ -Dkogito.service.url = https:// $SERVICE_NAME - $NAMESPACE . $BASE_URL \\ -Dkogito.jobs-service.url = https:// $SERVICE_NAME - $NAMESPACE . $BASE_URL \\ -Dkogito.dataindex.http.url = https:// $SERVICE_NAME - $NAMESPACE . $BASE_URL After this script is started, the Process/DMN package will be built as a Kogito-enabled service that will start building a base quarkus image of the service and from there it will publish it to the OpenShift Image Registry. After this is pushed, the deployment gets modified and exposed. After this is completed more deployments take place for the task-console and management-console. There is a 1:1 ratio of task/management console to process service in the 9.1 technical preview. When the script is completed, you will see something similar to the message below if running the deploy-dev-ocp.sh script, the important aspect of this is that the links in VSCode if you hold the control or command key (depending on operating system), you can click and open the links for the respective units.: Now, let's check if the service is up and running by accessing the Swagger UI: GET {yourserviceurl}/q/swagger-ui - this will open your process\u2019s Swagger-UI. If you do a POST, you can start your process similar to how it was done within the Dev-UI, the only difference is now your process is executing within a Quarkus Container on OpenShift and can interact with the different consoles. After you submit a sample process run, you can access the other links to see the process. Note, if there are any timers, they start the moment you submit the process, so keep this in mind if there are any present. If you navigate to the management console, it is setup in Development mode and has no security. When you use the Task Console, login with user jdoe or mscott depending on how your process was deployed at the Keycloak login. You will notice when you login, there is a task assigned to you and a form was autogenerated based on the task process data associated with the task to help you get quickly started with the development of the workflow. Additional Information Common troubleshooting tips Use this section to document common troubleshooting issues that users may experience. Getting help For assistance, please refer to the lab instructions or consult with an instructor if available.","title":"Running on OpenShift"},{"location":"guided_exercises/02_techXchange/09_running_on_openshift/#running-on-openshift","text":"6.1 Deploying the business service on OpenShift With the base project that was applied, we have a great kick start on the environment to get it deployed. Within the technical preview of BAMOE 9.1, there is a 1 to 1 ratio of Process Microservice to Task and Process Management consoles (one of each as they are connected via the services to exchange information). To simplify this roll out, the workspace includes several scripts that can be used for after this lab to assist in the roll out of your services. The one that we are most interested in because our environments are operating right now all locally, is to see this working in OpenShift to get the true benefit of Cloud Native Process. Log in to OpenShift with the command line. The easiest way is to use https://ibm.biz/bamoe-ocp-token and get the oc login command from the browser. This will link you to the Development Sandbox token of Red Hat OpenShift. Within the project itself, we have a few scripts that will assist in the roll out of the architecture. There are some limitations within the Technical Preview of IBM Business Automation Manager Open Editions in that for security, we are utilizing Keycloak and there is a one to one ratio for roll out of services. In the full release for BAMOE 9.2, these limitations will start to be greatly reduced and can be explained within a product roadmap session with your IBM account team! The script that we\u2019re most interested in is the one that will help us roll out all of these services. This will be handled through deploy-ocp-lab.sh and deploy-ocp-lab.bat depending on how you\u2019re accessing it, if you have time you can always refer to the scripts that can be used to setup a Keycloak instance at a high level utilizing the new-keycloak.sh. 3. This script when you run it through the usage of Package and deploy the application on OpenShift using the Quarkus Extension Maven. So if you look at the script there will be a section that looks like the below if using the Windows Batch File (deploy-to-ocp.bat): : : Build and deploy the application call mvn clean package ^ -Dquarkus.container-image.build=true ^ -Dquarkus.kubernetes-client.namespace= %NAMESPACE% ^ -Dquarkus.openshift.deploy=true ^ -Dquarkus.openshift.expose=true ^ -Dquarkus.application.name= %SERVICE_NAME% ^ -Dkogito.service.url=https:// %SERVICE_NAME% - %NAMESPACE% . %BASE_URL% ^ -Dkogito.jobs-service.url=https:// %SERVICE_NAME% - %NAMESPACE% . %BASE_URL% ^ -Dkogito.dataindex.http.url=https:// %SERVICE_NAME% - %NAMESPACE% . %BASE_URL% If using the script version (deploy-to-ocp.sh): mvn clean package \\ -Dquarkus.container-image.build = true \\ -Dquarkus.kubernetes-client.namespace = $NAMESPACE \\ -Dquarkus.openshift.deploy = true \\ -Dquarkus.openshift.expose = true \\ -Dquarkus.application.name = $SERVICE_NAME \\ -Dkogito.service.url = https:// $SERVICE_NAME - $NAMESPACE . $BASE_URL \\ -Dkogito.jobs-service.url = https:// $SERVICE_NAME - $NAMESPACE . $BASE_URL \\ -Dkogito.dataindex.http.url = https:// $SERVICE_NAME - $NAMESPACE . $BASE_URL After this script is started, the Process/DMN package will be built as a Kogito-enabled service that will start building a base quarkus image of the service and from there it will publish it to the OpenShift Image Registry. After this is pushed, the deployment gets modified and exposed. After this is completed more deployments take place for the task-console and management-console. There is a 1:1 ratio of task/management console to process service in the 9.1 technical preview. When the script is completed, you will see something similar to the message below if running the deploy-dev-ocp.sh script, the important aspect of this is that the links in VSCode if you hold the control or command key (depending on operating system), you can click and open the links for the respective units.: Now, let's check if the service is up and running by accessing the Swagger UI: GET {yourserviceurl}/q/swagger-ui - this will open your process\u2019s Swagger-UI. If you do a POST, you can start your process similar to how it was done within the Dev-UI, the only difference is now your process is executing within a Quarkus Container on OpenShift and can interact with the different consoles. After you submit a sample process run, you can access the other links to see the process. Note, if there are any timers, they start the moment you submit the process, so keep this in mind if there are any present. If you navigate to the management console, it is setup in Development mode and has no security. When you use the Task Console, login with user jdoe or mscott depending on how your process was deployed at the Keycloak login. You will notice when you login, there is a task assigned to you and a form was autogenerated based on the task process data associated with the task to help you get quickly started with the development of the workflow.","title":"Running on OpenShift"},{"location":"guided_exercises/02_techXchange/09_running_on_openshift/#additional-information","text":"","title":"Additional Information"},{"location":"guided_exercises/02_techXchange/09_running_on_openshift/#common-troubleshooting-tips","text":"Use this section to document common troubleshooting issues that users may experience.","title":"Common troubleshooting tips"},{"location":"guided_exercises/02_techXchange/09_running_on_openshift/#getting-help","text":"For assistance, please refer to the lab instructions or consult with an instructor if available.","title":"Getting help"},{"location":"guided_exercises/02_techXchange/introduction/","text":"TechXchange Session 1108 Welcome to TechXchange 2024! This series of hands-on exercises is designed to guide you through the essential aspects of process automation using BAMOE. Whether you are a developer or an architect, these labs will equip you with the skills needed to effectively leverage BAMOE for modern, cloud-native business automation solutions. Throughout this bootcamp, you will gain practical experience in designing, implementing, and deploying business processes. You will also explore how to integrate decision services and external systems within your automation workflows, all while utilizing the powerful tools provided by BAMOE. Info Some features covered in this bootcamp may be in tech preview. For the latest information on fully supported and tech preview features, please refer to the product documentation . By the end of these labs, you will: Understand the core components and architecture of BAMOE. Gain practical skills in designing, implementing, and deploying business processes. Learn to integrate decision services and external systems within your processes. Develop proficiency in using IBM Business Automation Manager Canvas and VS Code for process authoring. About the Bootcamp In today's rapidly evolving business landscape, lightweight automation is key to efficiency and scalability on the cloud. Through these guides, developers and architects can gain hands-on experience with the latest capabilities of IBM Business Automation Manager Open Edition. Participants will explore the development experience and experiment with multiple capabilities for efficiently creating process automation solutions backed by open-source practices and IBM. Who Is This For This bootcamp is designed for technical users, such as developers and architects, who are seeking to explore the latest in business automation. It is ideal for professionals looking to deepen their knowledge of IBM BAMOE and how it can be used to create cloud-native solutions. What You'll Learn Get Started with Cloud-Native Business Automation : Discover the principles and benefits of cloud-native business automation and how IBM BAMOE enables it. Explore the Features of Development Tools : Dive into IBM BAMOE Canvas and IBM BAMOE Developer Tools for Microsoft VSCode, understanding their functionalities and how they enhance the development experience. Getting Started with Process Services : Learn the basics of creating and managing process services, and understand their role in business automation. Explore the Concepts of Process Automation : Understand the fundamental concepts and workflows involved in process automation, including modeling, execution, and monitoring. Leverage Event-Driven Capabilities in Process Automation : Explore how event-driven architecture can be integrated into process automation to enhance responsiveness and scalability. Deploying on OpenShift : Gain insights into deploying process and decision services on OpenShift, ensuring scalable and resilient operations. This bootcamp will equip you with the skills and knowledge needed to effectively utilize IBM Business Automation Manager Open Editions for modern, cloud-native business automation.","title":"Introduction"},{"location":"guided_exercises/02_techXchange/introduction/#techxchange-session-1108","text":"Welcome to TechXchange 2024! This series of hands-on exercises is designed to guide you through the essential aspects of process automation using BAMOE. Whether you are a developer or an architect, these labs will equip you with the skills needed to effectively leverage BAMOE for modern, cloud-native business automation solutions. Throughout this bootcamp, you will gain practical experience in designing, implementing, and deploying business processes. You will also explore how to integrate decision services and external systems within your automation workflows, all while utilizing the powerful tools provided by BAMOE. Info Some features covered in this bootcamp may be in tech preview. For the latest information on fully supported and tech preview features, please refer to the product documentation . By the end of these labs, you will: Understand the core components and architecture of BAMOE. Gain practical skills in designing, implementing, and deploying business processes. Learn to integrate decision services and external systems within your processes. Develop proficiency in using IBM Business Automation Manager Canvas and VS Code for process authoring.","title":"TechXchange Session 1108"},{"location":"guided_exercises/02_techXchange/introduction/#about-the-bootcamp","text":"In today's rapidly evolving business landscape, lightweight automation is key to efficiency and scalability on the cloud. Through these guides, developers and architects can gain hands-on experience with the latest capabilities of IBM Business Automation Manager Open Edition. Participants will explore the development experience and experiment with multiple capabilities for efficiently creating process automation solutions backed by open-source practices and IBM.","title":"About the Bootcamp"},{"location":"guided_exercises/02_techXchange/introduction/#who-is-this-for","text":"This bootcamp is designed for technical users, such as developers and architects, who are seeking to explore the latest in business automation. It is ideal for professionals looking to deepen their knowledge of IBM BAMOE and how it can be used to create cloud-native solutions.","title":"Who Is This For"},{"location":"guided_exercises/02_techXchange/introduction/#what-youll-learn","text":"Get Started with Cloud-Native Business Automation : Discover the principles and benefits of cloud-native business automation and how IBM BAMOE enables it. Explore the Features of Development Tools : Dive into IBM BAMOE Canvas and IBM BAMOE Developer Tools for Microsoft VSCode, understanding their functionalities and how they enhance the development experience. Getting Started with Process Services : Learn the basics of creating and managing process services, and understand their role in business automation. Explore the Concepts of Process Automation : Understand the fundamental concepts and workflows involved in process automation, including modeling, execution, and monitoring. Leverage Event-Driven Capabilities in Process Automation : Explore how event-driven architecture can be integrated into process automation to enhance responsiveness and scalability. Deploying on OpenShift : Gain insights into deploying process and decision services on OpenShift, ensuring scalable and resilient operations. This bootcamp will equip you with the skills and knowledge needed to effectively utilize IBM Business Automation Manager Open Editions for modern, cloud-native business automation.","title":"What You'll Learn"},{"location":"guided_exercises/03_process_automation/","text":"Getting Started with Process Automation In this section, you will become familiar with the foundational concepts of IBM Business Automation Manager Open Editions (BAMOE) and use the main features of IBM Business Automation Manager Canvas for process automation using BPMN. Goals: Clone the starter project created with the accelerator using git Download to the computer to work on VSCode using the Dev Tools extensions Use Quarkus dev mode and use the management console and task inbox Expand the process design: Script Task, Human Task, Timer Event, Service Task Let's get started.","title":"Getting Started with Process Automation"},{"location":"guided_exercises/03_process_automation/#getting-started-with-process-automation","text":"In this section, you will become familiar with the foundational concepts of IBM Business Automation Manager Open Editions (BAMOE) and use the main features of IBM Business Automation Manager Canvas for process automation using BPMN.","title":"Getting Started with Process Automation"},{"location":"guided_exercises/03_process_automation/#goals","text":"Clone the starter project created with the accelerator using git Download to the computer to work on VSCode using the Dev Tools extensions Use Quarkus dev mode and use the management console and task inbox Expand the process design: Script Task, Human Task, Timer Event, Service Task Let's get started.","title":"Goals:"},{"location":"guided_exercises/03_process_automation/00_intro_process/","text":"Running a basic process service About BPMN in BAMOE BPMN nodes provide a variety of functionalities to automate business processes. Here's a quick overview of some key BPMN nodes and their purposes: BPMN Node Purpose When to Use Example Script Task Execute code within process Quick, simple logic Calculate a value in JavaScript Service Task Invoke external services or custom Java code Integrate with other systems or process custom logic Call a REST API, or trigger a logic in a java class Human Task Involve human interaction Manual input/approval needed Assign approval to manager Timer Event Time-based events Scheduling, delays, timeouts Wait 24h before reminder Gateway Control process flow Decision points, parallel processing Route high-value orders differently DMN Task Execute complex decision logic automated decisions Determine credit eligibility Note on Automatic Marshalling BAMOE auto-maps process variables to DMN inputs/outputs by name and type. Here's a simplified explanation: Automatic Marshalling in BAMOE : The basic principle of this capability relies on BAMOE usage of Java reflection to convert POJOs (Plain Old Java Objects) to DMN data types and vice versa. It can be seen when: When passing data from process to DMN: It maps POJO fields to corresponding DMN input fields. When receiving data from DMN to process: DMN output is converted back into the appropriate Java object. Tip Matching Criteria: Field names in the POJO should match DMN input/output names. Data types should be compatible (e.g., Java String to DMN string). This automatic marshalling simplifies data exchange between BAMOE processes and DMN decision services, reducing the need for manual data transformation in many cases. Cloning the Project and Running Quarkus Dev Mode First, let's clone the starter project to your local repository and run it using Quarkus dev mode. Steps: Clone the Repository : git clone https://github.com/kmacedovarela/cc-application-approval-starter cd cc-application-approval-starter Run Maven Quarkus Dev Mode: mvn quarkus:dev Open the Browser and navigate to http://localhost:8080/q/dev-ui. Locate and click on Process instances in the jBPM Quarkus Dev UI add-on . Start a new process from the process definition list. Open the process details, and check the process instance progress, and the variable values. Awesome!! You've successfully cloned the project and started it using Quarkus dev mode. Next, let's explore how to extend the process design with various tasks in IBM Business Automation Manager Canvas.","title":"Running a basic process service"},{"location":"guided_exercises/03_process_automation/00_intro_process/#running-a-basic-process-service","text":"","title":"Running a basic process service"},{"location":"guided_exercises/03_process_automation/00_intro_process/#about-bpmn-in-bamoe","text":"BPMN nodes provide a variety of functionalities to automate business processes. Here's a quick overview of some key BPMN nodes and their purposes: BPMN Node Purpose When to Use Example Script Task Execute code within process Quick, simple logic Calculate a value in JavaScript Service Task Invoke external services or custom Java code Integrate with other systems or process custom logic Call a REST API, or trigger a logic in a java class Human Task Involve human interaction Manual input/approval needed Assign approval to manager Timer Event Time-based events Scheduling, delays, timeouts Wait 24h before reminder Gateway Control process flow Decision points, parallel processing Route high-value orders differently DMN Task Execute complex decision logic automated decisions Determine credit eligibility","title":"About BPMN in BAMOE"},{"location":"guided_exercises/03_process_automation/00_intro_process/#note-on-automatic-marshalling","text":"BAMOE auto-maps process variables to DMN inputs/outputs by name and type. Here's a simplified explanation: Automatic Marshalling in BAMOE : The basic principle of this capability relies on BAMOE usage of Java reflection to convert POJOs (Plain Old Java Objects) to DMN data types and vice versa. It can be seen when: When passing data from process to DMN: It maps POJO fields to corresponding DMN input fields. When receiving data from DMN to process: DMN output is converted back into the appropriate Java object. Tip Matching Criteria: Field names in the POJO should match DMN input/output names. Data types should be compatible (e.g., Java String to DMN string). This automatic marshalling simplifies data exchange between BAMOE processes and DMN decision services, reducing the need for manual data transformation in many cases.","title":"Note on Automatic Marshalling"},{"location":"guided_exercises/03_process_automation/00_intro_process/#cloning-the-project-and-running-quarkus-dev-mode","text":"First, let's clone the starter project to your local repository and run it using Quarkus dev mode.","title":"Cloning the Project and Running Quarkus Dev Mode"},{"location":"guided_exercises/03_process_automation/00_intro_process/#steps","text":"Clone the Repository : git clone https://github.com/kmacedovarela/cc-application-approval-starter cd cc-application-approval-starter Run Maven Quarkus Dev Mode: mvn quarkus:dev Open the Browser and navigate to http://localhost:8080/q/dev-ui. Locate and click on Process instances in the jBPM Quarkus Dev UI add-on . Start a new process from the process definition list. Open the process details, and check the process instance progress, and the variable values. Awesome!! You've successfully cloned the project and started it using Quarkus dev mode. Next, let's explore how to extend the process design with various tasks in IBM Business Automation Manager Canvas.","title":"Steps:"},{"location":"guided_exercises/03_process_automation/01_using_dmn_bpmn/","text":"Using DMN Decisions in Processes In this section, you will learn how to use DMN-based business rules task to automate decisions in your process, leveraging process variables and data types. In VSCode, open the process approval.bpmn , and delete the script task named Check Card Eligibility . Add a new business rules task to the process diagram, named Is Eligible . Configuring the Business Rules Task Configure the business rules task to consume the decision model. You can find and double check the information below, in the DMN file CreditCardEligibility.dmn available in your project: Rule Language : DMN Filename : CreditCardEligibility . dmn Namespace : https : //kie.org/dmn/_639D6115-E08E-439D-8D29-45750C32DB28 Decision Name : IsEligible DMN Model Name : Credit Card Eligibility Validation Configure the inputs and outputs of the task, by clicking on Assignments and using the following info: Input : Applicant , org.acme.cc_approval.model.Applicant , applicant Output : IsEligible , String , approval Validating the DMN Configuration Open the DMN File and verify that the Applicant data type is part of the DMN model and matches the process variable. Now, open the class org.acme.cc_approval.model.Applicant and check the attributes there. See the similarity with the data type? The process variable applicant matches the data type in the DMN model, ensuring seamless data flow between the process and the decision model. Running the Process with DMN Automation Start Quarkus in Dev Mode: mvn quarkus:dev Open the Dev UI, navigate to http://localhost:8080/q/dev-ui . Start a new process instance from the approval process definition. Check Process Variables: Verify the variables of the completed instance to ensure the DMN decision was executed correctly. These test scenarios help you validate different outcomes based on the input data for the DMN-based decision automation. Test with different data that will result on different outcomes in the decision. Scenario Is Student Annual Income Credit Score Age Automatic Approval false 15000 750 25 Automatic Rejection false 15000 750 17 Manual Review false 30000 600 20 Stop Quarkus and close the dev ui in your browser. Configuring Different Outcomes Based on Automated Decisions Now, with the process working, let's add the gateways to handle the three possible scenarios: Automatic approval , Automatic rejection , Manual Approval . Add an exclusive gateway after the decision node, and let's configure the possible outcomes: a. Automatic Approval : - For the sequence flow leading to the approval end event, use the condition: return approval . toLowerCase (). equals ( \"approved\" ); b. Manual Approval : - For the sequence flow leading to the manual review end event, use the condition: return approval . toLowerCase (). equals ( \"manual\" ); c. Automatic Rejection : - For the sequence flow leading to the rejection end event, use the condition: return approval . toLowerCase (). equals ( \"rejected\" ); Add three different paths from the gateway, each leading to an end event. Connect each sequence flow to a respective end event. Generate the SVG diagram in VSCode to visualize the process. Run mvn quarkus:dev to start Quarkus in development mode. Start a new process instance and test the three different scenarios (approved, manual, rejected). Verify the instance details for each scenario. Close the dev-ui and stop quarkus. Awesome! You've successfully configured the process to handle different outcomes based on automated decisions. With the exclusive gateway and the conditions set for approval, manual review, and rejection, your process is now more dynamic and responsive. Next, let's proceed with using the service task to further enhance our process automation capabilities.","title":"Using DMN Decisions in Processes"},{"location":"guided_exercises/03_process_automation/01_using_dmn_bpmn/#using-dmn-decisions-in-processes","text":"In this section, you will learn how to use DMN-based business rules task to automate decisions in your process, leveraging process variables and data types. In VSCode, open the process approval.bpmn , and delete the script task named Check Card Eligibility . Add a new business rules task to the process diagram, named Is Eligible .","title":"Using DMN Decisions in Processes"},{"location":"guided_exercises/03_process_automation/01_using_dmn_bpmn/#configuring-the-business-rules-task","text":"Configure the business rules task to consume the decision model. You can find and double check the information below, in the DMN file CreditCardEligibility.dmn available in your project: Rule Language : DMN Filename : CreditCardEligibility . dmn Namespace : https : //kie.org/dmn/_639D6115-E08E-439D-8D29-45750C32DB28 Decision Name : IsEligible DMN Model Name : Credit Card Eligibility Validation Configure the inputs and outputs of the task, by clicking on Assignments and using the following info: Input : Applicant , org.acme.cc_approval.model.Applicant , applicant Output : IsEligible , String , approval","title":"Configuring the Business Rules Task"},{"location":"guided_exercises/03_process_automation/01_using_dmn_bpmn/#validating-the-dmn-configuration","text":"Open the DMN File and verify that the Applicant data type is part of the DMN model and matches the process variable. Now, open the class org.acme.cc_approval.model.Applicant and check the attributes there. See the similarity with the data type? The process variable applicant matches the data type in the DMN model, ensuring seamless data flow between the process and the decision model.","title":"Validating the DMN Configuration"},{"location":"guided_exercises/03_process_automation/01_using_dmn_bpmn/#running-the-process-with-dmn-automation","text":"Start Quarkus in Dev Mode: mvn quarkus:dev Open the Dev UI, navigate to http://localhost:8080/q/dev-ui . Start a new process instance from the approval process definition. Check Process Variables: Verify the variables of the completed instance to ensure the DMN decision was executed correctly. These test scenarios help you validate different outcomes based on the input data for the DMN-based decision automation. Test with different data that will result on different outcomes in the decision. Scenario Is Student Annual Income Credit Score Age Automatic Approval false 15000 750 25 Automatic Rejection false 15000 750 17 Manual Review false 30000 600 20 Stop Quarkus and close the dev ui in your browser.","title":"Running the Process with DMN Automation"},{"location":"guided_exercises/03_process_automation/01_using_dmn_bpmn/#configuring-different-outcomes-based-on-automated-decisions","text":"Now, with the process working, let's add the gateways to handle the three possible scenarios: Automatic approval , Automatic rejection , Manual Approval . Add an exclusive gateway after the decision node, and let's configure the possible outcomes: a. Automatic Approval : - For the sequence flow leading to the approval end event, use the condition: return approval . toLowerCase (). equals ( \"approved\" ); b. Manual Approval : - For the sequence flow leading to the manual review end event, use the condition: return approval . toLowerCase (). equals ( \"manual\" ); c. Automatic Rejection : - For the sequence flow leading to the rejection end event, use the condition: return approval . toLowerCase (). equals ( \"rejected\" ); Add three different paths from the gateway, each leading to an end event. Connect each sequence flow to a respective end event. Generate the SVG diagram in VSCode to visualize the process. Run mvn quarkus:dev to start Quarkus in development mode. Start a new process instance and test the three different scenarios (approved, manual, rejected). Verify the instance details for each scenario. Close the dev-ui and stop quarkus. Awesome! You've successfully configured the process to handle different outcomes based on automated decisions. With the exclusive gateway and the conditions set for approval, manual review, and rejection, your process is now more dynamic and responsive. Next, let's proceed with using the service task to further enhance our process automation capabilities.","title":"Configuring Different Outcomes Based on Automated Decisions"},{"location":"guided_exercises/03_process_automation/02_using_service_tasks/","text":"Custom Logic with Service Tasks Adding a Service Task for Credit Card Generation When a card application is approved, we need to generate the credit card details. We can use Java to process this generation, providing a more flexible and powerful way to create the card information. Understanding the Service Class Open the CreditCardService class in your project. This class contains the logic for generating credit card details: @ApplicationScoped public class CreditCardService { public CreditCard generateCreditCardDetails ( Applicant applicant ) { double creditLimit = applicant . getAnnualIncome () * 0.3 ; return new CreditCard ( applicant . getName (), creditLimit ); } } This service calculates a credit limit based on 30% of the applicant's annual income and creates a new CreditCard object with the applicant's name and the calculated credit limit. Adding the Service Task to Your BPMN You can incorporate custom Java code into your processes using service tasks. Here's how to add and configure a service task for credit card generation: Open the approval process in VSCode. Add a new service task on the approved path, after the existing gateway and before the end node. Name the new service task \"Generate CC Details\". Configure the service task with these attributes: Implementation: java Interface: org.acme.cc_approval.service.CreditCardService Operation: generateCreditCardDetails The input assignment sends the process data as a parameter to the method, while the output assignment brings the method's return value back to the process and assigns it to a process variable. Assignments: - Input Assignment: - Name: applicant - Data Type: Applicant - Source: applicant - Output Assignment: - Name: creditCard - Data Type: CreditCard - Target: `creditCard Testing the Updated Process To test your updated process: Start Quarkus in dev mode using the command line. Open the Quarkus Dev UI in your browser. Start a new process instance with an approved application to see the card details in the instance data. Examine the process instance details, paying particular attention to the generated credit card values. After testing, close the Dev UI and stop Quarkus. By following these steps, you've successfully integrated a Java service task into your BPMN process to generate credit card details upon approval. This demonstrates how you can leverage custom Java code to enhance your business processes with complex logic and data manipulation.","title":"Custom Logic with Service Tasks"},{"location":"guided_exercises/03_process_automation/02_using_service_tasks/#custom-logic-with-service-tasks","text":"","title":"Custom Logic with Service Tasks"},{"location":"guided_exercises/03_process_automation/02_using_service_tasks/#adding-a-service-task-for-credit-card-generation","text":"When a card application is approved, we need to generate the credit card details. We can use Java to process this generation, providing a more flexible and powerful way to create the card information.","title":"Adding a Service Task for Credit Card Generation"},{"location":"guided_exercises/03_process_automation/02_using_service_tasks/#understanding-the-service-class","text":"Open the CreditCardService class in your project. This class contains the logic for generating credit card details: @ApplicationScoped public class CreditCardService { public CreditCard generateCreditCardDetails ( Applicant applicant ) { double creditLimit = applicant . getAnnualIncome () * 0.3 ; return new CreditCard ( applicant . getName (), creditLimit ); } } This service calculates a credit limit based on 30% of the applicant's annual income and creates a new CreditCard object with the applicant's name and the calculated credit limit.","title":"Understanding the Service Class"},{"location":"guided_exercises/03_process_automation/02_using_service_tasks/#adding-the-service-task-to-your-bpmn","text":"You can incorporate custom Java code into your processes using service tasks. Here's how to add and configure a service task for credit card generation: Open the approval process in VSCode. Add a new service task on the approved path, after the existing gateway and before the end node. Name the new service task \"Generate CC Details\". Configure the service task with these attributes: Implementation: java Interface: org.acme.cc_approval.service.CreditCardService Operation: generateCreditCardDetails The input assignment sends the process data as a parameter to the method, while the output assignment brings the method's return value back to the process and assigns it to a process variable. Assignments: - Input Assignment: - Name: applicant - Data Type: Applicant - Source: applicant - Output Assignment: - Name: creditCard - Data Type: CreditCard - Target: `creditCard","title":"Adding the Service Task to Your BPMN"},{"location":"guided_exercises/03_process_automation/02_using_service_tasks/#testing-the-updated-process","text":"To test your updated process: Start Quarkus in dev mode using the command line. Open the Quarkus Dev UI in your browser. Start a new process instance with an approved application to see the card details in the instance data. Examine the process instance details, paying particular attention to the generated credit card values. After testing, close the Dev UI and stop Quarkus. By following these steps, you've successfully integrated a Java service task into your BPMN process to generate credit card details upon approval. This demonstrates how you can leverage custom Java code to enhance your business processes with complex logic and data manipulation.","title":"Testing the Updated Process"},{"location":"guided_exercises/03c_CICD/introduction/","text":"IBM Business Automation Open Edition 9.2.x CI/CD Overview This lab was forked from the KIE Live session delived by Rafael Soares on YouTube found here . This lab has been updated to more current releases and also includes a provisioning of Nexus3 to deploy a Maven Repository in your environment. You do not need to do this if you have a Maven repository you can deploy your artifacts to. This lab will be using the OpenShift provided Pipelines built upon Tekton. With IBM Business Automation Open Edition 9.2.x, the projects are based on Maven artifact architectures, which provides an easy to build and deploy mechanism for all of your product needs. These can be done in Tekton as seen here, or with your favorite/enterprise provided CI/CD tools. The project in this lab is a Spring Boot hosted Decision Service that is utilizing the embedded KIE Server that will have a build and deploy triggered through the changes in your code base that are pushed to the specified build branch. With Decision Service projects, it is highly recommended that on all push events you consider a build of the project. This ensures that all the resources that are required are constantly available, breaking changes are identified immediately and if you want to release immediately, it's very easy to do so. Lab Overview Within this lab you will see how you can use OpenShift Pipelines (a.k.a Tekton) to automate the delivery of decision services implemented with BAMOE. In this lab you will see: The automation of repeatable decisions using the DMN specification; Decision tables implementation using XLS. Usage of the rules engine based on KIE Server and running on top of SpringBoot CI/CD Pipeline implemented using Tekton How to configure webhooks in your pipeline to deploy based on changes on a git repository Automated tests for decisions (with Test Scenarios ) that are considered during the pipeline execution Deployment with zero downtime with OpenShift rolling deployment strategy Pre-requisites Java 8 OpenShift 4.10+ OpenShift Command Line client ( oc ) VSCode VSCode Business Automation Extension Installing on OpenShift Fork this repository, to get started . Clone your fork to your local machine. git clone https://github.com/ ${ yourgithubuser } /business-automation-showcase.git cd business-automation-showcase Run the provisioning script (Linux/MacOS): sh provision.sh When the script runs, you will be presented with a prompt to enter a namespace name, which is the namespace that will be created for your environment to be deployed in. business-automation-cicd-showcase % ./provision.sh Input a namespace root - first letter lowercase namespace: tim-demo When this completes you will get a console with a few links, as well as an admin password to the Nexus environment that was created. ****************************************************************** Use this URL in your GitHub Webhook configuration for automatic deployment http://el-ba-cicd-event-listener-tim-demo-rhdm-kieserver-cicd.openshift.io Use this URL to access the front-end application: http://decision-service-webclient-tim-demo-rhdm-kieserver-cicd.openshift.io Use this URL to access the Nexus Repository: http://nexus3-tim-demo-rhdm-kieserver-cicd.openshift.io Use this password for admin access to Nexus 3: long-strung-out-password-used-for-nexus-repo-admin ****************************************************************** Configuring the automatic deployment on GitHub To configure the webhook for automated deployment, open your fork in your GitHub. Next, add a new webhook by opening \" Settings -> Webhook -> Add webhook button\". Fill the form with the information below: Payload URL : provided after the provisioning. You can also get it using the command: echo $( oc get route el-ba-cicd-event-listener --template = 'http://{{.spec.host}}' ) Content type : application/json Secret : empty Which events would you like to trigger this webhook? : Just the push event . At this point, you should already have a fully automated integration and deployment lifecycle for the business application. Any changes pushed to your repository will trigger the pipeline in your OpenShift cluster. Maven Repository Setup Typically an environment would already have a Maven repository included, but if you wanted to set one up and use your own, this is a quick run through of what's required to get the Maven Repository to connect to the current location of the IBM Business Automation Open Edition 9.2.x Maven artifacts which reside in the Red Hat Maven General Access Repository. From the console log you got earlier, you would have received two messages with routes and password infromation for the Nexus repository location (the 3rd item) and the admin password (the commands to get these are found within the provision.sh file). If you hold Control (Linux/Windows) or Command (Mac) and click the link in VSCode, you will go to the location of your newly provisioned Nexus3 repository. By default, the Nexus repository includes a Mirror of Maven Central which anyone can publish to following this documentation and is the typical environment for Community releases, but enterprise releases of Open Source Software can be found here as well. The practice within Red Hat, has been to publish artifacts to its own repository found here as these are the enterprise releases and outside of community. No access is required to pull from this repository. We will configure our newly minted Nexus repository to have a Mirror of the Red Hat Maven General Access Repository so that for any product releases that are used, it will pull them from Red Hat Maven General Access Repository and publish them in the Nexus repository as a proxy. This is common in most enterprise environments instead of a direct connection to the repositories to offset downtime, have security checks, etc. To setup the mirrow follow these steps: From the route that was produced from the console output (or you can run oc get route | grep nexus where you will need to paste the output) and use that link to get to the Nexus admin screen. From here, using the admin password that was a part of your console output. Click Sign In . Alternatively the command to run is below. echo \" $( oc exec $( oc get pod -o template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}' | grep nexus ) -- cat /nexus-data/admin.password ) \" Copying the password, login as Admin with the password from the previous command. You will now have a Wizard popup to walk you through some setup, we're going to do very minimal changes: you can change the password (e.g. password ). Typically in this development/throwaway environment, enabling anonymous access is the best way to do this, so select that option Enable anonymous access . Then click finish. Now that the repository is ready to be configured, we are going to create a mirror of the Red Hat Maven General Access Repository so that we can pull resources through the proxy instead of direct. To do this, click the Gear icon ( ) to open the settings screen. From here click Repositories to view the current repositories and also to be able to create another. Here you will see a list of default repositorues. By default, Nexus will include the connection to Maven_Central , but we'd also like to add the Red Hat Maven General Access Repository connection to pull the resources from this repository. To do so, click Create Repository to build a new repository. Scroll down and we're going to create a maven2 (proxy) . The reason we're doing a proxy is that we're going to allow our repository to reach out to the Red Hat Maven General Access Repository to retrieve artifacts it does not already have, but if it has previously pulled them, will be stored in the Nexus repository's cache. This way if there are new releases, you can pull as required, but are not taking the entire repository as required. On the form that opens, you will add the following information (the name red_hat_ga needs to be exact to match the lab expectations): Name : red_hat_ga What type of artifacts does this repository store : Release Remote Storage : https://maven.repository.redhat.com/ga Click Create repository at the bottom to deploy the proxy instance of the Red Hat Maven General Access Repository Your repositories should now be setup and look similar to the below. First run of the Pipeline to configure Persistent Volume Claims for the workspaces With the first run of the pipeline, you may have to run the pipeline manually. To do so, login to the OpenShift console for your project. You can see the console URL by running oc whoami --show-console and that will return the console link, Command/Control click it to open from VS Code. Once logged in, make sure to go to our project that you created in the provision.sh script. Follow these steps to run the pipeline for the first time. Make sure you change your project to the one you created in provision.sh . From the cluster home page, you can click Pipelines and then click Pipelines From the Pipelines screen, confirm your namespace matches what you created in the first part of the lab and after that, click the kebab icon to open the menu to Start Pipeline . Once you click start pipeline, a form will come up to set some settings for the pipeline. These should only have to be the first time you run this pipeline. You will modify the ones below if you followed the previous section for the workspaces to align to the ConfigMap of settings.xml used by Maven to build and the Persistent Volumes used for workspace data (git clones and the Maven artifacts). maven-local-repo : click the dropdown menu and select PersistentVolumeClaim and in the Select a PVC dropdown that appears, select maven-repo-pvc maven-settings : click the drop down and select Config Map and select from the new dropdown custom-maven-settings shared-workspace : click the dropdown menu and select PersistentVolumeClaim and in the Select a PVC dropdown, select source-workspace-pvc With this complete you can click the Start button to begin the pipeline Testing GitHub and Pipeline integration If you run this test, a new deployment should be triggered. The pipeline will deploy the decision service for the first time. In your terminal, access your project folder. Commit and push. You can use this empty commit sample if you need: git commit -m \"an empty commit to test the pipeline\" --allow-empty git push origin master In OpenShift, access: \" Pipelines -> ba-cicd-pipeline -> Pipeline Runs \" and check the progress of your application deployment. Using the web application The web application allows you to interact with the deployed rules and decisions in a specific Decision Server (KieServer or Kogito runtime). To use the deployed web app to interact with the deployed decisions, first you need to set the KIE Server URL in the web app settings. The deployed decision service is now deployed and accessible. Get your deployed KIE Server route. You can use the command: echo \"http://\" $( oc get route business-application-service-route -n rhdm-kieserver-cicd | awk 'FNR > 1 {print $2}' ) \"/rest/server\" Open your web application. The URL was provided in the installation step. If you lost it, use the command oc get route decision-service-webclient --template = 'http://{{.spec.host}}' -n rhdm-kieserver-cicd In the web application, click on the settings icon on the top right corner. In the field Kie Server Base URL , insert KIE Server URL. You can use the \"Test Connection\" button to validate the communication between the two services, then Save. You should be able to test the available decisions and rules. With this, the whole demo is now set up and ready to use. NOTE: If you get interested in see how this webapp was developed the src code is available here Extra information The provisioning script provision.sh will: Create a new namespace called rhdm-kieserver-cicd Install OpenShift Pipelines Create the pipeline resources Deploy a front-end application that you can use to interact with the decision service once you deploy it. At the moment there are 4 projects in this repository: decisions-showcase : Decision use cases using Business Rules (Drools) and Decision Logic (DMN) business-application-service : Spring Boot runtime based Kie Server exposing the API for Decisions provided with this Showcase demo cicd : Tekton Pipeline resources to implement a fully automated CI/CD pipeline for your Business Application Services monitoring : working in progress... To see a detailed instruction on each service and each deployment processes (with images), check: Provisioning and testing the CI/CD Pipeline Provisioning and testing the webclient application Interested in Kogito? Check out the kogito-quarkus branch to see this same demo but using Kogito based Decision Services instead of KieServer.","title":"IBM Business Automation Open Edition 9.2.x CI/CD Overview"},{"location":"guided_exercises/03c_CICD/introduction/#ibm-business-automation-open-edition-92x-cicd-overview","text":"This lab was forked from the KIE Live session delived by Rafael Soares on YouTube found here . This lab has been updated to more current releases and also includes a provisioning of Nexus3 to deploy a Maven Repository in your environment. You do not need to do this if you have a Maven repository you can deploy your artifacts to. This lab will be using the OpenShift provided Pipelines built upon Tekton. With IBM Business Automation Open Edition 9.2.x, the projects are based on Maven artifact architectures, which provides an easy to build and deploy mechanism for all of your product needs. These can be done in Tekton as seen here, or with your favorite/enterprise provided CI/CD tools. The project in this lab is a Spring Boot hosted Decision Service that is utilizing the embedded KIE Server that will have a build and deploy triggered through the changes in your code base that are pushed to the specified build branch. With Decision Service projects, it is highly recommended that on all push events you consider a build of the project. This ensures that all the resources that are required are constantly available, breaking changes are identified immediately and if you want to release immediately, it's very easy to do so.","title":"IBM Business Automation Open Edition 9.2.x CI/CD Overview"},{"location":"guided_exercises/03c_CICD/introduction/#lab-overview","text":"Within this lab you will see how you can use OpenShift Pipelines (a.k.a Tekton) to automate the delivery of decision services implemented with BAMOE. In this lab you will see: The automation of repeatable decisions using the DMN specification; Decision tables implementation using XLS. Usage of the rules engine based on KIE Server and running on top of SpringBoot CI/CD Pipeline implemented using Tekton How to configure webhooks in your pipeline to deploy based on changes on a git repository Automated tests for decisions (with Test Scenarios ) that are considered during the pipeline execution Deployment with zero downtime with OpenShift rolling deployment strategy","title":"Lab Overview"},{"location":"guided_exercises/03c_CICD/introduction/#pre-requisites","text":"Java 8 OpenShift 4.10+ OpenShift Command Line client ( oc ) VSCode VSCode Business Automation Extension","title":"Pre-requisites"},{"location":"guided_exercises/03c_CICD/introduction/#installing-on-openshift","text":"Fork this repository, to get started . Clone your fork to your local machine. git clone https://github.com/ ${ yourgithubuser } /business-automation-showcase.git cd business-automation-showcase Run the provisioning script (Linux/MacOS): sh provision.sh When the script runs, you will be presented with a prompt to enter a namespace name, which is the namespace that will be created for your environment to be deployed in. business-automation-cicd-showcase % ./provision.sh Input a namespace root - first letter lowercase namespace: tim-demo When this completes you will get a console with a few links, as well as an admin password to the Nexus environment that was created. ****************************************************************** Use this URL in your GitHub Webhook configuration for automatic deployment http://el-ba-cicd-event-listener-tim-demo-rhdm-kieserver-cicd.openshift.io Use this URL to access the front-end application: http://decision-service-webclient-tim-demo-rhdm-kieserver-cicd.openshift.io Use this URL to access the Nexus Repository: http://nexus3-tim-demo-rhdm-kieserver-cicd.openshift.io Use this password for admin access to Nexus 3: long-strung-out-password-used-for-nexus-repo-admin ******************************************************************","title":"Installing on OpenShift"},{"location":"guided_exercises/03c_CICD/introduction/#configuring-the-automatic-deployment-on-github","text":"To configure the webhook for automated deployment, open your fork in your GitHub. Next, add a new webhook by opening \" Settings -> Webhook -> Add webhook button\". Fill the form with the information below: Payload URL : provided after the provisioning. You can also get it using the command: echo $( oc get route el-ba-cicd-event-listener --template = 'http://{{.spec.host}}' ) Content type : application/json Secret : empty Which events would you like to trigger this webhook? : Just the push event . At this point, you should already have a fully automated integration and deployment lifecycle for the business application. Any changes pushed to your repository will trigger the pipeline in your OpenShift cluster.","title":"Configuring the automatic deployment on GitHub"},{"location":"guided_exercises/03c_CICD/introduction/#maven-repository-setup","text":"Typically an environment would already have a Maven repository included, but if you wanted to set one up and use your own, this is a quick run through of what's required to get the Maven Repository to connect to the current location of the IBM Business Automation Open Edition 9.2.x Maven artifacts which reside in the Red Hat Maven General Access Repository. From the console log you got earlier, you would have received two messages with routes and password infromation for the Nexus repository location (the 3rd item) and the admin password (the commands to get these are found within the provision.sh file). If you hold Control (Linux/Windows) or Command (Mac) and click the link in VSCode, you will go to the location of your newly provisioned Nexus3 repository. By default, the Nexus repository includes a Mirror of Maven Central which anyone can publish to following this documentation and is the typical environment for Community releases, but enterprise releases of Open Source Software can be found here as well. The practice within Red Hat, has been to publish artifacts to its own repository found here as these are the enterprise releases and outside of community. No access is required to pull from this repository. We will configure our newly minted Nexus repository to have a Mirror of the Red Hat Maven General Access Repository so that for any product releases that are used, it will pull them from Red Hat Maven General Access Repository and publish them in the Nexus repository as a proxy. This is common in most enterprise environments instead of a direct connection to the repositories to offset downtime, have security checks, etc. To setup the mirrow follow these steps: From the route that was produced from the console output (or you can run oc get route | grep nexus where you will need to paste the output) and use that link to get to the Nexus admin screen. From here, using the admin password that was a part of your console output. Click Sign In . Alternatively the command to run is below. echo \" $( oc exec $( oc get pod -o template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}' | grep nexus ) -- cat /nexus-data/admin.password ) \" Copying the password, login as Admin with the password from the previous command. You will now have a Wizard popup to walk you through some setup, we're going to do very minimal changes: you can change the password (e.g. password ). Typically in this development/throwaway environment, enabling anonymous access is the best way to do this, so select that option Enable anonymous access . Then click finish. Now that the repository is ready to be configured, we are going to create a mirror of the Red Hat Maven General Access Repository so that we can pull resources through the proxy instead of direct. To do this, click the Gear icon ( ) to open the settings screen. From here click Repositories to view the current repositories and also to be able to create another. Here you will see a list of default repositorues. By default, Nexus will include the connection to Maven_Central , but we'd also like to add the Red Hat Maven General Access Repository connection to pull the resources from this repository. To do so, click Create Repository to build a new repository. Scroll down and we're going to create a maven2 (proxy) . The reason we're doing a proxy is that we're going to allow our repository to reach out to the Red Hat Maven General Access Repository to retrieve artifacts it does not already have, but if it has previously pulled them, will be stored in the Nexus repository's cache. This way if there are new releases, you can pull as required, but are not taking the entire repository as required. On the form that opens, you will add the following information (the name red_hat_ga needs to be exact to match the lab expectations): Name : red_hat_ga What type of artifacts does this repository store : Release Remote Storage : https://maven.repository.redhat.com/ga Click Create repository at the bottom to deploy the proxy instance of the Red Hat Maven General Access Repository Your repositories should now be setup and look similar to the below.","title":"Maven Repository Setup"},{"location":"guided_exercises/03c_CICD/introduction/#first-run-of-the-pipeline-to-configure-persistent-volume-claims-for-the-workspaces","text":"With the first run of the pipeline, you may have to run the pipeline manually. To do so, login to the OpenShift console for your project. You can see the console URL by running oc whoami --show-console and that will return the console link, Command/Control click it to open from VS Code. Once logged in, make sure to go to our project that you created in the provision.sh script. Follow these steps to run the pipeline for the first time. Make sure you change your project to the one you created in provision.sh . From the cluster home page, you can click Pipelines and then click Pipelines From the Pipelines screen, confirm your namespace matches what you created in the first part of the lab and after that, click the kebab icon to open the menu to Start Pipeline . Once you click start pipeline, a form will come up to set some settings for the pipeline. These should only have to be the first time you run this pipeline. You will modify the ones below if you followed the previous section for the workspaces to align to the ConfigMap of settings.xml used by Maven to build and the Persistent Volumes used for workspace data (git clones and the Maven artifacts). maven-local-repo : click the dropdown menu and select PersistentVolumeClaim and in the Select a PVC dropdown that appears, select maven-repo-pvc maven-settings : click the drop down and select Config Map and select from the new dropdown custom-maven-settings shared-workspace : click the dropdown menu and select PersistentVolumeClaim and in the Select a PVC dropdown, select source-workspace-pvc With this complete you can click the Start button to begin the pipeline","title":"First run of the Pipeline to configure Persistent Volume Claims for the workspaces"},{"location":"guided_exercises/03c_CICD/introduction/#testing-github-and-pipeline-integration","text":"If you run this test, a new deployment should be triggered. The pipeline will deploy the decision service for the first time. In your terminal, access your project folder. Commit and push. You can use this empty commit sample if you need: git commit -m \"an empty commit to test the pipeline\" --allow-empty git push origin master In OpenShift, access: \" Pipelines -> ba-cicd-pipeline -> Pipeline Runs \" and check the progress of your application deployment.","title":"Testing GitHub and Pipeline integration"},{"location":"guided_exercises/03c_CICD/introduction/#using-the-web-application","text":"The web application allows you to interact with the deployed rules and decisions in a specific Decision Server (KieServer or Kogito runtime). To use the deployed web app to interact with the deployed decisions, first you need to set the KIE Server URL in the web app settings. The deployed decision service is now deployed and accessible. Get your deployed KIE Server route. You can use the command: echo \"http://\" $( oc get route business-application-service-route -n rhdm-kieserver-cicd | awk 'FNR > 1 {print $2}' ) \"/rest/server\" Open your web application. The URL was provided in the installation step. If you lost it, use the command oc get route decision-service-webclient --template = 'http://{{.spec.host}}' -n rhdm-kieserver-cicd In the web application, click on the settings icon on the top right corner. In the field Kie Server Base URL , insert KIE Server URL. You can use the \"Test Connection\" button to validate the communication between the two services, then Save. You should be able to test the available decisions and rules. With this, the whole demo is now set up and ready to use. NOTE: If you get interested in see how this webapp was developed the src code is available here","title":"Using the web application"},{"location":"guided_exercises/03c_CICD/introduction/#extra-information","text":"The provisioning script provision.sh will: Create a new namespace called rhdm-kieserver-cicd Install OpenShift Pipelines Create the pipeline resources Deploy a front-end application that you can use to interact with the decision service once you deploy it. At the moment there are 4 projects in this repository: decisions-showcase : Decision use cases using Business Rules (Drools) and Decision Logic (DMN) business-application-service : Spring Boot runtime based Kie Server exposing the API for Decisions provided with this Showcase demo cicd : Tekton Pipeline resources to implement a fully automated CI/CD pipeline for your Business Application Services monitoring : working in progress... To see a detailed instruction on each service and each deployment processes (with images), check: Provisioning and testing the CI/CD Pipeline Provisioning and testing the webclient application","title":"Extra information"},{"location":"guided_exercises/03c_CICD/introduction/#interested-in-kogito","text":"Check out the kogito-quarkus branch to see this same demo but using Kogito based Decision Services instead of KieServer.","title":"Interested in Kogito?"},{"location":"guided_exercises/04_dmn/advanced-la-deprecated/","text":"Create a Decision Project To define and deploy a DMN decision model, we first need to create a new project in which we can store the model. To create a new project: Navigate to IBM Business Automation Manager Canvas Login to the platform with the provided username and password. Click on Design to navigate to the Design perspective. In the Design perspective, create a new project. If your space is empty, this can be done by clicking on the blue Add Project button in the center of the page. If you already have projects in your space, you can click on the blue Add Project icon at the top right of the page. Give the project the name call-centre-decisions , and the description \"Call Centre Decisions\". With the project created, we can now create our DMN model. Click on the blue Add Asset button. In the Add Asset page, select Decision in the dropdown filter selector. Click on the DMN tile to create a new DMN model. Give it the name call-centre . This will create the asset and open the DMN editor. Next Steps You can do this lab in 2 ways: If you already have (some) DMN knowledge, we would like to challenge you to build the solution by yourself. After you\u2019ve built solution, you can verify your answer by going to the next module in which we will explain the solution and will deploy it onto the runtime. Follow this step-by-step guide which will guide you through the implementation.","title":"Advanced la deprecated"},{"location":"guided_exercises/04_dmn/advanced-la-deprecated/#create-a-decision-project","text":"To define and deploy a DMN decision model, we first need to create a new project in which we can store the model. To create a new project: Navigate to IBM Business Automation Manager Canvas Login to the platform with the provided username and password. Click on Design to navigate to the Design perspective. In the Design perspective, create a new project. If your space is empty, this can be done by clicking on the blue Add Project button in the center of the page. If you already have projects in your space, you can click on the blue Add Project icon at the top right of the page. Give the project the name call-centre-decisions , and the description \"Call Centre Decisions\". With the project created, we can now create our DMN model. Click on the blue Add Asset button. In the Add Asset page, select Decision in the dropdown filter selector. Click on the DMN tile to create a new DMN model. Give it the name call-centre . This will create the asset and open the DMN editor.","title":"Create a Decision Project"},{"location":"guided_exercises/04_dmn/advanced-la-deprecated/#next-steps","text":"You can do this lab in 2 ways: If you already have (some) DMN knowledge, we would like to challenge you to build the solution by yourself. After you\u2019ve built solution, you can verify your answer by going to the next module in which we will explain the solution and will deploy it onto the runtime. Follow this step-by-step guide which will guide you through the implementation.","title":"Next Steps"},{"location":"guided_exercises/04_dmn/advanced-lab-authoring/","text":"Authoring the decisioning for the Call Center With this lab, it is highly recommended to use DMN Runner and while you're building your model and the various custom data types to have the example right next to you. Taking advantage of the tools at hand will make your decision design much smoother and faster! The problem statement describes a number of different inputs to our decision: Call : the incoming call into the call-center Employees : the employees of certain office. Office : an office to which the call could potentially be routed. Furthermore, the problem statement describes that phone numbers could be banned. So, also banned numbers can be regarded as an input to our model (although we will not implement it as an input in this lab). With the given input, we need to make the following decisions: Accept Call : the final decision we need to make is whether the given office will accept the call. Can Handle Call : whether the office can actually accept the call. As defined in the problem statement, this depends on: whether the phone number has been banned; the purpose of the phone call (\u201chelp\u201d or \u201cobjection\u201d). . Accept Call Decision Structure Accept Call : the final decision we need to make is whether the given office will accept the call. Add a Decision node to the diagram by clicking on the Decision node icon and placing it in the DRD. Double-click on the node to set the name. We will name this node Accept Call . With the Accept Call node selected, open the property panel. Set the Output data type to boolean . The input of this decision is the incoming call , office and employee . Create these 3 input nodes and connect them to the Accept Call decision. We can now set data types of our input nodes. Click on the incoming call node, open the property panel and in the Output data type section and click on the Manage button. This will open the Custom Data Types window. In the Custom Data Types window, click on the + Add button. Define the data type Phone Number as a structure and be sure to press the blue checkmark. When you click the blue checkmark, a new row will open for the next type, for the first element, call it phone number with type string and then press the blue checkmark. Now to create one more item, press the plus sign in the circle that appears where the checkmark was. For this data type call it country prefix with a type string . You can add a constraint to the limited values if you want, but not required, the example will use \"+421\" and \"+420\". These are more used for the forms rendering than anything else at this time. Make sure you check the blue checkmark when you're finished. Define another data type tCall as a Structure which will contain both the tPhoneNumber structure and an additional field purpose of type string : When you\u2019ve created the tCall type, go back to the DRD by clicking on the Editor tab. Select the incoming call node, and in the property panel, under Information item , set the node\u2019s Output data type to tCall Next, define the following data type similar to tCall called tOffice with a field location and set it as the Output data type of the office input as such: Define the data type for employees as follows. Note that we\u2019ve first defined the type tEmployee , and afterwards we\u2019ve defined tEmployees as a List of tEmployee . Decision Service With the main structure defined, we can now look at the requirements of the decision whether the office can actually accept the call. As defined in the problem statement, this depends on: whether the phone number has been banned. the purpose of the phone call (\"help\" or \"objection\"). We will model this decision as a DMN Decision Service that can be called by our main decision Accept Call . First, model the Decision Service in the DRD and give it the name Can Handle Call . Set it\u2019s Output data type to boolean . Add a Decision Node to the Decision Service . Name it Call Can Be Handled and set it\u2019s Output data type to boolean . Add 2 additional Decision Nodes and name them Is Banned and Call Purpose Accepted . Both should have an Output data type of type boolean . These will be put \"below the bar\" on the Decision Service node, so the decisions will be evaluated, but the results will not be passed back from final service by default. Connect the 2 Decision Nodes to the Call Can Be Handled node. The input to both the Is Banned and Call Purpose Accepted decisions is going to be a separate item called call . You will create the input node called call with data type tCall . While it will look like you're creating a separate call item here (and that is definitely how it looks), this decision service could be called independently of the whole service, which is why this call object can be used. Later in the lab, you will see how this ultimately works! The Is Banned decision also needs a collection of banned phone numbers. Instead of implementing this as an Input node, we will implement this as a DMN Relation Decision . Create a new Decision Node and name it Banned Phone Numbers . Connect it to the Is Banned decision node. The Ouput data type of this nodes is a new custom data type, which is a list of tPhoneNumber . We\u2019ll name this type tPhoneNumbers , similar to how tEmployees was setup before: Click on the Edit button of the Banned Phone Numbers node. Set the logic type of the decision to Relation . Create the following table, you will need to add one extra column and use the headers as shown of country prefix with type string and phone number with type string : We can now implement the logic of the Is Banned decision. Click on the Edit button of the decision node. We will implement the logic as a Literal Expression . Define the following FEEL expression as below showing that we're going to take the Relational List from Banned Phone Numbers and comparing it to call.phone which is using the tCall's phone number input: list contains ( Banned Phone Numbers , call . phone ) The next node for which we want to implement the decision logic is Call Purpose Accepted . Click on the node, and click on the Edit button. Implement the following logic as a Decision Table : Click on the Call Can Be Handled node and click on the node\u2019s Edit button. In the decision editor, click the Select expression to Decision Table and implement the following table: We can now implement the decision of Accept Call Decision . To do this we need to link the Decision Service to the Accept Call , to do this, you will click the outside of the Decision Service node and use the Create DMN Knowledge Requirement to connect the Decision Service to the Accept Call . \"Accept Call\" Decision Logic Finally we need to Implement the Accept Call decision. This is going to be built as a Context since this can be built as a boxed expression that will have multiple steps yielding the final decision to either Accept or Deny the call! With the Context now the Decision Type. The first row is going to be Call can be handled with type boolean . Now we need to set the expression for the first row, this is going to be of type Invocation . For the invocation, we need to use it to call the Decision Service that was created. Notice that the line 1 is the invocation of the decision service \"Can Handle Call\". This is an Invocation of the Can Handle Call service, passing the incoming call input as the variable call . The output of this invocation will be the boolean variable Call can be handled . This will be done using a Literal FEEL function to do this. Next create a new line with a data type Employee at the Office with data type boolean . Set the expression as another Literal expression. In this function, you will be mapping the Office locations of the employees who are working to the location of the office location. Since this is a list, you're essentially getting a collection of the employees office location and the location of the office that the call is coming from and trying to map them. employees [ office location = office.location ] After this is set, the last row, <result> can be set with a Literal expression again with the following: if Call can be handled then count ( Employee at the Office ) > 0 else false The Call can be handled variable as then used to validate the decision result in the last line. Next steps Next, we should deploy the project in OpenShift as a sample service.","title":"Call Centre - Authoring Decisions"},{"location":"guided_exercises/04_dmn/advanced-lab-authoring/#authoring-the-decisioning-for-the-call-center","text":"With this lab, it is highly recommended to use DMN Runner and while you're building your model and the various custom data types to have the example right next to you. Taking advantage of the tools at hand will make your decision design much smoother and faster! The problem statement describes a number of different inputs to our decision: Call : the incoming call into the call-center Employees : the employees of certain office. Office : an office to which the call could potentially be routed. Furthermore, the problem statement describes that phone numbers could be banned. So, also banned numbers can be regarded as an input to our model (although we will not implement it as an input in this lab). With the given input, we need to make the following decisions: Accept Call : the final decision we need to make is whether the given office will accept the call. Can Handle Call : whether the office can actually accept the call. As defined in the problem statement, this depends on: whether the phone number has been banned; the purpose of the phone call (\u201chelp\u201d or \u201cobjection\u201d).","title":"Authoring the decisioning for the Call Center"},{"location":"guided_exercises/04_dmn/advanced-lab-authoring/#accept-call-decision-structure","text":"Accept Call : the final decision we need to make is whether the given office will accept the call. Add a Decision node to the diagram by clicking on the Decision node icon and placing it in the DRD. Double-click on the node to set the name. We will name this node Accept Call . With the Accept Call node selected, open the property panel. Set the Output data type to boolean . The input of this decision is the incoming call , office and employee . Create these 3 input nodes and connect them to the Accept Call decision. We can now set data types of our input nodes. Click on the incoming call node, open the property panel and in the Output data type section and click on the Manage button. This will open the Custom Data Types window. In the Custom Data Types window, click on the + Add button. Define the data type Phone Number as a structure and be sure to press the blue checkmark. When you click the blue checkmark, a new row will open for the next type, for the first element, call it phone number with type string and then press the blue checkmark. Now to create one more item, press the plus sign in the circle that appears where the checkmark was. For this data type call it country prefix with a type string . You can add a constraint to the limited values if you want, but not required, the example will use \"+421\" and \"+420\". These are more used for the forms rendering than anything else at this time. Make sure you check the blue checkmark when you're finished. Define another data type tCall as a Structure which will contain both the tPhoneNumber structure and an additional field purpose of type string : When you\u2019ve created the tCall type, go back to the DRD by clicking on the Editor tab. Select the incoming call node, and in the property panel, under Information item , set the node\u2019s Output data type to tCall Next, define the following data type similar to tCall called tOffice with a field location and set it as the Output data type of the office input as such: Define the data type for employees as follows. Note that we\u2019ve first defined the type tEmployee , and afterwards we\u2019ve defined tEmployees as a List of tEmployee .","title":". Accept Call Decision Structure"},{"location":"guided_exercises/04_dmn/advanced-lab-authoring/#decision-service","text":"With the main structure defined, we can now look at the requirements of the decision whether the office can actually accept the call. As defined in the problem statement, this depends on: whether the phone number has been banned. the purpose of the phone call (\"help\" or \"objection\"). We will model this decision as a DMN Decision Service that can be called by our main decision Accept Call . First, model the Decision Service in the DRD and give it the name Can Handle Call . Set it\u2019s Output data type to boolean . Add a Decision Node to the Decision Service . Name it Call Can Be Handled and set it\u2019s Output data type to boolean . Add 2 additional Decision Nodes and name them Is Banned and Call Purpose Accepted . Both should have an Output data type of type boolean . These will be put \"below the bar\" on the Decision Service node, so the decisions will be evaluated, but the results will not be passed back from final service by default. Connect the 2 Decision Nodes to the Call Can Be Handled node. The input to both the Is Banned and Call Purpose Accepted decisions is going to be a separate item called call . You will create the input node called call with data type tCall . While it will look like you're creating a separate call item here (and that is definitely how it looks), this decision service could be called independently of the whole service, which is why this call object can be used. Later in the lab, you will see how this ultimately works! The Is Banned decision also needs a collection of banned phone numbers. Instead of implementing this as an Input node, we will implement this as a DMN Relation Decision . Create a new Decision Node and name it Banned Phone Numbers . Connect it to the Is Banned decision node. The Ouput data type of this nodes is a new custom data type, which is a list of tPhoneNumber . We\u2019ll name this type tPhoneNumbers , similar to how tEmployees was setup before: Click on the Edit button of the Banned Phone Numbers node. Set the logic type of the decision to Relation . Create the following table, you will need to add one extra column and use the headers as shown of country prefix with type string and phone number with type string : We can now implement the logic of the Is Banned decision. Click on the Edit button of the decision node. We will implement the logic as a Literal Expression . Define the following FEEL expression as below showing that we're going to take the Relational List from Banned Phone Numbers and comparing it to call.phone which is using the tCall's phone number input: list contains ( Banned Phone Numbers , call . phone ) The next node for which we want to implement the decision logic is Call Purpose Accepted . Click on the node, and click on the Edit button. Implement the following logic as a Decision Table : Click on the Call Can Be Handled node and click on the node\u2019s Edit button. In the decision editor, click the Select expression to Decision Table and implement the following table: We can now implement the decision of Accept Call Decision . To do this we need to link the Decision Service to the Accept Call , to do this, you will click the outside of the Decision Service node and use the Create DMN Knowledge Requirement to connect the Decision Service to the Accept Call .","title":"Decision Service"},{"location":"guided_exercises/04_dmn/advanced-lab-authoring/#accept-call-decision-logic","text":"Finally we need to Implement the Accept Call decision. This is going to be built as a Context since this can be built as a boxed expression that will have multiple steps yielding the final decision to either Accept or Deny the call! With the Context now the Decision Type. The first row is going to be Call can be handled with type boolean . Now we need to set the expression for the first row, this is going to be of type Invocation . For the invocation, we need to use it to call the Decision Service that was created. Notice that the line 1 is the invocation of the decision service \"Can Handle Call\". This is an Invocation of the Can Handle Call service, passing the incoming call input as the variable call . The output of this invocation will be the boolean variable Call can be handled . This will be done using a Literal FEEL function to do this. Next create a new line with a data type Employee at the Office with data type boolean . Set the expression as another Literal expression. In this function, you will be mapping the Office locations of the employees who are working to the location of the office location. Since this is a list, you're essentially getting a collection of the employees office location and the location of the office that the call is coming from and trying to map them. employees [ office location = office.location ] After this is set, the last row, <result> can be set with a Literal expression again with the following: if Call can be handled then count ( Employee at the Office ) > 0 else false The Call can be handled variable as then used to validate the decision result in the last line.","title":"\"Accept Call\" Decision Logic"},{"location":"guided_exercises/04_dmn/advanced-lab-authoring/#next-steps","text":"Next, we should deploy the project in OpenShift as a sample service.","title":"Next steps"},{"location":"guided_exercises/04_dmn/advanced-lab-deployment/","text":"Deploying and testing the Decision Service With our decision model completed, we can now package our DMN model in a Kogito service. This can be done using the Accelerators included with IBM Business Automation Manager Canvas. These accelerators will quickly take the DMN file and start building the basis of the project as a IBM Business Automation Open Edition 9.2.x compatible decision service. Applying the Accelerator To take this decision service from a standalone DMN model to a full Kogito architected decision, you will use the accelerator by clickig the button at the top of the screen. To do so follow the instructions and then we will synchronize the project to your GitHub account and also be capable being deployed as an OpenShift service. At the top of the DMN model page for call-center-decisions , click the button Apply Accelerator and select Quarkus... . If you want to learn more about the accelerator, you can click the GitHub link and change the branch to the 9.0.0-quarkus-full to learn more about it. Afterwards just click Apply to restructure your project into a Kogito service. The wizard will come with a pop-up asking for a commit message for the change since this will create a Git project. You can use the default message or put whatever you would like, to do so press Commit . To create the project in Git, click Share , use your GitHub tokened ID. Once you select your ID, a new option Create GitHub repository... is now available, select this. The repository you create, can be anything you want, for the purposes of this lab, it will be called techxchange-call-center . When you create the repository, a message will open at the top with a link to the Git repository that was created. In the case of the example it is at this example repository . Deploying to OpenShift Now that this repository is created, you can work on it locally or deploy it to OpenShift. For this lab, we will choose OpenShift since we have a cluster ready to use for TechXchange or you can use the Red Hat OpenShift Sandbox to provision a small environment to try it out on your own. Login to get an OpenShift Token using the bookmark on Chrome. This will be used to connect IBM Business Automation Manager Canvas to OpenShift and deploy a sample development environment of the decision service. In the login screen use your username and password - this will be some combination of student01 to student20 with password Passw0rd . The username is all lowercase. This will Click the Display Token link and switch back to the tab with IBM Business Automation Manager Canvas open on it. You will use the token contents from the OpenShift token page shortly by clicking on Dev deployments and then selecting the pull down Select authentication and then Connect to an account . On the next wizard, select the OpenShift Tile to connect to an OpenShift instance. From here you will use the information from the OpenShift token page. Your namespace will be your student##-namespace , so if your username is student07, then you would connect to student07-namespace . The token and the console locations are both a part of the token as well. You will see a checkbox to Insecurely disable TLS validation, check it as this is a self contained environment with certificates not present. Press Connect when the form is completed. As long as it was successful to connect, you will be greeted with a Connect to OpenShift successful modal, click Continue . Now you can press the Deploy button and have the capability to select the namespace that you connected to and deploy a sample of your service to it. From here a modal will pop up explaining that the deployment can take a few minutes and it is only intended for development, etc. Click Confirm on this modal and your service will be deployed. This process will take several minutes to complete. The pulldown with deploy will refresh every 30 seconds, when the service is available, it will reflect as such. The way that the deployments from these services work are is that they are completely immutable and unaware of one another, so you could have all versions deployed as you make changes to validate how you want to eventually promote your DMN model to later environments. So if you were to deploy it again, there would be multiple different versions of it deployed. IBM Business Automation Manager Canvas can manage these deployment samples, which can later be removed when you're done with them if you choose. If you click the green check marked box, it will take you to the service. When you access the service with a form similar to the one you had in IBM Business Automation Manager Canvas and from that page you can explore it more using the kebab icon in the top right to access the Swagger-UI page to get access to the auto-generated DMN service's API page. The first post can be used to execute the decision itself. This completes this lab as you can see how the deployment to OpenShit works with IBM Business Automation Manager Canvas!","title":"Call Centre - Consuming Decisions"},{"location":"guided_exercises/04_dmn/advanced-lab-deployment/#deploying-and-testing-the-decision-service","text":"With our decision model completed, we can now package our DMN model in a Kogito service. This can be done using the Accelerators included with IBM Business Automation Manager Canvas. These accelerators will quickly take the DMN file and start building the basis of the project as a IBM Business Automation Open Edition 9.2.x compatible decision service.","title":"Deploying and testing the Decision Service"},{"location":"guided_exercises/04_dmn/advanced-lab-deployment/#applying-the-accelerator","text":"To take this decision service from a standalone DMN model to a full Kogito architected decision, you will use the accelerator by clickig the button at the top of the screen. To do so follow the instructions and then we will synchronize the project to your GitHub account and also be capable being deployed as an OpenShift service. At the top of the DMN model page for call-center-decisions , click the button Apply Accelerator and select Quarkus... . If you want to learn more about the accelerator, you can click the GitHub link and change the branch to the 9.0.0-quarkus-full to learn more about it. Afterwards just click Apply to restructure your project into a Kogito service. The wizard will come with a pop-up asking for a commit message for the change since this will create a Git project. You can use the default message or put whatever you would like, to do so press Commit . To create the project in Git, click Share , use your GitHub tokened ID. Once you select your ID, a new option Create GitHub repository... is now available, select this. The repository you create, can be anything you want, for the purposes of this lab, it will be called techxchange-call-center . When you create the repository, a message will open at the top with a link to the Git repository that was created. In the case of the example it is at this example repository .","title":"Applying the Accelerator"},{"location":"guided_exercises/04_dmn/advanced-lab-deployment/#deploying-to-openshift","text":"Now that this repository is created, you can work on it locally or deploy it to OpenShift. For this lab, we will choose OpenShift since we have a cluster ready to use for TechXchange or you can use the Red Hat OpenShift Sandbox to provision a small environment to try it out on your own. Login to get an OpenShift Token using the bookmark on Chrome. This will be used to connect IBM Business Automation Manager Canvas to OpenShift and deploy a sample development environment of the decision service. In the login screen use your username and password - this will be some combination of student01 to student20 with password Passw0rd . The username is all lowercase. This will Click the Display Token link and switch back to the tab with IBM Business Automation Manager Canvas open on it. You will use the token contents from the OpenShift token page shortly by clicking on Dev deployments and then selecting the pull down Select authentication and then Connect to an account . On the next wizard, select the OpenShift Tile to connect to an OpenShift instance. From here you will use the information from the OpenShift token page. Your namespace will be your student##-namespace , so if your username is student07, then you would connect to student07-namespace . The token and the console locations are both a part of the token as well. You will see a checkbox to Insecurely disable TLS validation, check it as this is a self contained environment with certificates not present. Press Connect when the form is completed. As long as it was successful to connect, you will be greeted with a Connect to OpenShift successful modal, click Continue . Now you can press the Deploy button and have the capability to select the namespace that you connected to and deploy a sample of your service to it. From here a modal will pop up explaining that the deployment can take a few minutes and it is only intended for development, etc. Click Confirm on this modal and your service will be deployed. This process will take several minutes to complete. The pulldown with deploy will refresh every 30 seconds, when the service is available, it will reflect as such. The way that the deployments from these services work are is that they are completely immutable and unaware of one another, so you could have all versions deployed as you make changes to validate how you want to eventually promote your DMN model to later environments. So if you were to deploy it again, there would be multiple different versions of it deployed. IBM Business Automation Manager Canvas can manage these deployment samples, which can later be removed when you're done with them if you choose. If you click the green check marked box, it will take you to the service. When you access the service with a form similar to the one you had in IBM Business Automation Manager Canvas and from that page you can explore it more using the kebab icon in the top right to access the Swagger-UI page to get access to the auto-generated DMN service's API page. The first post can be used to execute the decision itself. This completes this lab as you can see how the deployment to OpenShit works with IBM Business Automation Manager Canvas!","title":"Deploying to OpenShift"},{"location":"guided_exercises/04_dmn/advanced-lab-intro/","text":"Call Centre - Intro and Use Case This is an advanced Decision Model & Notation lab that introduces DMN Decision Services, Relations, nested boxed expressions, etc. It also explores a number of different FEEL constructs and expressions like, for example, list contains . Goals Implement a DMN model using the IBM Business Automation Manager Canvas DMN editor Deploy the existing DMN project to OpenShift Problem Statement In this lab we will create a decision that determines if a call-center can take an incoming call. Whether a call will be accepted by a certain office depends on: The office accepts the call. There are employees currently available at the office. Whether the office can accepts a call depends on: whether the phone number has been banned the purpose of the phone call (\"help\" or \"objection\"). Create a new DMN Decision Similar to the first lab, we're going to use IBM Business Automation Manager Canvas to define and deploy a DMN decision model, we first need to create a new model: Navigate to IBM Business Automation Manager Canvas From the IBM Business Automation Manager Canvas landing page, you can create and edit various types of open-standards models in BPMN, DMN and PMML using the editors here. We will be creating a DMN model for this lab, so to do this, you can click New Decision to create a new DMN model. You will now have an empty canvas and can start working on your DMN model for designing the number of vacation days decision. Change the name at the top of the model from Untitled to call-center-decisions or whatever you want to call it. In the next section we will start to create the decision. Next Steps You can do this lab in 2 ways: If you already have (some) DMN knowledge, we would like to challenge you to build the solution by yourself. After you\u2019ve built solution, you can verify your answer by going to the next module in which we will explain the solution and will deploy it onto the runtime. Follow this step-by-step guide which will guide you through the implementation. To do this on your own without the walk-through your model will ultimately end up looking like the below model with 5 Decision Nodes ( Banned Phone Numbers, Is Banned, Call Purpose Accepted, Call Can Be Handled, Accept Call ). Four of these are built in a Decision Service to control what parts of the decision are exposed when calling the service and ultimately a final decision to Accept Call .","title":"Advanced DMN Exercises - Call Centre - Introduction"},{"location":"guided_exercises/04_dmn/advanced-lab-intro/#call-centre-intro-and-use-case","text":"This is an advanced Decision Model & Notation lab that introduces DMN Decision Services, Relations, nested boxed expressions, etc. It also explores a number of different FEEL constructs and expressions like, for example, list contains .","title":"Call Centre - Intro and Use Case"},{"location":"guided_exercises/04_dmn/advanced-lab-intro/#goals","text":"Implement a DMN model using the IBM Business Automation Manager Canvas DMN editor Deploy the existing DMN project to OpenShift","title":"Goals"},{"location":"guided_exercises/04_dmn/advanced-lab-intro/#problem-statement","text":"In this lab we will create a decision that determines if a call-center can take an incoming call. Whether a call will be accepted by a certain office depends on: The office accepts the call. There are employees currently available at the office. Whether the office can accepts a call depends on: whether the phone number has been banned the purpose of the phone call (\"help\" or \"objection\").","title":"Problem Statement"},{"location":"guided_exercises/04_dmn/advanced-lab-intro/#create-a-new-dmn-decision","text":"Similar to the first lab, we're going to use IBM Business Automation Manager Canvas to define and deploy a DMN decision model, we first need to create a new model: Navigate to IBM Business Automation Manager Canvas From the IBM Business Automation Manager Canvas landing page, you can create and edit various types of open-standards models in BPMN, DMN and PMML using the editors here. We will be creating a DMN model for this lab, so to do this, you can click New Decision to create a new DMN model. You will now have an empty canvas and can start working on your DMN model for designing the number of vacation days decision. Change the name at the top of the model from Untitled to call-center-decisions or whatever you want to call it. In the next section we will start to create the decision.","title":"Create a new DMN Decision"},{"location":"guided_exercises/04_dmn/advanced-lab-intro/#next-steps","text":"You can do this lab in 2 ways: If you already have (some) DMN knowledge, we would like to challenge you to build the solution by yourself. After you\u2019ve built solution, you can verify your answer by going to the next module in which we will explain the solution and will deploy it onto the runtime. Follow this step-by-step guide which will guide you through the implementation. To do this on your own without the walk-through your model will ultimately end up looking like the below model with 5 Decision Nodes ( Banned Phone Numbers, Is Banned, Call Purpose Accepted, Call Can Be Handled, Accept Call ). Four of these are built in a Decision Service to control what parts of the decision are exposed when calling the service and ultimately a final decision to Accept Call .","title":"Next Steps"},{"location":"guided_exercises/04_dmn/getting-started/","text":"Getting Started with Decision Model and Notation This lab introduces you to the deployment of an existing Decision Model and Notation (DMN) and validation of it's decisions. Explore an existing DMN file created using IBM Business Automation Manager Canvas Test the DMN using IBM Business Automation Manager Canvas's Extended Services Deploy the existing DMN project to OpenShift Test the deployed DMN Examine Existing DMN Diagram The following example describes an insurance price calculator based on an applicant\u2019s age and accident history. This is a simple decision process based on the following decision table: DMN Decision Table The decision table was designed with using a DMN editor, this could be done in IBM Business Automation Manager Canvas, VSCode, Business Central or realistically any DMN compatible design tool. With how IBM Business Automation Open Edition 9.2.x utilizes the DMN specification, we're easily able to import this diagram seamlessly in the tool that makes the most sense for our end user. Notice that: The DMN decision table includes a hit policy , inputs , and outputs . Unlike the classic Drools Rules Language (DRL) based decision tables that can be created in Business Central, input and output names in DMN decision tables accept spaces. This allows for more clarity in your decisions aligning more to a business acumen. The conditions for the Age input is defined using the Friendly Enough Expression Language (FEEL). The decision can also be represented by the following decision requirements diagram: In this decision requirements diagram, you may note that the applicant\u2019s age and accident history are found in squared ovals, these indicate that these are the required inputs for the decision table \"Insurance Total Price\". You can find the DMN component in the DMN GitHub repository . Import the DMN File into IBM Business Automation Manager Canvas In this section, you can import the GitHub repository to IBM Business Automation Manager Canvas directly from the file or clone the repository, if you do the direct file. Open IBM Business Automation Manager Canvas by navigating to your instance of it (within the lab environments these are at http://localhost:9090/#. From here, you will copy the URL to the gist of the Insurance Pricing DMN located at this repository location . https://raw.githubusercontent.com/timwuthenow/dmn-workshop-labs/master/policy-price/insurance-pricing.dmn With this link navigate to the IBM Business Automation Manager Canvas and under Import From URL paste the link from the previous step and click Import . When the project is imported, you will see the DMN Editor with the insurance-pricing DMN model displayed. If you instead of pointing to a particular DMN, pointed to an entire project, any DMN/BPMN models associated with it would be able to viewed/edited within IBM Business Automation Manager Canvas. You can then click the Run button to get a local copy of this DMN running within the browser session itself. The best part about the DMN specification is that it is both a modeling specification, but also has an execution specification which allows for it to be as portable as it is. This will have a section of the browser turn into a form and you can run the model right there. Modifying the checkbox based on the boolean of had previous incidents and set an Age based on the data type being a number. Info Within this form you may see that if you just put an age in that the result returns (null) . This is because the way that a typical form works when generated is that a non-checked box assumes null , which the rules by default do not check for null. To quickly get around this from the form you can check and uncheck the checkbox and move forward with your testing. This initializes the value, this is because the form when it sends it for initial execution has the boolean value as a null. How would you address this in the rules? HINT : since we're checking if the number of incidents has a checkbox or not, you can write it as ? = false or ? = null . Why the ? ? This is a way that DMN through the Friendly Enough Expression Language uses the column header object that you're testing to simplify checking a variable against a condition! This service can be deployed as a sample Quarkus service to OpenShift right away as a sample service that you can invoke immediately. In the next section we will show how this is done from a development point of view. Deploying this sample service to OpenShift If you are using the provided lab environment for TechXchange, the environment itself provides access to a shared cluster for all attendees to login to OpenShift there is a bookmark in Chrome called OpenShift Token that will take you to a login page to create an OpenShift login token. Every user for the TechXchange environment has a created namespace to the student id that was given to your workstation at the start of the lab (student01 through student20 with the namespace to that username with -namespace ). The password for all of these instances is Passw0rd . To create the OpenShift token to connect to the console at the bookmarked link OpenShift Token from Chrome. From here you will login to OpenShift using your username and password created for the lab which is of the format student01 through student20 with password Passw0rd . If you get security issues with the link, accept them using the Advanced option in Chrome to proceed to the login screen. After logging in to the token screen, you will need to copy the token, server and use your assigned namespace ( student##-namespace ) to login. Please be sure to check the Insecurely disable TLS certificate validation as this is a local instance of OpenShift and does not have all the proper certificates in place. After you're successfully connected, you can use the deploy feature found on IBM Business Automation Manager Canvas by clicking the Deploy button and selecting the insurance-pricing decision to deploy. Confirm to deploy it to your namespace and let it load for approximately 30-45 seconds after. Then you can click the link to load the Quarkus service that was created from the DMN file. Just a note you need to click the checkbox for had previous incidents to initialize the value . Conclusion Congratulations, you've finished the first part of the DMN exercise. Next, you will have an intermediate level exercise that will guide you through the implementation, deployment and testing of the Vacation Days use case.","title":"Getting Started DMN Exercises - Insurance Price - Getting Started"},{"location":"guided_exercises/04_dmn/getting-started/#getting-started-with-decision-model-and-notation","text":"This lab introduces you to the deployment of an existing Decision Model and Notation (DMN) and validation of it's decisions. Explore an existing DMN file created using IBM Business Automation Manager Canvas Test the DMN using IBM Business Automation Manager Canvas's Extended Services Deploy the existing DMN project to OpenShift Test the deployed DMN","title":"Getting Started with Decision Model and Notation"},{"location":"guided_exercises/04_dmn/getting-started/#examine-existing-dmn-diagram","text":"The following example describes an insurance price calculator based on an applicant\u2019s age and accident history. This is a simple decision process based on the following decision table: DMN Decision Table The decision table was designed with using a DMN editor, this could be done in IBM Business Automation Manager Canvas, VSCode, Business Central or realistically any DMN compatible design tool. With how IBM Business Automation Open Edition 9.2.x utilizes the DMN specification, we're easily able to import this diagram seamlessly in the tool that makes the most sense for our end user. Notice that: The DMN decision table includes a hit policy , inputs , and outputs . Unlike the classic Drools Rules Language (DRL) based decision tables that can be created in Business Central, input and output names in DMN decision tables accept spaces. This allows for more clarity in your decisions aligning more to a business acumen. The conditions for the Age input is defined using the Friendly Enough Expression Language (FEEL). The decision can also be represented by the following decision requirements diagram: In this decision requirements diagram, you may note that the applicant\u2019s age and accident history are found in squared ovals, these indicate that these are the required inputs for the decision table \"Insurance Total Price\". You can find the DMN component in the DMN GitHub repository .","title":"Examine Existing DMN Diagram"},{"location":"guided_exercises/04_dmn/getting-started/#import-the-dmn-file-into-ibm-business-automation-manager-canvas","text":"In this section, you can import the GitHub repository to IBM Business Automation Manager Canvas directly from the file or clone the repository, if you do the direct file. Open IBM Business Automation Manager Canvas by navigating to your instance of it (within the lab environments these are at http://localhost:9090/#. From here, you will copy the URL to the gist of the Insurance Pricing DMN located at this repository location . https://raw.githubusercontent.com/timwuthenow/dmn-workshop-labs/master/policy-price/insurance-pricing.dmn With this link navigate to the IBM Business Automation Manager Canvas and under Import From URL paste the link from the previous step and click Import . When the project is imported, you will see the DMN Editor with the insurance-pricing DMN model displayed. If you instead of pointing to a particular DMN, pointed to an entire project, any DMN/BPMN models associated with it would be able to viewed/edited within IBM Business Automation Manager Canvas. You can then click the Run button to get a local copy of this DMN running within the browser session itself. The best part about the DMN specification is that it is both a modeling specification, but also has an execution specification which allows for it to be as portable as it is. This will have a section of the browser turn into a form and you can run the model right there. Modifying the checkbox based on the boolean of had previous incidents and set an Age based on the data type being a number. Info Within this form you may see that if you just put an age in that the result returns (null) . This is because the way that a typical form works when generated is that a non-checked box assumes null , which the rules by default do not check for null. To quickly get around this from the form you can check and uncheck the checkbox and move forward with your testing. This initializes the value, this is because the form when it sends it for initial execution has the boolean value as a null. How would you address this in the rules? HINT : since we're checking if the number of incidents has a checkbox or not, you can write it as ? = false or ? = null . Why the ? ? This is a way that DMN through the Friendly Enough Expression Language uses the column header object that you're testing to simplify checking a variable against a condition! This service can be deployed as a sample Quarkus service to OpenShift right away as a sample service that you can invoke immediately. In the next section we will show how this is done from a development point of view.","title":"Import the DMN File into IBM Business Automation Manager Canvas"},{"location":"guided_exercises/04_dmn/getting-started/#deploying-this-sample-service-to-openshift","text":"If you are using the provided lab environment for TechXchange, the environment itself provides access to a shared cluster for all attendees to login to OpenShift there is a bookmark in Chrome called OpenShift Token that will take you to a login page to create an OpenShift login token. Every user for the TechXchange environment has a created namespace to the student id that was given to your workstation at the start of the lab (student01 through student20 with the namespace to that username with -namespace ). The password for all of these instances is Passw0rd . To create the OpenShift token to connect to the console at the bookmarked link OpenShift Token from Chrome. From here you will login to OpenShift using your username and password created for the lab which is of the format student01 through student20 with password Passw0rd . If you get security issues with the link, accept them using the Advanced option in Chrome to proceed to the login screen. After logging in to the token screen, you will need to copy the token, server and use your assigned namespace ( student##-namespace ) to login. Please be sure to check the Insecurely disable TLS certificate validation as this is a local instance of OpenShift and does not have all the proper certificates in place. After you're successfully connected, you can use the deploy feature found on IBM Business Automation Manager Canvas by clicking the Deploy button and selecting the insurance-pricing decision to deploy. Confirm to deploy it to your namespace and let it load for approximately 30-45 seconds after. Then you can click the link to load the Quarkus service that was created from the DMN file. Just a note you need to click the checkbox for had previous incidents to initialize the value .","title":"Deploying this sample service to OpenShift"},{"location":"guided_exercises/04_dmn/getting-started/#conclusion","text":"Congratulations, you've finished the first part of the DMN exercise. Next, you will have an intermediate level exercise that will guide you through the implementation, deployment and testing of the Vacation Days use case.","title":"Conclusion"},{"location":"guided_exercises/04_dmn/intermediate-la-deprecated/","text":"To define and deploy a DMN decision model, we first need to create a new project in which we can store the model. To create a new project: Navigate to Business Central Login to the platform with the provided username and password. Click on Design to navigate to the Design perspective. 4. In the Design perspective, create a new project. If your space is empty, this can be done by clicking on the blue Add Project button in the center of the page. If you already have projects in your space, you can click on the blue Add Project icon at the top right of the page. Give the project the name vacation-days-decisions , and the description \"Vacation Days Decisions\". With the project created, we can now create our DMN model. Click on the blue Add Asset button. In the Add Asset page, select Decision in the dropdown filter selector. Click on the DMN tile to create a new DMN model. Give it the name vacation-days . This will create the asset and open the DMN editor.","title":"Intermediate la deprecated"},{"location":"guided_exercises/04_dmn/intermediate-lab-authoring/","text":"Vacation Days - Authoring Decisions Let's get working on the decision model! When writing decisions with IBM Business Automation Manager Canvas, with DMN, we are able to define, model and create our decision all in one editor. Input Nodes The problem statement describes a number of different inputs to our decision: Age of the employee Years of Service of the employee Therefore, we should create two input nodes, one for each input: Add an Input node to the diagram by clicking on the Input node icon and placing it in the DRD. Double-click on the node to set the name. We will name this node Age . With the Age node selected, open the property panel. Set the data type to number . In the same way, create an Input node for Years of Service . This node should also have its data type set to number . The model is constantly saved in browser storage, so you don't have to save it constantly. Constants The problem statement describes that every employee receives at least 22 days. So, if no other decisions apply, an employee receives 22 days. This is can be seen as a constant input value into our decision model. In DMN we can model such constant inputs with a Decision node with a Literal boxed expression that defines the constant value: Add a Decision node to the DRD Give the node the name Base Vacation Days . Click on the node to select it and open the property panel. Set the node\u2019s data type to number . Click on the node and click on the Edit icon to open the expression editor. In the expression editor, click on the box that says Select expression and select Literal expression . Simply set the Literal Expression to 22 , the number of base vacation days defined in the problem statement. When completed press the link to go Back to vacation-days to return to the main DMN diagram. Decisions The problem statement defines 3 decisions which can cause extra days to be given to employees based on various criteria. Let\u2019s simply call these decision: Extra days case 1 Extra days case 2 Extra days case 3 Although these decisions could be implemented in a single decision node, we\u2019ve decided, in order to improve maintainability of the solution and visibility of the solution, to define these decisions in 3 separate decision nodes. In your DRD, create 3 decision nodes with these given names. Set their data types to number . You can start setting one of the Decision nodes if you like as Extra days case with the number type and use the keyboard short cut with the node highlighted to copy using Control + C or Command + C then using Control + V or Command + V to copy and paste the same node two times. This will take the underlying data from the node and use it to craft your extra nodes. In my case this would produce Extra days case , Extra days case-1 and Extra days case-2 complete with the number data type already selected. You can do it however you feel most comfortable. We need to attach both input nodes, Age and Years of Service to all 3 decision nodes. We can do this by clicking on an Input node, clicking on its arrow icon, and attaching the arrow to the Decision node. This tells the Decision node that is has a requirement of both Age and Years of Service for each respective node it connects. With DMN this explicit linkage is how decisions requirements diagrams (DRD) are crafted. Select the Extra days case 1 node and open its expression editor by clicking on the Edit button. Select the expression Decision Table to create a boxed expression implemented as a decision table. The first case defines 2 decisions which can be modelled with 2 rows in our decision table as such: employees younger than 18 or at least 60 years will receive 5 extra days, or \u2026 employees with at least 30 years of service will receive 7 extra days To add new lines to your table, right click the first column and select \"Insert below\" under Decision Rule or you can click the Plus Sign below the 1 under the U . Note that the hit-policy of the decision table is by default set to U , which means Unique . This implies that only one rule is expected to fire for a given input. In this case however, we would like to set it to Collect Max , as, for a given input, multiple decisions might match, but we would like to collect the output from the rule with the highest number of additional vacation days. To do this, click on the U in the upper-left corner of the decision table. Now, set the Hit Policy to Collect and when you click this, it will ask the Aggregator function to use, select MAX so you pull the largest value from the table. Finally, we need to set the default result of the decision. This is the result that will be returned when none of the rules match the given input. This is done as follows: Select the output/result column of the decision table ( Extra days case-1 ). Open the properties panel on the right-side of the editor. Expand the Default output section. Set the Default output value to 0 . Decision Runner Your model will be saved throughout this process, if you want to see how the decision is starting to execute. You can start to use the DMN Runner (which we could have started from the moment we added input values, but more fun with decisions in place!). To do this click Run just below the Black IBM banner. Now that the DMN Runner is open, you can see that the IBM Business Automation Manager Canvas created a form based on the inputs you have and is also displaying the results of the various decision nodes that we have. You will see that Base Vacation Days is currently displaying 22 and the rest of the decisions are (null) because we have no input data. If you start adding data, e.g. an Age of 60 with 5 years of service, you will see the result also update to have Extra days case-1 having 5 as the final result. Since DMN is also an execution specification with how it is written, all of the updates we make to our decisions, as we make them will be taken once the cell that was being actively changed is no longer selected. You can try this now or just leave it open while you're creating your other decisions so that you can see how your model is actively behaving based on your inputs. Continuining with Decisions for Use Case 2 and Use Case 3 The other two decisions can be implemented in the same way. Now, implement the following two decision tables: Case 2: Case 3: NOTE: If you want to resize the columns, you can double click the lines between them to auto-resize. More auto-resizing to column headers features are coming soon! If you check the DMN Runner if you are using the same Age: 60 and Years of Service: 30 will now render a decision for each one in the table and you can adjust to execute other decisions. Total Vacation Days The total vacation days needs to be determined from the base vacation days and the decisions taken by our 3 decision nodes. As such, we need to create a new Decision node, which takes the output of our 4 Decision nodes (3 decision tables and a literal expression) as input and determines the final output. To do this, we need to: Create a new Decision node in the model. Give the node the name Total Vacation Days and set its data type to number . Connect the 4 existing Decision nodes to the node. This defines that the output of these nodes will be the input of the next node. Without putting a line connecting Age or Years of Service connected to Total Vacation Days, this data will not be available to this decision, not that they are needed here, but just an overall note of how DMN works! Click on the Total Vacation Days node and click on Edit to open the expression editor. Configure the expression as a literal expression. We need to configure the following logic: Everyone gets the Base Vacation Days. If both case 1 and case 3 add extra days, only the extra days of one of this decision is added. So, in that case we take the maximum. If case 2 adds extra days, add them to the total. The above logic can be implemented with the following FEEL expression - note that these are using the values that are produced by the Decision Nodes names and match the case exactly : Base Vacation Days + max ( Extra days case -1, Extra days-case3 ) + Extra days case -2 Using your DMN Runner you can test your decision exactly as it sits today and return a Total Days based on the sample employee from before or any other changes. Your model is now ready for consumption. There are a few ways to explore how to use it which we will cover in the next section!","title":"Vacation Days - Authoring Decisions"},{"location":"guided_exercises/04_dmn/intermediate-lab-authoring/#vacation-days-authoring-decisions","text":"Let's get working on the decision model! When writing decisions with IBM Business Automation Manager Canvas, with DMN, we are able to define, model and create our decision all in one editor.","title":"Vacation Days - Authoring Decisions"},{"location":"guided_exercises/04_dmn/intermediate-lab-authoring/#input-nodes","text":"The problem statement describes a number of different inputs to our decision: Age of the employee Years of Service of the employee Therefore, we should create two input nodes, one for each input: Add an Input node to the diagram by clicking on the Input node icon and placing it in the DRD. Double-click on the node to set the name. We will name this node Age . With the Age node selected, open the property panel. Set the data type to number . In the same way, create an Input node for Years of Service . This node should also have its data type set to number . The model is constantly saved in browser storage, so you don't have to save it constantly.","title":"Input Nodes"},{"location":"guided_exercises/04_dmn/intermediate-lab-authoring/#constants","text":"The problem statement describes that every employee receives at least 22 days. So, if no other decisions apply, an employee receives 22 days. This is can be seen as a constant input value into our decision model. In DMN we can model such constant inputs with a Decision node with a Literal boxed expression that defines the constant value: Add a Decision node to the DRD Give the node the name Base Vacation Days . Click on the node to select it and open the property panel. Set the node\u2019s data type to number . Click on the node and click on the Edit icon to open the expression editor. In the expression editor, click on the box that says Select expression and select Literal expression . Simply set the Literal Expression to 22 , the number of base vacation days defined in the problem statement. When completed press the link to go Back to vacation-days to return to the main DMN diagram.","title":"Constants"},{"location":"guided_exercises/04_dmn/intermediate-lab-authoring/#decisions","text":"The problem statement defines 3 decisions which can cause extra days to be given to employees based on various criteria. Let\u2019s simply call these decision: Extra days case 1 Extra days case 2 Extra days case 3 Although these decisions could be implemented in a single decision node, we\u2019ve decided, in order to improve maintainability of the solution and visibility of the solution, to define these decisions in 3 separate decision nodes. In your DRD, create 3 decision nodes with these given names. Set their data types to number . You can start setting one of the Decision nodes if you like as Extra days case with the number type and use the keyboard short cut with the node highlighted to copy using Control + C or Command + C then using Control + V or Command + V to copy and paste the same node two times. This will take the underlying data from the node and use it to craft your extra nodes. In my case this would produce Extra days case , Extra days case-1 and Extra days case-2 complete with the number data type already selected. You can do it however you feel most comfortable. We need to attach both input nodes, Age and Years of Service to all 3 decision nodes. We can do this by clicking on an Input node, clicking on its arrow icon, and attaching the arrow to the Decision node. This tells the Decision node that is has a requirement of both Age and Years of Service for each respective node it connects. With DMN this explicit linkage is how decisions requirements diagrams (DRD) are crafted. Select the Extra days case 1 node and open its expression editor by clicking on the Edit button. Select the expression Decision Table to create a boxed expression implemented as a decision table. The first case defines 2 decisions which can be modelled with 2 rows in our decision table as such: employees younger than 18 or at least 60 years will receive 5 extra days, or \u2026 employees with at least 30 years of service will receive 7 extra days To add new lines to your table, right click the first column and select \"Insert below\" under Decision Rule or you can click the Plus Sign below the 1 under the U . Note that the hit-policy of the decision table is by default set to U , which means Unique . This implies that only one rule is expected to fire for a given input. In this case however, we would like to set it to Collect Max , as, for a given input, multiple decisions might match, but we would like to collect the output from the rule with the highest number of additional vacation days. To do this, click on the U in the upper-left corner of the decision table. Now, set the Hit Policy to Collect and when you click this, it will ask the Aggregator function to use, select MAX so you pull the largest value from the table. Finally, we need to set the default result of the decision. This is the result that will be returned when none of the rules match the given input. This is done as follows: Select the output/result column of the decision table ( Extra days case-1 ). Open the properties panel on the right-side of the editor. Expand the Default output section. Set the Default output value to 0 .","title":"Decisions"},{"location":"guided_exercises/04_dmn/intermediate-lab-authoring/#decision-runner","text":"Your model will be saved throughout this process, if you want to see how the decision is starting to execute. You can start to use the DMN Runner (which we could have started from the moment we added input values, but more fun with decisions in place!). To do this click Run just below the Black IBM banner. Now that the DMN Runner is open, you can see that the IBM Business Automation Manager Canvas created a form based on the inputs you have and is also displaying the results of the various decision nodes that we have. You will see that Base Vacation Days is currently displaying 22 and the rest of the decisions are (null) because we have no input data. If you start adding data, e.g. an Age of 60 with 5 years of service, you will see the result also update to have Extra days case-1 having 5 as the final result. Since DMN is also an execution specification with how it is written, all of the updates we make to our decisions, as we make them will be taken once the cell that was being actively changed is no longer selected. You can try this now or just leave it open while you're creating your other decisions so that you can see how your model is actively behaving based on your inputs.","title":"Decision Runner"},{"location":"guided_exercises/04_dmn/intermediate-lab-authoring/#continuining-with-decisions-for-use-case-2-and-use-case-3","text":"The other two decisions can be implemented in the same way. Now, implement the following two decision tables: Case 2: Case 3: NOTE: If you want to resize the columns, you can double click the lines between them to auto-resize. More auto-resizing to column headers features are coming soon! If you check the DMN Runner if you are using the same Age: 60 and Years of Service: 30 will now render a decision for each one in the table and you can adjust to execute other decisions.","title":"Continuining with Decisions for Use Case 2 and Use Case 3"},{"location":"guided_exercises/04_dmn/intermediate-lab-authoring/#total-vacation-days","text":"The total vacation days needs to be determined from the base vacation days and the decisions taken by our 3 decision nodes. As such, we need to create a new Decision node, which takes the output of our 4 Decision nodes (3 decision tables and a literal expression) as input and determines the final output. To do this, we need to: Create a new Decision node in the model. Give the node the name Total Vacation Days and set its data type to number . Connect the 4 existing Decision nodes to the node. This defines that the output of these nodes will be the input of the next node. Without putting a line connecting Age or Years of Service connected to Total Vacation Days, this data will not be available to this decision, not that they are needed here, but just an overall note of how DMN works! Click on the Total Vacation Days node and click on Edit to open the expression editor. Configure the expression as a literal expression. We need to configure the following logic: Everyone gets the Base Vacation Days. If both case 1 and case 3 add extra days, only the extra days of one of this decision is added. So, in that case we take the maximum. If case 2 adds extra days, add them to the total. The above logic can be implemented with the following FEEL expression - note that these are using the values that are produced by the Decision Nodes names and match the case exactly : Base Vacation Days + max ( Extra days case -1, Extra days-case3 ) + Extra days case -2 Using your DMN Runner you can test your decision exactly as it sits today and return a Total Days based on the sample employee from before or any other changes. Your model is now ready for consumption. There are a few ways to explore how to use it which we will cover in the next section!","title":"Total Vacation Days"},{"location":"guided_exercises/04_dmn/intermediate-lab-deployment/","text":"Vacation Days - Consuming Decisions There are several ways to consume decisions, but the first way we will explore is through the utilization of the IBM Business Automation Manager Canvas provided methods, but there will also be methods showing how to do it with IBM Business Automation Open Edition 8.0 provided as informational and not required for these exercises. Deploying and testing the Decision Service With our decision model completed, we can now package our DMN model in a Kogito service. This can be done using the Accelerators included with IBM Business Automation Manager Canvas. These accelerators will quickly take the DMN file and start building the basis of the project as a IBM Business Automation Open Edition 9.2.x compatible decision service. Applying the Accelerator To take this decision service from a standalone DMN model to a full Kogito architected decision, you will use the accelerator by clickig the button at the top of the screen. To do so follow the instructions and then we will synchronize the project to your GitHub account and also be capable being deployed as an OpenShift service. At the top of the DMN model page for vacation-days , click the button Apply Accelerator and select Quarkus... . If you want to learn more about the accelerator, you can click the GitHub link and change the branch to the 9.0.0-quarkus-full to learn more about it. Afterwards just click Apply to restructure your project into a Kogito service. The wizard will come with a pop-up asking for a commit message for the change since this will create a Git project. You can use the default message or put whatever you would like, to do so press Commit . To create the project in Git, click Share , use your GitHub tokened ID. Once you select your ID, a new option Create GitHub repository... is now available, select this. The repository you create, can be anything you want, for the purposes of this lab, it will be called techxchange-vacation-days . When you create the repository, a message will open at the top with a link to the Git repository that was created. In the case of the example it is at this example repository . Deploying to OpenShift Now that this repository is created, you can work on it locally or deploy it to OpenShift. For this lab, we will choose OpenShift since we have a cluster ready to use for TechXchange or you can use the Red Hat OpenShift Sandbox to provision a small environment to try it out on your own. Login to get an OpenShift Token using the bookmark on Chrome. This will be used to connect IBM Business Automation Manager Canvas to OpenShift and deploy a sample development environment of the decision service. In the login screen use your username and password - this will be some combination of student01 to student20 with password Passw0rd . The username is all lowercase. This will Click the Display Token link and switch back to the tab with IBM Business Automation Manager Canvas open on it. You will use the token contents from the OpenShift token page shortly by clicking on Dev deployments and then selecting the pull down Select authentication and then Connect to an account . On the next wizard, select the OpenShift Tile to connect to an OpenShift instance. From here you will use the information from the OpenShift token page. Your namespace will be your student##-namespace , so if your username is student07, then you would connect to student07-namespace . The token and the console locations are both a part of the token as well. You will see a checkbox to Insecurely disable TLS validation, check it as this is a self contained environment with certificates not present. Press Connect when the form is completed. As long as it was successful to connect, you will be greeted with a Connect to OpenShift successful modal, click Continue . Now you can press the Deploy button and have the capability to select the namespace that you connected to and deploy a sample of your service to it. From here a modal will pop up explaining that the deployment can take a few minutes and it is only intended for development, etc. Click Confirm on this modal and your service will be deployed. This process will take several minutes to complete. The pulldown with deploy will refresh every 30 seconds, when the service is available, it will reflect as such. The way that the deployments from these services work are is that they are completely immutable and unaware of one another, so you could have all versions deployed as you make changes to validate how you want to eventually promote your DMN model to later environments. So if you were to deploy it again, there would be multiple different versions of it deployed. Canvas can manage these deployment samples, which can later be removed when you're done with them if you choose. If you click the green check marked box, it will take you to the service. When you access the service with a form similar to the one you had in IBM Business Automation Manager Canvas and from that page you can explore it more using the kebab icon in the top right to access the Swagger-UI page to get access to the auto-generated DMN service's API page. The first post can be used to execute the decision itself. This completes this lab as you can see how the deployment to OpenShit works with IBM Business Automation Manager Canvas! Deploying the Decision Service in KIE Server (Not for TechXchange) This section is not needed for TechXchange, but is just to provide some extra ways to showcase now you can deploy your decision. With our decision model completed, we can now package our DMN model in a Deployment Unit (KJAR) and deploy it on the Execution Server. To do this: In the bread-crumb navigation in the upper-left corner, click on vacation-days-decisions to go back to the project\u2019s Library View. Click on the Deploy button in the upper-right corner of the screen. This will package our DMN mode in a Deployment Unit (KJAR) and deploy it onto the Execution Server (KIE-Server). Go to the Execution Servers perspective by clicking on \"Menu \u2192 Deploy \u2192 Execution Servers\". You will see the Deployment Unit deployed on the Execution Server. Testing DMN Solution In this section, you will test the DMN solution with Execution Server\u2019s Swagger interface and via Java KIE Client API. Testing the solution via REST API In this section, you will test the DMN solution with KIE Server\u2019s Swagger interface. The Swagger interface provides the description and documentation of the Execution Server\u2019s RESTful API. At the same time, it allows the APIs to be called from the UI. This enables developers and users to quickly test, in this case, a deployed DMN Service. Navigate to KIE Server Locate the DMN Models section. The DMN API provides the DMN model as a RESTful resources, which accepts 2 operations: GET : Retrieves the DMN model. POST : Evaluates the decisions for a given input. Expand the GET operation by clicking on it. Click on the Try it out button. Set the containerId field to vacation-days-decisions and set the Response content type to application/json and click on Execute If requested, provide the username and password of your Business Central and KIE-Server user. The response will be the model-description of your DMN model. Next, we will evaluate our model with some input data. We need to provide our model with the age of an employee and the number of years of service . Let\u2019s try a number of different values to test our deicions. Expand the POST operation and click on the Try it out button Set the containerId field to vacation-days-decisions . Set the Parameter content type and Response content type fields to application/json . Pass the following request to lookup the number of vacation days for an employee of 16 years old with 1 year of service (note that the namespace of your model is probably different as it is generated. You can lookup the namespace of your model in the response/result of the GET operation you executed ealier, which returned the model description). { \"dmn-context\" :{ \"Age\" : 16 , \"Years of Service\" : 1 } } Click on Execute . The result value of the Total Vacation Days should be 27. Test the service with a number of other values. See the following table for some sample values and expected output. Age Years of Service Total Vacation Days 16 1 27 25 5 22 44 20 24 44 30 30 50 20 24 50 30 30 60 20 30 Using the KIE Java Client (if you were using IBM Business Automation Open Edition 8.0) IBM IBM Business Automation Open Edition 8.0 provides a KIE Java Client API that allows the user to interact with the KIE-Server from a Java client using a higher level API. It abstracts the data marshalling and unmarshalling and the creation and execution of the RESTful commands from the developer, allowing him/her to focus on developing business logic. In this section we will create a simple Java client for our DMN model. IMPORTANT: If your KIE Server is exposed via https you need to configure the `javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword in the Java client code using the Remote Java API. If not, you may get a rest.NoEndpointFoundException`. For more information check this solution Red Hat's knowledge base. Create a new Maven Java JAR project in your favourite IDE (e.g. IntelliJ, Eclipse, Visual Studio Code). Add the following dependency to your project: <dependency> <groupId> org.kie.server </groupId> <artifactId> kie-server-client </artifactId> <version> 7.67.2.Final-redhat-00017 </version> <scope> compile </scope> </dependency> Create a Java package in your src/main/java folder with the name org.kie.dmn.lab . In the package you\u2019ve just created, create a Java class called Main.java . Add a public static void main(String[] args) method to your main class. Before we implement our method, we first define a number of constants that we will need when implementing our method (note that the values of your constants can be different depending on your environment, model namespace, etc.): private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"vacation-days-decisions\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; !!! \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. ~~~java private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\"; private static final String CONTAINER_ID = \"vacation-days-decisions\"; private static final String USERNAME = \"pamadmin\"; private static final String PASSWORD = \"pamadm1n\"; ~~~ KIE-Server client API classes can mostly be retrieved from the KieServicesFactory class. We first need to create a KieServicesConfiguration instance that will hold our credentials and defines how we want our client to communicate with the server: KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , new EnteredCredentialsProvider ( USERNAME , PASSWORD )); Next, we create the KieServicesClient : KieServicesClient kieServicesClient = KieServicesFactory . newKieServicesClient ( kieServicesConfig ); From this client we retrieve our DMNServicesClient: DMNServicesClient dmnServicesClient = kieServicesClient . getServicesClient ( DMNServicesClient . class ); To pass the input values to our model to the Execution Server, we need to create a DMNContext : DMNContext dmnContext = dmnServicesClient . newContext (); dmnContext . set ( \"Age\" , 16 ); dmnContext . set ( \"Years of Service\" , 1 ); We now have defined all the required instances needed to send a DMN evaluation request to the server: ServiceResponse < DMNResult > dmnResultResponse = dmnServicesClient . evaluateAll ( CONTAINER_ID , dmnContext ); Finally we can retrieve the DMN evaluation result and print it in the console: DMNDecisionResult decisionResult = dmnResultResponse . getResult (). getDecisionResultByName ( \"Total Vacation Days\" ); System . out . println ( \"Total vacation days: \" + decisionResult . getResult ()); Compile your project and run it. Observe the output in the console, which should say: Total vacation days: 27 The complete project can be found here package org.kie.dmn.lab ; import org.kie.api.builder.KieScannerFactoryService ; import org.kie.api.internal.weaver.KieWeaverService ; import org.kie.dmn.api.core.DMNContext ; import org.kie.dmn.api.core.DMNDecisionResult ; import org.kie.dmn.api.core.DMNResult ; import org.kie.server.api.model.ServiceResponse ; import org.kie.server.client.CredentialsProvider ; import org.kie.server.client.DMNServicesClient ; import org.kie.server.client.KieServicesClient ; import org.kie.server.client.KieServicesConfiguration ; import org.kie.server.client.KieServicesFactory ; import org.kie.server.client.credentials.EnteredCredentialsProvider ; /** * Vacation Days DMN Client */ public class Main { private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"vacation-days-decisions\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; // Comment out the above 2 lines if using the Skytap image and uncomment the two below to use those logins // private static final String USERNAME = \"pamadmin\"; // private static final String PASSWORD = \"pamadm1n\"; public static void main ( String [] args ) { CredentialsProvider credentialsProvider = new EnteredCredentialsProvider ( USERNAME , PASSWORD ); KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , credentialsProvider ); KieServicesClient kieServicesClient = KieServicesFactory . newKieServicesClient ( kieServicesConfig ); DMNServicesClient dmnServicesClient = kieServicesClient . getServicesClient ( DMNServicesClient . class ); DMNContext dmnContext = dmnServicesClient . newContext (); dmnContext . set ( \"Age\" , 16 ); dmnContext . set ( \"Years of Service\" , 1 ); ServiceResponse < DMNResult > dmnResultResponse = dmnServicesClient . evaluateAll ( CONTAINER_ID , dmnContext ); DMNDecisionResult decisionResult = dmnResultResponse . getResult (). getDecisionResultByName ( \"Total Vacation Days\" ); System . out . println ( \"Total vacation days: \" + decisionResult . getResult ()); } }","title":"Vacation Days - Consuming Decisions"},{"location":"guided_exercises/04_dmn/intermediate-lab-deployment/#vacation-days-consuming-decisions","text":"There are several ways to consume decisions, but the first way we will explore is through the utilization of the IBM Business Automation Manager Canvas provided methods, but there will also be methods showing how to do it with IBM Business Automation Open Edition 8.0 provided as informational and not required for these exercises.","title":"Vacation Days - Consuming Decisions"},{"location":"guided_exercises/04_dmn/intermediate-lab-deployment/#deploying-and-testing-the-decision-service","text":"With our decision model completed, we can now package our DMN model in a Kogito service. This can be done using the Accelerators included with IBM Business Automation Manager Canvas. These accelerators will quickly take the DMN file and start building the basis of the project as a IBM Business Automation Open Edition 9.2.x compatible decision service.","title":"Deploying and testing the Decision Service"},{"location":"guided_exercises/04_dmn/intermediate-lab-deployment/#applying-the-accelerator","text":"To take this decision service from a standalone DMN model to a full Kogito architected decision, you will use the accelerator by clickig the button at the top of the screen. To do so follow the instructions and then we will synchronize the project to your GitHub account and also be capable being deployed as an OpenShift service. At the top of the DMN model page for vacation-days , click the button Apply Accelerator and select Quarkus... . If you want to learn more about the accelerator, you can click the GitHub link and change the branch to the 9.0.0-quarkus-full to learn more about it. Afterwards just click Apply to restructure your project into a Kogito service. The wizard will come with a pop-up asking for a commit message for the change since this will create a Git project. You can use the default message or put whatever you would like, to do so press Commit . To create the project in Git, click Share , use your GitHub tokened ID. Once you select your ID, a new option Create GitHub repository... is now available, select this. The repository you create, can be anything you want, for the purposes of this lab, it will be called techxchange-vacation-days . When you create the repository, a message will open at the top with a link to the Git repository that was created. In the case of the example it is at this example repository .","title":"Applying the Accelerator"},{"location":"guided_exercises/04_dmn/intermediate-lab-deployment/#deploying-to-openshift","text":"Now that this repository is created, you can work on it locally or deploy it to OpenShift. For this lab, we will choose OpenShift since we have a cluster ready to use for TechXchange or you can use the Red Hat OpenShift Sandbox to provision a small environment to try it out on your own. Login to get an OpenShift Token using the bookmark on Chrome. This will be used to connect IBM Business Automation Manager Canvas to OpenShift and deploy a sample development environment of the decision service. In the login screen use your username and password - this will be some combination of student01 to student20 with password Passw0rd . The username is all lowercase. This will Click the Display Token link and switch back to the tab with IBM Business Automation Manager Canvas open on it. You will use the token contents from the OpenShift token page shortly by clicking on Dev deployments and then selecting the pull down Select authentication and then Connect to an account . On the next wizard, select the OpenShift Tile to connect to an OpenShift instance. From here you will use the information from the OpenShift token page. Your namespace will be your student##-namespace , so if your username is student07, then you would connect to student07-namespace . The token and the console locations are both a part of the token as well. You will see a checkbox to Insecurely disable TLS validation, check it as this is a self contained environment with certificates not present. Press Connect when the form is completed. As long as it was successful to connect, you will be greeted with a Connect to OpenShift successful modal, click Continue . Now you can press the Deploy button and have the capability to select the namespace that you connected to and deploy a sample of your service to it. From here a modal will pop up explaining that the deployment can take a few minutes and it is only intended for development, etc. Click Confirm on this modal and your service will be deployed. This process will take several minutes to complete. The pulldown with deploy will refresh every 30 seconds, when the service is available, it will reflect as such. The way that the deployments from these services work are is that they are completely immutable and unaware of one another, so you could have all versions deployed as you make changes to validate how you want to eventually promote your DMN model to later environments. So if you were to deploy it again, there would be multiple different versions of it deployed. Canvas can manage these deployment samples, which can later be removed when you're done with them if you choose. If you click the green check marked box, it will take you to the service. When you access the service with a form similar to the one you had in IBM Business Automation Manager Canvas and from that page you can explore it more using the kebab icon in the top right to access the Swagger-UI page to get access to the auto-generated DMN service's API page. The first post can be used to execute the decision itself. This completes this lab as you can see how the deployment to OpenShit works with IBM Business Automation Manager Canvas!","title":"Deploying to OpenShift"},{"location":"guided_exercises/04_dmn/intermediate-lab-deployment/#deploying-the-decision-service-in-kie-server-not-for-techxchange","text":"This section is not needed for TechXchange, but is just to provide some extra ways to showcase now you can deploy your decision. With our decision model completed, we can now package our DMN model in a Deployment Unit (KJAR) and deploy it on the Execution Server. To do this: In the bread-crumb navigation in the upper-left corner, click on vacation-days-decisions to go back to the project\u2019s Library View. Click on the Deploy button in the upper-right corner of the screen. This will package our DMN mode in a Deployment Unit (KJAR) and deploy it onto the Execution Server (KIE-Server). Go to the Execution Servers perspective by clicking on \"Menu \u2192 Deploy \u2192 Execution Servers\". You will see the Deployment Unit deployed on the Execution Server.","title":"Deploying the Decision Service in KIE Server (Not for TechXchange)"},{"location":"guided_exercises/04_dmn/intermediate-lab-deployment/#testing-dmn-solution","text":"In this section, you will test the DMN solution with Execution Server\u2019s Swagger interface and via Java KIE Client API.","title":"Testing DMN Solution"},{"location":"guided_exercises/04_dmn/intermediate-lab-deployment/#testing-the-solution-via-rest-api","text":"In this section, you will test the DMN solution with KIE Server\u2019s Swagger interface. The Swagger interface provides the description and documentation of the Execution Server\u2019s RESTful API. At the same time, it allows the APIs to be called from the UI. This enables developers and users to quickly test, in this case, a deployed DMN Service. Navigate to KIE Server Locate the DMN Models section. The DMN API provides the DMN model as a RESTful resources, which accepts 2 operations: GET : Retrieves the DMN model. POST : Evaluates the decisions for a given input. Expand the GET operation by clicking on it. Click on the Try it out button. Set the containerId field to vacation-days-decisions and set the Response content type to application/json and click on Execute If requested, provide the username and password of your Business Central and KIE-Server user. The response will be the model-description of your DMN model. Next, we will evaluate our model with some input data. We need to provide our model with the age of an employee and the number of years of service . Let\u2019s try a number of different values to test our deicions. Expand the POST operation and click on the Try it out button Set the containerId field to vacation-days-decisions . Set the Parameter content type and Response content type fields to application/json . Pass the following request to lookup the number of vacation days for an employee of 16 years old with 1 year of service (note that the namespace of your model is probably different as it is generated. You can lookup the namespace of your model in the response/result of the GET operation you executed ealier, which returned the model description). { \"dmn-context\" :{ \"Age\" : 16 , \"Years of Service\" : 1 } } Click on Execute . The result value of the Total Vacation Days should be 27. Test the service with a number of other values. See the following table for some sample values and expected output. Age Years of Service Total Vacation Days 16 1 27 25 5 22 44 20 24 44 30 30 50 20 24 50 30 30 60 20 30","title":"Testing the solution via REST API"},{"location":"guided_exercises/04_dmn/intermediate-lab-deployment/#using-the-kie-java-client-if-you-were-using-ibm-business-automation-open-edition-80","text":"IBM IBM Business Automation Open Edition 8.0 provides a KIE Java Client API that allows the user to interact with the KIE-Server from a Java client using a higher level API. It abstracts the data marshalling and unmarshalling and the creation and execution of the RESTful commands from the developer, allowing him/her to focus on developing business logic. In this section we will create a simple Java client for our DMN model. IMPORTANT: If your KIE Server is exposed via https you need to configure the `javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword in the Java client code using the Remote Java API. If not, you may get a rest.NoEndpointFoundException`. For more information check this solution Red Hat's knowledge base. Create a new Maven Java JAR project in your favourite IDE (e.g. IntelliJ, Eclipse, Visual Studio Code). Add the following dependency to your project: <dependency> <groupId> org.kie.server </groupId> <artifactId> kie-server-client </artifactId> <version> 7.67.2.Final-redhat-00017 </version> <scope> compile </scope> </dependency> Create a Java package in your src/main/java folder with the name org.kie.dmn.lab . In the package you\u2019ve just created, create a Java class called Main.java . Add a public static void main(String[] args) method to your main class. Before we implement our method, we first define a number of constants that we will need when implementing our method (note that the values of your constants can be different depending on your environment, model namespace, etc.): private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"vacation-days-decisions\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; !!! \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. ~~~java private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\"; private static final String CONTAINER_ID = \"vacation-days-decisions\"; private static final String USERNAME = \"pamadmin\"; private static final String PASSWORD = \"pamadm1n\"; ~~~ KIE-Server client API classes can mostly be retrieved from the KieServicesFactory class. We first need to create a KieServicesConfiguration instance that will hold our credentials and defines how we want our client to communicate with the server: KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , new EnteredCredentialsProvider ( USERNAME , PASSWORD )); Next, we create the KieServicesClient : KieServicesClient kieServicesClient = KieServicesFactory . newKieServicesClient ( kieServicesConfig ); From this client we retrieve our DMNServicesClient: DMNServicesClient dmnServicesClient = kieServicesClient . getServicesClient ( DMNServicesClient . class ); To pass the input values to our model to the Execution Server, we need to create a DMNContext : DMNContext dmnContext = dmnServicesClient . newContext (); dmnContext . set ( \"Age\" , 16 ); dmnContext . set ( \"Years of Service\" , 1 ); We now have defined all the required instances needed to send a DMN evaluation request to the server: ServiceResponse < DMNResult > dmnResultResponse = dmnServicesClient . evaluateAll ( CONTAINER_ID , dmnContext ); Finally we can retrieve the DMN evaluation result and print it in the console: DMNDecisionResult decisionResult = dmnResultResponse . getResult (). getDecisionResultByName ( \"Total Vacation Days\" ); System . out . println ( \"Total vacation days: \" + decisionResult . getResult ()); Compile your project and run it. Observe the output in the console, which should say: Total vacation days: 27 The complete project can be found here package org.kie.dmn.lab ; import org.kie.api.builder.KieScannerFactoryService ; import org.kie.api.internal.weaver.KieWeaverService ; import org.kie.dmn.api.core.DMNContext ; import org.kie.dmn.api.core.DMNDecisionResult ; import org.kie.dmn.api.core.DMNResult ; import org.kie.server.api.model.ServiceResponse ; import org.kie.server.client.CredentialsProvider ; import org.kie.server.client.DMNServicesClient ; import org.kie.server.client.KieServicesClient ; import org.kie.server.client.KieServicesConfiguration ; import org.kie.server.client.KieServicesFactory ; import org.kie.server.client.credentials.EnteredCredentialsProvider ; /** * Vacation Days DMN Client */ public class Main { private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"vacation-days-decisions\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; // Comment out the above 2 lines if using the Skytap image and uncomment the two below to use those logins // private static final String USERNAME = \"pamadmin\"; // private static final String PASSWORD = \"pamadm1n\"; public static void main ( String [] args ) { CredentialsProvider credentialsProvider = new EnteredCredentialsProvider ( USERNAME , PASSWORD ); KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , credentialsProvider ); KieServicesClient kieServicesClient = KieServicesFactory . newKieServicesClient ( kieServicesConfig ); DMNServicesClient dmnServicesClient = kieServicesClient . getServicesClient ( DMNServicesClient . class ); DMNContext dmnContext = dmnServicesClient . newContext (); dmnContext . set ( \"Age\" , 16 ); dmnContext . set ( \"Years of Service\" , 1 ); ServiceResponse < DMNResult > dmnResultResponse = dmnServicesClient . evaluateAll ( CONTAINER_ID , dmnContext ); DMNDecisionResult decisionResult = dmnResultResponse . getResult (). getDecisionResultByName ( \"Total Vacation Days\" ); System . out . println ( \"Total vacation days: \" + decisionResult . getResult ()); } }","title":"Using the KIE Java Client (if you were using IBM Business Automation Open Edition 8.0)"},{"location":"guided_exercises/04_dmn/intermediate-lab-intro/","text":"Vacation Days - Use case and project creation In this lab you'll try out the combination of DMN decision tables with literal expressions. You will also explore a number of different FEEL constructs and expressions like, for example, ranges. Finally, you'll learn how to deploy the solution as a sample to OpenShift. Goal Implement a DMN model using the IBM Business Automation Manager Canvas DMN editor Deploy the existing DMN project to OpenShift Consume the DMN project using the REST API Problem Statement In this lab we will create a decision service that determines the number of vacation days assigned to a given employee. The number of vacation days depends on age and years of service. Every employee receives at least 22 days. Additional days are provided according to the following criteria which will be broken into separate decision nodes: Only employees younger than 18 or at least 60 years, will receive 5 days of vacation or employees with at least 30 years of service will receive an 7 extra days; Employees with at least 30 years of service get 4 more days and also employees of age 60 or more, receive 3 extra days, on top of possible additional days already given; If an employee has at least 15 but less than 30 years of service, 3 extra days are given. Two days are also provided for employees of age 45 or more. These extra days can not be combined with the 5 extra days. Link IBM Business Automation Manager Canvas to your GitHub account and create a project in GitHub using Extended Services When using IBM Business Automation Manager Canvas you can start from the decision model creation and work top-down to deliver your decision model. This is a feature that will enable you to create the decision model and publish a sample project to your Git provider. From here the project can be deployed in your traditional means or you can also utilize a sample service that IBM Business Automation Manager Canvas can create if you desire for testing purposes. To do this, you need to make sure your connection to GitHub is active with the token and from there you can jump right in! Link GitHub to your IBM Business Automation Manager Canvas In this section we will link your GitHub account to the IBM Business Automation Manager Canvas so we can easily synchronize changes in DMN with GitHub and our tooling, in this case IBM Business Automation Manager Canvas. First click the User icon to connect your public GitHub account to the IBM Business Automation Manager Canvas. Click the Connect to an account button to add a new Git provider Select the card for GitHub Click Generate new token to create a new token that will be used by IBM Business Automation Manager Canvas You can use similar properties to the token created below in the screenshot, but the main 2 to have right now are repo and gist - the others can be beneficial if you reuse this token for other purposes too, but are not required nor needed. You can change the date to never expiring or be as short as you want. Once the token is generated though, that is the only time you will see the actual token value. Name : Name your token a unique name from any previously created Expiration : This can either be a set time period, up to 1 year or never expiring The checkboxes you need are repo and gist to get the full benefit of IBM Business Automation Manager Canvas Use the copy button that's created with the Token to use in IBM Business Automation Manager Canvas. Return to IBM Business Automation Manager Canvas and insert the Token into the wizard. When your token is pasted, the IBM Business Automation Manager Canvas will return a similar screen to below towards your GitHub account signifying you've connected, your GitHub user ID and some extra details. Create a DMN Model and Create a Project in IBM Business Automation Manager Canvas Now that our account is linked, let's go ahead and create a new DMN model and then later produce a project for it that will reside in our GitHub repositories. IBM Business Automation Manager Canvas provides an excellent way to both push and pull from your GitHub repositories. This methodology provides a true canvas over your process and decision models that sits on top of your artifact repositories as a means of seeing/creating/editing some of your most important assets for process and decision services. From IBM Business Automation Manager Canvas, you can create and edit various types of open-standards models in BPMN, DMN and PMML using the editors here. The editors are based on the standards, so if your model follows the standards, it will make a best effort representation of them to be able to edit. In theory a different DMN model written for another DMN platform would be editable if it follows the specification. In this lab you will see how we can use these editors to produce a model and a project that's ready to be deployable in OpenShift. Navigate to http://localhost:9090/# and we will start creating a DMN model for a Vacation Days Decision. This will provide some of the basics of a DMN decision and start getting you acquainted to IBM Business Automation Manager Canvas and DMN in general. This can either be done from the bookmark from Chrome on the desktop or by navigating to https://localhost:9090/ on the provided image. From the IBM Business Automation Manager Canvas landing page, you can create and edit various types of open-standards models in BPMN, DMN and PMML using the editors here. We will be creating a DMN model for this lab, so to do this, you can click New Decision to create a new DMN model. You will now have an empty canvas and can start working on your DMN model for designing the number of vacation days decision. Change the name at the top of the model from Untitled to vacation-days or whatever you want to call it. In the next section we will start to create the decision.","title":"Intermediate DMN Exercises - Vacation Days - Introduction"},{"location":"guided_exercises/04_dmn/intermediate-lab-intro/#vacation-days-use-case-and-project-creation","text":"In this lab you'll try out the combination of DMN decision tables with literal expressions. You will also explore a number of different FEEL constructs and expressions like, for example, ranges. Finally, you'll learn how to deploy the solution as a sample to OpenShift.","title":"Vacation Days - Use case and project creation"},{"location":"guided_exercises/04_dmn/intermediate-lab-intro/#goal","text":"Implement a DMN model using the IBM Business Automation Manager Canvas DMN editor Deploy the existing DMN project to OpenShift Consume the DMN project using the REST API","title":"Goal"},{"location":"guided_exercises/04_dmn/intermediate-lab-intro/#problem-statement","text":"In this lab we will create a decision service that determines the number of vacation days assigned to a given employee. The number of vacation days depends on age and years of service. Every employee receives at least 22 days. Additional days are provided according to the following criteria which will be broken into separate decision nodes: Only employees younger than 18 or at least 60 years, will receive 5 days of vacation or employees with at least 30 years of service will receive an 7 extra days; Employees with at least 30 years of service get 4 more days and also employees of age 60 or more, receive 3 extra days, on top of possible additional days already given; If an employee has at least 15 but less than 30 years of service, 3 extra days are given. Two days are also provided for employees of age 45 or more. These extra days can not be combined with the 5 extra days.","title":"Problem Statement"},{"location":"guided_exercises/04_dmn/intermediate-lab-intro/#link-ibm-business-automation-manager-canvas-to-your-github-account-and-create-a-project-in-github-using-extended-services","text":"When using IBM Business Automation Manager Canvas you can start from the decision model creation and work top-down to deliver your decision model. This is a feature that will enable you to create the decision model and publish a sample project to your Git provider. From here the project can be deployed in your traditional means or you can also utilize a sample service that IBM Business Automation Manager Canvas can create if you desire for testing purposes. To do this, you need to make sure your connection to GitHub is active with the token and from there you can jump right in!","title":"Link IBM Business Automation Manager Canvas to your GitHub account and create a project in GitHub using Extended Services"},{"location":"guided_exercises/04_dmn/intermediate-lab-intro/#link-github-to-your-ibm-business-automation-manager-canvas","text":"In this section we will link your GitHub account to the IBM Business Automation Manager Canvas so we can easily synchronize changes in DMN with GitHub and our tooling, in this case IBM Business Automation Manager Canvas. First click the User icon to connect your public GitHub account to the IBM Business Automation Manager Canvas. Click the Connect to an account button to add a new Git provider Select the card for GitHub Click Generate new token to create a new token that will be used by IBM Business Automation Manager Canvas You can use similar properties to the token created below in the screenshot, but the main 2 to have right now are repo and gist - the others can be beneficial if you reuse this token for other purposes too, but are not required nor needed. You can change the date to never expiring or be as short as you want. Once the token is generated though, that is the only time you will see the actual token value. Name : Name your token a unique name from any previously created Expiration : This can either be a set time period, up to 1 year or never expiring The checkboxes you need are repo and gist to get the full benefit of IBM Business Automation Manager Canvas Use the copy button that's created with the Token to use in IBM Business Automation Manager Canvas. Return to IBM Business Automation Manager Canvas and insert the Token into the wizard. When your token is pasted, the IBM Business Automation Manager Canvas will return a similar screen to below towards your GitHub account signifying you've connected, your GitHub user ID and some extra details.","title":"Link GitHub to your IBM Business Automation Manager Canvas"},{"location":"guided_exercises/04_dmn/intermediate-lab-intro/#create-a-dmn-model-and-create-a-project-in-ibm-business-automation-manager-canvas","text":"Now that our account is linked, let's go ahead and create a new DMN model and then later produce a project for it that will reside in our GitHub repositories. IBM Business Automation Manager Canvas provides an excellent way to both push and pull from your GitHub repositories. This methodology provides a true canvas over your process and decision models that sits on top of your artifact repositories as a means of seeing/creating/editing some of your most important assets for process and decision services. From IBM Business Automation Manager Canvas, you can create and edit various types of open-standards models in BPMN, DMN and PMML using the editors here. The editors are based on the standards, so if your model follows the standards, it will make a best effort representation of them to be able to edit. In theory a different DMN model written for another DMN platform would be editable if it follows the specification. In this lab you will see how we can use these editors to produce a model and a project that's ready to be deployable in OpenShift. Navigate to http://localhost:9090/# and we will start creating a DMN model for a Vacation Days Decision. This will provide some of the basics of a DMN decision and start getting you acquainted to IBM Business Automation Manager Canvas and DMN in general. This can either be done from the bookmark from Chrome on the desktop or by navigating to https://localhost:9090/ on the provided image. From the IBM Business Automation Manager Canvas landing page, you can create and edit various types of open-standards models in BPMN, DMN and PMML using the editors here. We will be creating a DMN model for this lab, so to do this, you can click New Decision to create a new DMN model. You will now have an empty canvas and can start working on your DMN model for designing the number of vacation days decision. Change the name at the top of the model from Untitled to vacation-days or whatever you want to call it. In the next section we will start to create the decision.","title":"Create a DMN Model and Create a Project in IBM Business Automation Manager Canvas"},{"location":"guided_exercises/04_dmn/introduction/","text":"Learn Decision Automation with DMN It's time to learn how Decision Automation can be made simple with Open Standards And Open Editions. Throughout these exercises, you'll experiment the development of decisions using Decision Model and Notation - DMN, and learn how to run, deploy and consume your decisions. Before getting started, let's get familiar with the exercises you'll be working on and get introduced to the concepts of the DMN Standard, an increasingly adopted strategy for developing rules in the decision automation market. Goals of the Guided Exercises These are the labs you have available in this workshop: Getting Started, Insurance Price calculation: Start by importing an existing module. Explore, deploy and test it using local development capabilities of Open Editions. Intermediate, Vacation Days: Author a model from scratch, use decision tables, work with different hit policies, different FEEL constructs and expressions. Finally, you will deploy it and test it, optionally running it on OpenShift. Advanced, Call Center: You will author a model from scratch, create data types, consume DMN decision services from within decision nodes, and more FEEL constructs and expressions. Finally, you will deploy and test it locally and optionally on OpenShift. These are independent guided exercises and you don't need to implement the previous use case to implement the next one. If you are more comfortable with DMN, you can go to the more advanced labs immediately. Before moving forward, get up to speed with what is DMN and explore the editor capabilities you'll be using. Introduction to DMN What is DMN DMN, along with BPMN, is managed by the Object Management Group (OMG). Let's have a look at what OMG describes about the DMN standard, in their website: OMG explanains what is DMN \"DMN is a modeling language and notation for the precise specification of business decisions and business rules. DMN is easily readable by the different types of people involved in decision management. These include: business people who specify the rules and monitor their application; business analysts. DMN is designed to work alongside BPMN and/or CMMN, providing a mechanism to model the decision-making associated with processes and cases. While BPMN, CMMN and DMN can be used independently, they were carefully designed to be complementary. Indeed, many organizations require a combination of process models for their prescriptive workflows, case models for their reactive activities, and decision models for their more complex, multi-criteria business rules. Those organizations will benefit from using the three standards in combination, selecting which one is most appropriate to each type of activity modeling. This is why BPMN, CMMN and DMN really constitute the \u201ctriple crown\u201d of process improvement standards.\"_ IBM Business Automation Open Edition 9.2.x brings a set of graphical tooling that allow you to author decisions using DMN and a lightweight engine that can execute these decisions. The engine and the authoring tooling set are decoupled and you can scale it independently. Tooling Set With IBM Business Automation Open Edition 9.2.x you can author decisions in multiple ways - and using the exact same editor across different environments: IBM Business Automation Manager Canvas brings a new experience for easily working with decisions and processes in IBM Business Automation Open Edition 9.2.x. This is a highly scalable and lightweight method for maintaining diagrams that the community has moved towards and which IBM has most recently embraced and been working hard on! Business Automation VSCode Extension A developer IDE ( Visual Studio Code ) extension that allows the visualization and editing of BPMN, DMN and Test Scenarios inside VSCode. If you are currently using IBM Business Automation Open Edition 8.0 and are utilizing things like KIE Server and Business Central, you can also build the DMN model within there. The preference for this lab is to focus on IBM Business Automation Open Edition 9.2.x, but this option is capable of being done. A core component of IBM Business Automation Open Edition 9.2.x is intercompatability between versions and IBM's heavy emphasis on DMN is that the models produced for varying levels of the product are still compatible across releases. The walkthroughs are going to focus on IBM Business Automation Manager Canvas for the DMN labs. Adding to the tools above, there are also other tools and knowledge sources you can leverage from the open source community. Listed below are you'll find resources developed in collaboration between IBM and Red Hat, for the community (not supported): DMN FEEL Handbook A handbook for the FEEL expression language from the DMN specification, as implemented by the Drools DMN open source engine. Learn DMN in 15 minutes A guided tour in a website through the elements of DMN Open Source Online Editors BPMN.new - A free online editor for business processes; DMN.new - A free online editor for decision models; PMML.new - A free online editor for scorecards; Exploring The Decisions Editor Both in Canvas and VSCode, you'll have the same components within the the DMN Editor. They consist of: Editor Pane From the Editor pane you can edit and view multiple areas including: Decision Navigator : shows the nodes used in the Decision Requirements Diagram (DRD, the diagram), and the decisions behind the nodes. Allows for quick navigation through the model. Decision Requirements Diagram Editor : the canvas in which the model can be created. Palette : Contains all the DMN constructs that can be used in a DRD, e.g. Input Node, Decision Node, etc. Expression Editor : Editor in which DMN boxed expressions, like decision tables and literal expressions, can be created. Property Panel : provides access to the properties of the model (name, namespace, etc), nodes, etc. Data Types Pane Within the Data Types pane you can define data types that are used for your decision, including complex types.","title":"Introduction"},{"location":"guided_exercises/04_dmn/introduction/#learn-decision-automation-with-dmn","text":"It's time to learn how Decision Automation can be made simple with Open Standards And Open Editions. Throughout these exercises, you'll experiment the development of decisions using Decision Model and Notation - DMN, and learn how to run, deploy and consume your decisions. Before getting started, let's get familiar with the exercises you'll be working on and get introduced to the concepts of the DMN Standard, an increasingly adopted strategy for developing rules in the decision automation market.","title":"Learn Decision Automation with DMN"},{"location":"guided_exercises/04_dmn/introduction/#goals-of-the-guided-exercises","text":"These are the labs you have available in this workshop: Getting Started, Insurance Price calculation: Start by importing an existing module. Explore, deploy and test it using local development capabilities of Open Editions. Intermediate, Vacation Days: Author a model from scratch, use decision tables, work with different hit policies, different FEEL constructs and expressions. Finally, you will deploy it and test it, optionally running it on OpenShift. Advanced, Call Center: You will author a model from scratch, create data types, consume DMN decision services from within decision nodes, and more FEEL constructs and expressions. Finally, you will deploy and test it locally and optionally on OpenShift. These are independent guided exercises and you don't need to implement the previous use case to implement the next one. If you are more comfortable with DMN, you can go to the more advanced labs immediately. Before moving forward, get up to speed with what is DMN and explore the editor capabilities you'll be using.","title":"Goals of the Guided Exercises"},{"location":"guided_exercises/04_dmn/introduction/#introduction-to-dmn","text":"","title":"Introduction to DMN"},{"location":"guided_exercises/04_dmn/introduction/#what-is-dmn","text":"DMN, along with BPMN, is managed by the Object Management Group (OMG). Let's have a look at what OMG describes about the DMN standard, in their website: OMG explanains what is DMN \"DMN is a modeling language and notation for the precise specification of business decisions and business rules. DMN is easily readable by the different types of people involved in decision management. These include: business people who specify the rules and monitor their application; business analysts. DMN is designed to work alongside BPMN and/or CMMN, providing a mechanism to model the decision-making associated with processes and cases. While BPMN, CMMN and DMN can be used independently, they were carefully designed to be complementary. Indeed, many organizations require a combination of process models for their prescriptive workflows, case models for their reactive activities, and decision models for their more complex, multi-criteria business rules. Those organizations will benefit from using the three standards in combination, selecting which one is most appropriate to each type of activity modeling. This is why BPMN, CMMN and DMN really constitute the \u201ctriple crown\u201d of process improvement standards.\"_ IBM Business Automation Open Edition 9.2.x brings a set of graphical tooling that allow you to author decisions using DMN and a lightweight engine that can execute these decisions. The engine and the authoring tooling set are decoupled and you can scale it independently.","title":"What is DMN"},{"location":"guided_exercises/04_dmn/introduction/#tooling-set","text":"With IBM Business Automation Open Edition 9.2.x you can author decisions in multiple ways - and using the exact same editor across different environments: IBM Business Automation Manager Canvas brings a new experience for easily working with decisions and processes in IBM Business Automation Open Edition 9.2.x. This is a highly scalable and lightweight method for maintaining diagrams that the community has moved towards and which IBM has most recently embraced and been working hard on! Business Automation VSCode Extension A developer IDE ( Visual Studio Code ) extension that allows the visualization and editing of BPMN, DMN and Test Scenarios inside VSCode. If you are currently using IBM Business Automation Open Edition 8.0 and are utilizing things like KIE Server and Business Central, you can also build the DMN model within there. The preference for this lab is to focus on IBM Business Automation Open Edition 9.2.x, but this option is capable of being done. A core component of IBM Business Automation Open Edition 9.2.x is intercompatability between versions and IBM's heavy emphasis on DMN is that the models produced for varying levels of the product are still compatible across releases. The walkthroughs are going to focus on IBM Business Automation Manager Canvas for the DMN labs. Adding to the tools above, there are also other tools and knowledge sources you can leverage from the open source community. Listed below are you'll find resources developed in collaboration between IBM and Red Hat, for the community (not supported): DMN FEEL Handbook A handbook for the FEEL expression language from the DMN specification, as implemented by the Drools DMN open source engine. Learn DMN in 15 minutes A guided tour in a website through the elements of DMN Open Source Online Editors BPMN.new - A free online editor for business processes; DMN.new - A free online editor for decision models; PMML.new - A free online editor for scorecards;","title":"Tooling Set"},{"location":"guided_exercises/04_dmn/introduction/#exploring-the-decisions-editor","text":"Both in Canvas and VSCode, you'll have the same components within the the DMN Editor. They consist of:","title":"Exploring The Decisions Editor"},{"location":"guided_exercises/04_dmn/introduction/#editor-pane","text":"From the Editor pane you can edit and view multiple areas including: Decision Navigator : shows the nodes used in the Decision Requirements Diagram (DRD, the diagram), and the decisions behind the nodes. Allows for quick navigation through the model. Decision Requirements Diagram Editor : the canvas in which the model can be created. Palette : Contains all the DMN constructs that can be used in a DRD, e.g. Input Node, Decision Node, etc. Expression Editor : Editor in which DMN boxed expressions, like decision tables and literal expressions, can be created. Property Panel : provides access to the properties of the model (name, namespace, etc), nodes, etc.","title":"Editor Pane"},{"location":"guided_exercises/04_dmn/introduction/#data-types-pane","text":"Within the Data Types pane you can define data types that are used for your decision, including complex types.","title":"Data Types Pane"},{"location":"guided_exercises/04_order_management/01_try-order-management-app/","text":"Getting started with BAMOE This guide shows you the experience of using IBM Business Automation Open Edition 8.0 to author, deploy, and execute your business automation applications. With three steps, this guide will get you from installation to deployment and testing of a business application: We will install BAMOE locally, and it will run on top of Red Hat JBoss EAP (a.k.a. WildFly). Once we have it up and running, we will import an existing application, so that we have an overview of some capabilities by exploring the tool and the project itself. Finally, we'll wrap up by deploying the project and testing it. Pre-requisites We expect you to have installed in your machine: Java JDK 11 ( if you don't have it yet, you can download OpenJDK built by Red Hat https://developers.redhat.com/openjdk-install ) GIT client (https://git-scm.com/) IBM Business Automation Open Edition 8.0 Installation Demo : NOTE : You should use this installer to quickly install EAP, PAM and pre-configure the environment and user access you'll need. $ git clone https://github.com/timwuthenow/ibamoe-setup.git You should now have successfully installed IBM Business Automation Open Edition 8.0. You have two key components deployed in your Red Hat EAP right now: Business Central and KIE Server . Business Central is the component that allows you to develop business assets like processes and decisions, to manage projects, build and package them. Finally, you can deploy it in KIE Server. KIE Server is a lightweight engine capable of executing business assets like processes, cases and decisions. It can be easily integrated with your services, for example via REST or JMS. Luckily, IBM Business Automation Open Edition 8.0 comes with a number of out-of-the-box template and example applications that can be used to quickly build and deploy a process microservice. Explore the Asset Let's start by accessing Business Central. In your browser, access Business Central by navigating to http://localhost:8080/business-central Log in with the credentials: User: bamAdmin Password: ibmpam1! \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use pamadmin : pamadm1n as the username password Click on \"Design, create and modify projects and pages\" Select \"MySpace\", and next, click on \"Import Project\": Insert the following repository URL, and click on Import. https://github.com/jbossdemocentral/rhpam7-order-management-demo-repo.git Select the Order-Management project and click on OK. Once the project has been imported, notice it has 27 assets. Click on the filter button \"All\" and select Process. Open the order-management process. This is the automated process that determines the approval or denial of an order request. As you see below, it is implemented with the BPMN2 standard. The final element of this process, is a sub-process \"Place Order in ERP\". This subprocess includes advanced bpmn2 modeling concepts like compensation and event based gateways. Have in mind that PAM supports the modeling of advanced flows using the bpmn2 specification, but don't worry if you don't fully get what is happening in this subprocess. Notice this process tasks are aggregated in three lanes: Manager, Purchase and Supplier. The approval decision will be made based on multiple authors, but, in this process we even have the support of automated decision. The automated decision is made on the node \"Auto Approve Decision\", that references a DMN Model that is also part of this business project. Close the process modeler. Now, filter the assets by Decision. You should see a Test Scenario and a DMN model. Open the order-approval. It is a simple decision model that can define the \"Approve\" decision based on the data input \"Order Information\" and on the \"Price Tolerance\" business rules. Now, close the decision asset. In your project page, click on the Deploy button. Business Central will trigger the build of this maven project, that will be packaged in a KJAR (the deployment unit which contains the assets) and will be deployed on the KIE server. Once the build and deployment has finished, you'll see a successful deployment message. Click on the \"View deployment details\" link. The page will show a running \u201cdefault-kieserver\u201d with the \u201corder-management_1.1-SNAPSHOT\u201d container deployed. Our business project is now available to be consumed by client applications! Let's have a look at how we can consume this business application. Experience The engine, KIE Server, is the service which exposes the business project and also the one we use when integrating with client applications. It comes with a Swagger UI that allows us to test the RESTful endpoints of the engine and consume rules deployed on it. Another way to consume our business project is to use Business Central UI to interact with the engine and test our business assets. For this hello world, let's use Business Central process and task management capabilities. In Business Central, let's open the Menu in the top bar and navigate to \"Process Definitions\" We can see three different process definitions. We'll start a new process instance based on the \"order-management\" process. Click on the actions kebab, and select \"Start\" The form that opened is also part of our business process and we can customize it if needed. For now, let's just fill in the data required to start our process instance, and click the \"Submit\" button. Item Name: Laptop Dell XPS 15 Urgency: Medium A new process instance will start in the engine. In order to visualize the current status, click on \"Diagram\". Notice we currently have a Human Task named \"Request Offer\" waiting for human intervention. Now, let's work on this task. In the Menu, access the \"Task Inbox\": In the list you should see a list of tasks you have permission to see and work on. Let's claim the Request Offer task to our user, and start working on it. Click on the kebab and select the \"Claim and Work\" option: You'll see the task data available for your analysis, as a knowledge worker - someone responsible for executing the task. Click on the blue \"Start\" button to start working on the task.Based on this offer, we'll define our reply. Inform the following data and click on the blue \"Complete\" button: Category : optional Target Price: 250 Suplier list: supplier 1 According to our process, a new task will be created for the suppliers. The supplier should provide an offer - so let's do it. Still on the task list, claim and work the task \"Prepare Offer\": Click \"Start\" blue button, inform any date, and the best offer as 1000 . Click on complete. At this point, the automatic approval was already taken, and our request was not automatically approved. You can confirm this by visualizing the process instance. On the kebab, select \"View Process\" You'll be redirected to the list of process instances. Select the process instance with id 1, and then, choose the \"Diagram\" option: At this point, you have learned how you manage processes and tasks using Business Central. You know how to start new process instances, how to interact with the process tasks and how to complete them. What about finishing this process by your own? Following the same idea, In Business Central, you can reprove the request, reject the order and reach the end of this process instance. Conclusion Congratulations, you successfully concluded a Hello World in IBM Business Automation Open Edition 8.0. In this guide, we installed Red Hat PAM, imported a project directly from GitHub, checked out the a process definition modeled and an automation decision. We wrapped up our tutorial by deploying and testing our services using Business Central UI. If you want to know more about the Order Management demo, we recommend you take a look at the project's instructions at the github repository located here .","title":"Introduction"},{"location":"guided_exercises/04_order_management/01_try-order-management-app/#getting-started-with-bamoe","text":"This guide shows you the experience of using IBM Business Automation Open Edition 8.0 to author, deploy, and execute your business automation applications. With three steps, this guide will get you from installation to deployment and testing of a business application: We will install BAMOE locally, and it will run on top of Red Hat JBoss EAP (a.k.a. WildFly). Once we have it up and running, we will import an existing application, so that we have an overview of some capabilities by exploring the tool and the project itself. Finally, we'll wrap up by deploying the project and testing it.","title":"Getting started with BAMOE"},{"location":"guided_exercises/04_order_management/01_try-order-management-app/#pre-requisites","text":"We expect you to have installed in your machine: Java JDK 11 ( if you don't have it yet, you can download OpenJDK built by Red Hat https://developers.redhat.com/openjdk-install ) GIT client (https://git-scm.com/) IBM Business Automation Open Edition 8.0 Installation Demo : NOTE : You should use this installer to quickly install EAP, PAM and pre-configure the environment and user access you'll need. $ git clone https://github.com/timwuthenow/ibamoe-setup.git You should now have successfully installed IBM Business Automation Open Edition 8.0. You have two key components deployed in your Red Hat EAP right now: Business Central and KIE Server . Business Central is the component that allows you to develop business assets like processes and decisions, to manage projects, build and package them. Finally, you can deploy it in KIE Server. KIE Server is a lightweight engine capable of executing business assets like processes, cases and decisions. It can be easily integrated with your services, for example via REST or JMS. Luckily, IBM Business Automation Open Edition 8.0 comes with a number of out-of-the-box template and example applications that can be used to quickly build and deploy a process microservice.","title":"Pre-requisites"},{"location":"guided_exercises/04_order_management/01_try-order-management-app/#explore-the-asset","text":"Let's start by accessing Business Central. In your browser, access Business Central by navigating to http://localhost:8080/business-central Log in with the credentials: User: bamAdmin Password: ibmpam1! \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use pamadmin : pamadm1n as the username password Click on \"Design, create and modify projects and pages\" Select \"MySpace\", and next, click on \"Import Project\": Insert the following repository URL, and click on Import. https://github.com/jbossdemocentral/rhpam7-order-management-demo-repo.git Select the Order-Management project and click on OK. Once the project has been imported, notice it has 27 assets. Click on the filter button \"All\" and select Process. Open the order-management process. This is the automated process that determines the approval or denial of an order request. As you see below, it is implemented with the BPMN2 standard. The final element of this process, is a sub-process \"Place Order in ERP\". This subprocess includes advanced bpmn2 modeling concepts like compensation and event based gateways. Have in mind that PAM supports the modeling of advanced flows using the bpmn2 specification, but don't worry if you don't fully get what is happening in this subprocess. Notice this process tasks are aggregated in three lanes: Manager, Purchase and Supplier. The approval decision will be made based on multiple authors, but, in this process we even have the support of automated decision. The automated decision is made on the node \"Auto Approve Decision\", that references a DMN Model that is also part of this business project. Close the process modeler. Now, filter the assets by Decision. You should see a Test Scenario and a DMN model. Open the order-approval. It is a simple decision model that can define the \"Approve\" decision based on the data input \"Order Information\" and on the \"Price Tolerance\" business rules. Now, close the decision asset. In your project page, click on the Deploy button. Business Central will trigger the build of this maven project, that will be packaged in a KJAR (the deployment unit which contains the assets) and will be deployed on the KIE server. Once the build and deployment has finished, you'll see a successful deployment message. Click on the \"View deployment details\" link. The page will show a running \u201cdefault-kieserver\u201d with the \u201corder-management_1.1-SNAPSHOT\u201d container deployed. Our business project is now available to be consumed by client applications! Let's have a look at how we can consume this business application.","title":"Explore the Asset"},{"location":"guided_exercises/04_order_management/01_try-order-management-app/#experience","text":"The engine, KIE Server, is the service which exposes the business project and also the one we use when integrating with client applications. It comes with a Swagger UI that allows us to test the RESTful endpoints of the engine and consume rules deployed on it. Another way to consume our business project is to use Business Central UI to interact with the engine and test our business assets. For this hello world, let's use Business Central process and task management capabilities. In Business Central, let's open the Menu in the top bar and navigate to \"Process Definitions\" We can see three different process definitions. We'll start a new process instance based on the \"order-management\" process. Click on the actions kebab, and select \"Start\" The form that opened is also part of our business process and we can customize it if needed. For now, let's just fill in the data required to start our process instance, and click the \"Submit\" button. Item Name: Laptop Dell XPS 15 Urgency: Medium A new process instance will start in the engine. In order to visualize the current status, click on \"Diagram\". Notice we currently have a Human Task named \"Request Offer\" waiting for human intervention. Now, let's work on this task. In the Menu, access the \"Task Inbox\": In the list you should see a list of tasks you have permission to see and work on. Let's claim the Request Offer task to our user, and start working on it. Click on the kebab and select the \"Claim and Work\" option: You'll see the task data available for your analysis, as a knowledge worker - someone responsible for executing the task. Click on the blue \"Start\" button to start working on the task.Based on this offer, we'll define our reply. Inform the following data and click on the blue \"Complete\" button: Category : optional Target Price: 250 Suplier list: supplier 1 According to our process, a new task will be created for the suppliers. The supplier should provide an offer - so let's do it. Still on the task list, claim and work the task \"Prepare Offer\": Click \"Start\" blue button, inform any date, and the best offer as 1000 . Click on complete. At this point, the automatic approval was already taken, and our request was not automatically approved. You can confirm this by visualizing the process instance. On the kebab, select \"View Process\" You'll be redirected to the list of process instances. Select the process instance with id 1, and then, choose the \"Diagram\" option: At this point, you have learned how you manage processes and tasks using Business Central. You know how to start new process instances, how to interact with the process tasks and how to complete them. What about finishing this process by your own? Following the same idea, In Business Central, you can reprove the request, reject the order and reach the end of this process instance.","title":"Experience"},{"location":"guided_exercises/04_order_management/01_try-order-management-app/#conclusion","text":"Congratulations, you successfully concluded a Hello World in IBM Business Automation Open Edition 8.0. In this guide, we installed Red Hat PAM, imported a project directly from GitHub, checked out the a process definition modeled and an automation decision. We wrapped up our tutorial by deploying and testing our services using Business Central UI. If you want to know more about the Order Management demo, we recommend you take a look at the project's instructions at the github repository located here .","title":"Conclusion"},{"location":"guided_exercises/04_order_management/02_create-order-management-app/","text":"Overview of Order Management Process This is a Process Management lab in which will implement an Order Management process. The process will use BPMN2 constructs like Swimlanes , User Tasks , Gateways , combined with decision-based routing based on a DMN Model (Decision Model & Notation). It also introduces more dynamic concepts of the IBM Business Automation Open Edition 8.0 process engine, like dynamic assignments of tasks based on process instance data. Goals Create an Order Management project in IBM Business Automation Open Edition 8.0 Define and create the process' domain model using the platform\u2019s Data Modeller. Implement an order management process in the process designer Implement decision logic in a DMN model. Create forms with the platform\u2019s Form Modeller. Deploy the project to the platform\u2019s Execution Server. Execute the end-to-end process. Pre-reqs Successful completion of the Environment Setup Lab or An existing, accessible, DM/PAM 7.3+ environment. Problem Statement In this lab we will create an Order Management process that manages the process of ordering a new phone or laptop. Start the process by providing the order information. The supplier sends an offer stating the expected delivery date and its best offer. Depending on the urgency of the urgency and the price, the order can be auto-approved by a DMN decision. If the order is not auto-approved, the manager needs to complete an approval step.","title":"Order Management Background"},{"location":"guided_exercises/04_order_management/02_create-order-management-app/#overview-of-order-management-process","text":"This is a Process Management lab in which will implement an Order Management process. The process will use BPMN2 constructs like Swimlanes , User Tasks , Gateways , combined with decision-based routing based on a DMN Model (Decision Model & Notation). It also introduces more dynamic concepts of the IBM Business Automation Open Edition 8.0 process engine, like dynamic assignments of tasks based on process instance data.","title":"Overview of Order Management Process"},{"location":"guided_exercises/04_order_management/02_create-order-management-app/#goals","text":"Create an Order Management project in IBM Business Automation Open Edition 8.0 Define and create the process' domain model using the platform\u2019s Data Modeller. Implement an order management process in the process designer Implement decision logic in a DMN model. Create forms with the platform\u2019s Form Modeller. Deploy the project to the platform\u2019s Execution Server. Execute the end-to-end process.","title":"Goals"},{"location":"guided_exercises/04_order_management/02_create-order-management-app/#pre-reqs","text":"Successful completion of the Environment Setup Lab or An existing, accessible, DM/PAM 7.3+ environment.","title":"Pre-reqs"},{"location":"guided_exercises/04_order_management/02_create-order-management-app/#problem-statement","text":"In this lab we will create an Order Management process that manages the process of ordering a new phone or laptop. Start the process by providing the order information. The supplier sends an offer stating the expected delivery date and its best offer. Depending on the urgency of the urgency and the price, the order can be auto-approved by a DMN decision. If the order is not auto-approved, the manager needs to complete an approval step.","title":"Problem Statement"},{"location":"guided_exercises/04_order_management/lab-walkthrough/","text":"Create a Project To define and deploy a business process, we first need to create a new project in which we can store the BPMN2 model, our domain model and the forms required for user interaction. To create a new project: Navigate to Business Central Login to the platform with the provided username and password. Click on Design to navigate to the Design perspective. In the Design perspective, create a new project. If your space is empty, this can be done by clicking on the blue Add Project button in the center of the page. If you already have projects in your space, you can click on the blue Add Project icon at the top right of the page. Give the project the name order-management , and the description \"Order Management\". With the project created, we can now start building our solution. Lab Walk through In this section we will first create the Domain Model within Business Central and then walk through the creation of the assets associated with the Process. The Domain Model The business process will collect and carry data through the execution of the process. This data is stored in a data model or domain model. In this lab, we collect two types of data: OrderInfo : contains information about the order, like the item and the price. SupplierInfo : contains information about the supplier, like the name and the expected delivery date. In your project, click on the Add Asset button in the middle of the screen. In the drop-down menu in the upper-left corner, select Model . Click on the Data Object tile. Give the Data Object the name OrderInfo . Leave the package set to default. Add the following fields to the OrderInfo data object: Identifier Label Type item item name String urgency urgency String targetPrice target price double managerApproval approved Boolean When you\u2019ve added the fields, save the data object by clicking on the Save button in the top menu. Use the _breadcrumb` navigator at the top-left of the screen to navigate back to our order-management project. Click on the blue Add Asset button in the top-right corner and create a new Data Object Give it the name SupplierInfo Give the SupplierInfo object the following fields: Identifier Label Type offer best offer double deliveryDate delivery date Date user user String We\u2019re done creating our data model. We can now start with our process design. Process Design With the domain model defined, we can now sketch out the main flow of the process, the actors, the user task nodes and the required automation decisions. Create a new Business Process asset. Name it OrderManagement . You can do this by clicking Add an Asset and then selecting Business Process and then setting the name as OrderManagement . When the process designer opens, scroll down in the property panel on the right side of the screen, until you see the section Process Data . Expand the Process Data section and add the following 3 Process Variables by clicking on the + sign. Name Data Type orderInfo OrderInfo supplierInfo SupplierInfo approved Boolean Prepare Offer In the palette on the left-side of the editor, select the Lane component: Create the following 3 swimlanes: Supplier , Purchase , Manager Create the Start Event node in the Purchase swimlane. Create the Prepare Offer User Task node in the Supplier swimlane and connect it to the Start Event node. Set the following properties on the node via the properties panel on the right side of the screen: Task Name: PrepareOffer Subject: Prepare Offer for #{orderInfo.item} Actors: #{supplierInfo.user} Input: Data Inputs and Assignments Name Data Type Source orderInfo OrderInfo orderInfo supplierInfo SupplierInfo supplierInfo Output Data Outputs and Assignments Name Data Type Source supplierInfo SupplierInfo supplierInfo Create the Auto Approve Order Business Rule node in the Purchase swimlane and connect it to the Prepare Offer node. Set the following properties: Rule language: DMN Assigments: Data Inputs and Assignments Name Data Type Source Order Information OrderInfo orderInfo Supplier Information SupplierInfo supplierInfo Data Outputs and Assignments Name Data Type Target Approve Boolean approved \ud83d\udcd8 INFO: After we've created our DMN Decision Model, we will revisit the configuration of this node to reference this DMN model via its name and namespace properties. Exclusive Gateway Create an X-OR Gateway / Exclusive Gateway in the Manager swimlane, below the Auto Approve Order node and connect it to that node. Create the Approve User Task in the Manager swimlane and connect it to the X-OR gateway. Set the following properties: Task Name: Approve Subject: Approve Order of #{orderInfo.item} group: rest-all Assignments: Data Inputs and Assignments Name Data Type Source orderInfo OrderInfo orderInfo supplierInfo SupplierInfo supplierInfo Data Outputs and Assignments Name Data Type Target orderInfo OrderInfo orderInfo Create an X-OR Gateway / Exclusive Gateway in the Manager swimlane, after the Approve node and connect it to that node. Create another X-OR Gateway / Exclusive Gateway under the Manager swimlane (so outside of the swimlane) and connect it to the two other X-OR Gateways / Exclusive Gateways as shown in image below: Create the Place Order in ERP Script Task under the Manager swimlane (so outside of the swimlanes) and connect it to the X-OR Gateway we created earlier. Set the following script in the node\u2019s properties properties: System . out . println ( \"Place Order in ERP\" ); Create an End Event node under the Manager swimlane (so outside of the swimlanes) and connect it to the Place Order in ERP node. Name it Approved . Create an End Event node in the Purchase swimlane and connect it to the X-OR Gateway . Name it Rejected . On the Sequence Flow from the X/OR Gateway before the Approve node that is connnected ot the other X/OR Gateway , set the following condition, which tells the process engine that this path should be taken when the order is not automatically approved: Process Variable: approved Condition: Is true On the Gateway before the Approve node , set the Default Route property to Approve . On the Sequence Flow from the X/OR Gateway after the Approve task, which is connected to the X/OR Gateway before the Place Order in ERP task, set the following condition: Process Variable: orderInfo.managerApproval Condition: Is true On the X/OR Gateway after the Approval node , set the Default Route to Rejected . Save the process definition. With the overall layout of the process definition complete, the routing logic implemented, and the I/O assignments defined, we can now implement the business rules of our automated approval decision. Business Rules and Decisions Our Order Management process contains a Business Rule Task , but we have not yet defined the Decision Model that will be used in the task. In this paragraph we will implement the automatic approval rules in the form of a DMN model. Creating the DMN Inputs and BKM In the main project page, the so called library view , click on the Add Asset button. In the next screen, set the drop-down filter to Decision . Select the DMN asset. Give it the name order-approval . In the DMN editor, open the property-panel on the right-side of the screen and set the Namespace property to: http://www.redhat.com/dmn/demo/order-management-dmn . First we need to import our data-model, so we can use it in our DMN decisions. In the DMN editor, click on the Data Types tab and click on the Import Data Object button at the right-hand side of the screen: Select both the OrderInfo and SupplierInfo objects and click on the Import button: \u200b With the 2 datatypes imported, we need to create a third type that will hold the possible values for the urgency field of our Order Information . Click on the blue Add button in the top-right corner. In the entry that opens, give the data type the Name Urgency and the Type string : Click on the Add Constraints button, select Enumeration as the constraint type , and set the values low and high`. Click on the blue checkmark button to save the type. Navigate back to the model via the Model tab. Add 2 Input nodes to the model and name them Order Information and Supplier Information Select the Order Information node. Open the properties panel on the right-hand side of the screen, and set the Data type to OrderInfo . Do the same for the Supplier Information node. Set the Data type to SupplierInfo . Create a new Business Knowledge Model node, name it Price Tolerance . Click on the node, and click on the Edit button to start editting the node: Click in the Edit parameters . An editor will open. Click on Add parameter . Name the parameter order information and set the type to OrderInfo . Right click in the empty white cell under the parameter definitions and select Clear . The text Select expression will appear in the cell. Click on the cell and select Decision Table . Add an input clause to the decision table. The name of the input clause is order information.urgency , which references the urgency attribute of the order information parameter. Set the type to Urgency , which references the Urgency enumeration we created earlier. Set the output clause data type to number . Leave the name empty. Click on the Price Tolerance cell (top cell of the table), and set the data type to number . Implement the rest of the decision table as shown below. And save the DMN model. Writing the DMN Decision In this section we will complete the writing of the DMN decision. Navigate back to the model by clicking on the Back to order-approval link at the top-left of the editor. Create a new Decision Node and name it Approve . Connect the 2 input nodes and out Price Tolerance busines knowledge model node to the new decision node. Select the Approve decision node and click on the edit button. Click on _Select Expression, and set the logic type to Literal Expression . Enter the following expression: Supplier Information.offer < Price Tolerance(Order Information) * Order Information.targetPrice Click on the Approve cell (top cell of the table), and set the data type to boolean . Connecting the Decision to the Process Navigate back to the model by clicking on the Back to order-approval link at the top-left of the editor. Our DMN model is now complete. Make sure to save your model. With our DMN model implemented, we can now revisit our Business Rules Task in our BPMN2 model. Open the order-management process definition and click on the Auto Approval Order node. Open the node\u2019s properties in the property-panel on the right side of the editor, open the Implementation/Execution section and set: Namespace: http://www.redhat.com/dmn/lab/order-approval-dmn Name: order-approval In the same properties panel, expand the Data Assignments section and open the Assignments editor Implement the following data input and output assignments. Our BPMN model is now complete. Make sure to save the model. Now, we should be able to create and implement our forms. Creating Forms In this section we are going to create the process start and user-task forms. We could simply generate these forms with the click of a button, which gives us some standard forms based on the process and task data. In this lab however, we will be creating these forms using the Form Modeler tool. This allows us to design these forms to our specific needs. Process Start Form Let\u2019s start with the process start form. We want to create the following form: In the project\u2019s library view, click on Add Asset . Filter on Form , click on the Form tile. Enter the details as shown in the screenshot below: On this form we want to specify the initial order. We therefore require fields from the orderInfo and supplierInfo process variable. When we expand the Model Fields section, we can see our 2 process variables ( orderInfo and supplierInfo ). These are both complex objects. To work with complex objects (as opposed to simple types like integers and booleans), we require a data-form for that specific object. We therefore first need to create a data-form for our OrderInfo and SupplierInfo objects. Go back to the project\u2019s library view, click again on Add Asset and create a new form. Use the following details: Using the Form Modeler constructs, create the following form: To create this form, drag both the item , urgency and targetPrice onto the canvas and configure them as follows. List Box: Radio Group: Decimal Box: Save the form and create another new form for our supplierInfo . Use the following details. . Using the Form Modeler constructs, create the following form: To create this form, drag the user field onto the canvas and configure it as follows. Save the form and open the OrderManagement form (the first form we created). Drag the orderInfo process variable onto the canvas. In the pop-up form, set the OrderManagement-Order form we just created as the Nested Form : Drag the supplierInfo process variable ontoo the canvas. In the pop-up form, set the OrderManagement-SupplierInfo form we just created as the Nested Form : Prepare Offer Form Next, we will create the form for the Prepare Offer User Task . Create a new form. Provide the following details: Our aim is to create a form that looks as such: As with the process start form, this user-task form operates on 2 variables, orderInfo and supplierInfo . And, as with the process start form, we need to create a data-object form for each of these variables. Technically, data-object forms for a certain data-object can be reused in multiple task-forms. However, creating a data-object form per task-form allows us to design these data-object forms aimed for that specific task. PrepareOffer-OrderInfo PrepareOffer-SupplierInfo Finally, we need to create the task form for the Approve task. Create a new form. Provide the following details. . Our aim is create a form that looks like this: As with the other forms, this user-task form operates on 2 variables, orderInfo , supplierInfo . And, as with the other forms, we need to create a data-object form for each of these variables. Approve-SupplierInfo Approve-OrderInfo Don\u2019t forget to save all your forms!!! The implementation of our process is complete. It\u2019s now time to deploy and test our application. Deploying the Process Service With our Order Management project\u2019s process, decisions and forms completed, we can now package our project in a Deployment Unit (KJAR) and deploy it on the Execution Server. To do this: Go back to our project\u2019s Library View (for example by clicking on the Order Management link in the breadcrumb navigation in the upper-left of the screen). Click on the Deploy button in the upper-right corner of the screen. This will package our project in a Deployment Unit (KJAR) and deploy it onto the Execution Server (KIE-Server). Go to the Execution Servers perspective by clicking on \"Menu \u2192 Deploy \u2192 Execution Servers\". You will see the Deployment Unit deployed on the Execution Server. Execute the process In this section, you will execute the process deployed on the Process Execution Server via the Business Central workbench. Navigate to Menu \u2192 Manage \u2192 Process Definitions . If everything is correct, the order-management process will be listed. Click on the kebab icon of the order-management process and click on Start . In the form that opens, pick the Huawei P10 Phone as the item and set the urgency to low . Set the target price to 700 and set the supplier name to the name of your own Business Central user (e.g. bamAmdmin ). Click on Submit . In the process instance details screen that opens, click on the Diagram tab to open the process instance diagram, which shows the current state of the process. The process is in a wait state at the Prepare Offer task. Navigateto Menu \u2192 Track Task Inbox**. Click on the Prepare Offer task to open its task window. Click on the Start button to start working on the task. Because the task has been assigned to a single user (via #{supplierInfo.user}), you don\u2019t have to first claim the task. Select a random delivery date. Set the best offer to 900 . Click on Complete . The process will continue to the Auto Approve Order decision node. Because of the target prices set, and the offered price, the decision will evaluale to false . Hence, the process will continue to the Approve task. Go back to the Task Inbox and open the Approve task. Click on Claim and on Start . In this form we can approve or disapprove the order via the approved checkbox, and specify a rejection reason if we reject the order. Approve the task by checking the approved checkbox and clicking on Complete : Go back to the process instance view and observe that the process instance is gone. Enable the Completed checkbox in the State filter on the left-hand-side of the screen. Observe that we can see our process instance in the list. Open the process instance, open it\u2019s Diagram tab. Observe that the order has been accepted: Run a couple more process instances with different values to test, for example, the functionality of the Automated Approval Rules . Correcting problems and errors During process instance execution, a lot of things can go wrong. Users might fill in incorrect data, remote services are not available, etc. In an ideal world, the process definition takes a lot of these possible problems into account in its design. E.g. the process definition might contain exception handling logic via boundary catching error events and retry-loops. However, there are situations in which an operator or administrator would like to manually change the process to another statem for example, restart an already completed User Task . In the latest version of IBM Business Automation Open Edition 8.0 this is now possible via the Process Instance interface in Business Central. Start a new process instance of our Order Management process. Complete the Prepare Offer task in such a way that the order is not automatically approved and the process will hit the Approve User Task wait state. Go to the Process Instances view and select the process instance. Navigate to the Diagram tab. Observe that the process is waiting in the Approve User Task . Click on the Prepare Offer node to select it. In the Node Actions panel on the left-hand-side of the screen, verify that the Prepare Offer node is selected and click on Trigger . Observe that the Prepare Offer User Task has been activated. Although we have re-activated the Prepare Offer node, we have not yet de-activated the Approve task. Click on the active Approve task and expand the Node Instances section in the Node Actions panel. Click on the kebab icon of the active Approve instance and click on Cancel : 6. Open the Task Inbox . Observe that the Approve User Task is gone and that we have a new Prepare Offer task. Open the Prepare Offer task, set the price to a price which will trigger the rules to automatically approve the order, and complete the task. Go to the process instances view and observe that the process instance has been completed. Enable the Completed filter in the State filter panel on the left-hand-side of the screen. Open the completed process instance and open its Diagram tab. Execute the process via APIs The Execution Server provides a rich RESTful API that allows user to interact with the process engine and deployed processes via a REST. This powerful feature allows users to create modern user interface and applications in their technology of choice (e.g. Entando DXP, ReactJS/Redux, AngularJS, etc.) and integrate these applications with the process engine to create modern, process driven, enterprise applications. The Swagger interface provides the description and documentation of the Execution Server\u2019s RESTful API. At the same time, it allows the APIs to be called from the UI. This enables developers and users to quickly test a, in this case, a deployed business process. Navigate to the KIE Server Swagger Page Locate the Process instances section. The Process Instances API provides a vast array of operations to interact with the process engine. Locate the POST operation for the resource /server/containers/{containerId}/processes/{processId}/instances . This is the RESTful operation with which we can start a new process instance. Expand the operation: Click on the Try it out button. Set the containerId to order-management (in this case we use the alias of the container). Set the processId to order-management.OrderManagement . Set Parameter content type to application/json . Set the Response content type to application/json . Set the body to: { \"orderInfo\" : { \"com.myspace.order\\_management.OrderInfo\" : { \"item\" : \"Huawei P10\" , \"urgency\" : \"low\" , \"price\" : 0.0 , \"targetPrice\" : \"700.0\" } }, \"supplierInfo\" : { \"com.myspace.order\\_management.SupplierInfo\" : { \"user\" : \"bamAdmin\" } } } Click on the Execute button. If requested, provide the username and password of your Business Central and KIE-Server user (in this example we have been using u: bamAdmin , p: ibmpam1! ). \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the pamadmin:pamadm1n information Inspect the response. Note that the operation returns the process instance id of the started process. Go back to the Business Central workbench. Go the process instances view and inspect the process instance we have just started. The RESTful API provides many more operations. Let\u2019s use the API to fetch our Task List and complete the Request Offer task. In the Swagger API, navigate to the Process queries section. Find the GET operation for the resource /server/queries/tasks/instances/pot-owners . Expand the operation and click on the Try it out button. Make sure the *Response content type is set to application/json . Leave all the other fields set to their default values. Click on the Execute button. This will return all the tasks for our user (in the case of this example this is the bamAdmin user). We can see the Prepare Offer task that is available in our inbox. Let\u2019s complete this task. Go to the Task Instances section in the Swagger interface and locate the PUT operation of the /server/containers/{containerId}/tasks/{taskInstanceId}/states/completed resource. This is the operation with which we can complete a task. Set the containerId to order-management . Set the taskInstanceId to the id of the task instance you want to complete. The task instance id an be found in the list of task instances we got back from our previous REST operation. Set auto-progress to true . This controls the auto progression of the taks through the various states of the task lifecycle (i.e. claimed, started, etc.) Set the Parameter content type to application/json . Set the Response content type to application/json . Set the body to: { \"supplierInfo\" : { \"com.myspace.order\\_management.SupplierInfo\" : { \"user\" : \"bamAdmin\" , \"offer\" : \"900\" , \"deliveryDate\" : \"2020-03-11T12:00:00.000Z\" } } } \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. { \"supplierInfo\" : { \"com.myspace.order\\_management.SupplierInfo\" : { \"user\" : \"pamadmin\" , \"offer\" : \"900\" , \"deliveryDate\" : \"2020-03-11T12:00:00.000Z\" } } } Click on the Execute button. If you\u2019ve entered everything correctly, the task will be completed and the process will move to the next wait state, the Prepare Offer task. . Go back to the Business Central workbench. Go to the process instances view. Select the process instance of the task you\u2019ve just completed. Observe that the Prepare Offer task has been completed and that the process is now waiting on the Approve User Task . The rest of the tasks can be completed in the same way via the API. Using the KIE-Server Client IBM Business Automation Open Edition 8.0 provides a KIE-Server Client API that allows the user to interact with the KIE-Server from a Java client using a higher level API. It abstracts the data marshalling and unmarshalling and the creation and execution of the RESTful commands from the developer, allowing him/her to focus on developing business logic. In this section we will create a simple Java client for our Order Management process. Create a new Maven Java JAR project in your favourite IDE (e.g. IntelliJ, Eclipse, Visual Studio Code). Add the following dependency to your project: <dependency> <groupid> org.kie.server </groupid> <artifactId> kie-server-client </artifactId> <version> 7.67.2.Final-redhat-00017 </version> <scope> compile </scope> </dependency> Create a Java package in your src/main/java folder with the name com.myspace.order_management . Download the OrderInfo.java file from this location and add it to the package you\u2019ve just created. Download the SupplierInfo.java file from this location and add it to the package. Create a new Java class called Main . Add a public static void main(String[] args) method to your main class. Before we implement our method, we first define a number of constants that we will need when implementing our method (note that the values of your constants can be different depending on your environment, model namespace, etc.): private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"order-management\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; private static final String PROCESS_ID = \"order-management.OrderManagement\" ; \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"order-management\" ; private static final String USERNAME = \"pamadmin\" ; private static final String PASSWORD = \"pamadm1n\" ; private static final String PROCESS_ID = \"order-management.OrderManagement\" ; KIE-Server client API classes can mostly be retrieved from the KieServicesFactory class. We first need to create a KieServicesConfiguration instance that will hold our credentials and defines how we want our client to communicate with the server: KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , new EnteredCredentialsProvider ( USERNAME , PASSWORD )); To allow the KIE-Server Client\u2019s marshaller to marshall and unmarshall instances of our domain model, we need to add our domain model classes to the KieServicesConfiguration . Set < Class <?>> extraClasses = new HashSet <> (); extraClasses . add ( OrderInfo . class ); extraClasses . add ( SupplierInfo . class ); kieServicesConfig . addExtraClasses ( extraClasses ); Next, we create the KieServicesClient : ~~~java KieServicesClient kieServicesClient = KieServicesFactory.newKieServicesClient(kieServicesConfig); ~~ From this client we retrieve our ProcessServicesClient : ProcessServicesClient processServicesClient = kieServicesClient . getServicesClient ( ProcessServicesClient . class ); We now create a Map which we will use to pass the process input variables. We create a new OrderInfo instance and SupplierInfo instance and put them in the Map . Map < String , Object > inputData = new HashMap <> (); OrderInfo orderInfo = new OrderInfo (); orderInfo . setItem ( \"Huawei P10\" ); orderInfo . setUrgency ( \"low\" ); inputData . put ( \"orderInfo\" , orderInfo ); SupplierInfo supplierInfo = new SupplierInfo (); supplierInfo . setUser ( \"bamAdmin\" ); inputData . put ( \"supplierInfo\" , supplierInfo ); \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use pamadmin : pamadm1n as the username password Map < String , Object > inputData = new HashMap <> (); OrderInfo orderInfo = new OrderInfo (); orderInfo . setItem ( \"Huawei P10\" ); orderInfo . setUrgency ( \"low\" ); inputData . put ( \"orderInfo\" , orderInfo ); SupplierInfo supplierInfo = new SupplierInfo (); supplierInfo . setUser ( \"pamadmin\" ); inputData . put ( \"supplierInfo\" , supplierInfo ); We can now start a new process instance via the ProcessServicesClient . Long processInstanceId = processServicesClient . startProcess ( CONTAINER_ID , PROCESS_ID , inputData ); Finally, we can print the process instance id to System.out . System . out . println ( \"New *Order Management* process instance started with instance-id: \" + processInstanceId ); Compile your project and run it. Observe the output in the console, which should say: New Order Management process instance started with instance-id The complete project can be found here: https://github.com/timwuthenow/rhpam7-order-management-demo-repo The KIE-Server Client provides more services to interact with the Execution Server: UserTaskServicesClient : provides functionality to interact with the UserTask services, for example to claim, start and complete a User Task . CaseServicesClient : provides functionality to interact with the Case Management features of the Execution Server. ProcessAdminServicesClient : provides the administration API for processes. etc. We leave as an exercise to the reader to try to complete a User Task , of the process instance we\u2019ve just created, using the UserTaskServicesClient .","title":"Lab Walk Through"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#create-a-project","text":"To define and deploy a business process, we first need to create a new project in which we can store the BPMN2 model, our domain model and the forms required for user interaction. To create a new project: Navigate to Business Central Login to the platform with the provided username and password. Click on Design to navigate to the Design perspective. In the Design perspective, create a new project. If your space is empty, this can be done by clicking on the blue Add Project button in the center of the page. If you already have projects in your space, you can click on the blue Add Project icon at the top right of the page. Give the project the name order-management , and the description \"Order Management\". With the project created, we can now start building our solution.","title":"Create a Project"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#lab-walk-through","text":"In this section we will first create the Domain Model within Business Central and then walk through the creation of the assets associated with the Process.","title":"Lab Walk through"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#the-domain-model","text":"The business process will collect and carry data through the execution of the process. This data is stored in a data model or domain model. In this lab, we collect two types of data: OrderInfo : contains information about the order, like the item and the price. SupplierInfo : contains information about the supplier, like the name and the expected delivery date. In your project, click on the Add Asset button in the middle of the screen. In the drop-down menu in the upper-left corner, select Model . Click on the Data Object tile. Give the Data Object the name OrderInfo . Leave the package set to default. Add the following fields to the OrderInfo data object: Identifier Label Type item item name String urgency urgency String targetPrice target price double managerApproval approved Boolean When you\u2019ve added the fields, save the data object by clicking on the Save button in the top menu. Use the _breadcrumb` navigator at the top-left of the screen to navigate back to our order-management project. Click on the blue Add Asset button in the top-right corner and create a new Data Object Give it the name SupplierInfo Give the SupplierInfo object the following fields: Identifier Label Type offer best offer double deliveryDate delivery date Date user user String We\u2019re done creating our data model. We can now start with our process design.","title":"The Domain Model"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#process-design","text":"With the domain model defined, we can now sketch out the main flow of the process, the actors, the user task nodes and the required automation decisions. Create a new Business Process asset. Name it OrderManagement . You can do this by clicking Add an Asset and then selecting Business Process and then setting the name as OrderManagement . When the process designer opens, scroll down in the property panel on the right side of the screen, until you see the section Process Data . Expand the Process Data section and add the following 3 Process Variables by clicking on the + sign. Name Data Type orderInfo OrderInfo supplierInfo SupplierInfo approved Boolean","title":"Process Design"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#prepare-offer","text":"In the palette on the left-side of the editor, select the Lane component: Create the following 3 swimlanes: Supplier , Purchase , Manager Create the Start Event node in the Purchase swimlane. Create the Prepare Offer User Task node in the Supplier swimlane and connect it to the Start Event node. Set the following properties on the node via the properties panel on the right side of the screen: Task Name: PrepareOffer Subject: Prepare Offer for #{orderInfo.item} Actors: #{supplierInfo.user} Input: Data Inputs and Assignments Name Data Type Source orderInfo OrderInfo orderInfo supplierInfo SupplierInfo supplierInfo Output Data Outputs and Assignments Name Data Type Source supplierInfo SupplierInfo supplierInfo Create the Auto Approve Order Business Rule node in the Purchase swimlane and connect it to the Prepare Offer node. Set the following properties: Rule language: DMN Assigments: Data Inputs and Assignments Name Data Type Source Order Information OrderInfo orderInfo Supplier Information SupplierInfo supplierInfo Data Outputs and Assignments Name Data Type Target Approve Boolean approved \ud83d\udcd8 INFO: After we've created our DMN Decision Model, we will revisit the configuration of this node to reference this DMN model via its name and namespace properties.","title":"Prepare Offer"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#exclusive-gateway","text":"Create an X-OR Gateway / Exclusive Gateway in the Manager swimlane, below the Auto Approve Order node and connect it to that node. Create the Approve User Task in the Manager swimlane and connect it to the X-OR gateway. Set the following properties: Task Name: Approve Subject: Approve Order of #{orderInfo.item} group: rest-all Assignments: Data Inputs and Assignments Name Data Type Source orderInfo OrderInfo orderInfo supplierInfo SupplierInfo supplierInfo Data Outputs and Assignments Name Data Type Target orderInfo OrderInfo orderInfo Create an X-OR Gateway / Exclusive Gateway in the Manager swimlane, after the Approve node and connect it to that node. Create another X-OR Gateway / Exclusive Gateway under the Manager swimlane (so outside of the swimlane) and connect it to the two other X-OR Gateways / Exclusive Gateways as shown in image below: Create the Place Order in ERP Script Task under the Manager swimlane (so outside of the swimlanes) and connect it to the X-OR Gateway we created earlier. Set the following script in the node\u2019s properties properties: System . out . println ( \"Place Order in ERP\" ); Create an End Event node under the Manager swimlane (so outside of the swimlanes) and connect it to the Place Order in ERP node. Name it Approved . Create an End Event node in the Purchase swimlane and connect it to the X-OR Gateway . Name it Rejected . On the Sequence Flow from the X/OR Gateway before the Approve node that is connnected ot the other X/OR Gateway , set the following condition, which tells the process engine that this path should be taken when the order is not automatically approved: Process Variable: approved Condition: Is true On the Gateway before the Approve node , set the Default Route property to Approve . On the Sequence Flow from the X/OR Gateway after the Approve task, which is connected to the X/OR Gateway before the Place Order in ERP task, set the following condition: Process Variable: orderInfo.managerApproval Condition: Is true On the X/OR Gateway after the Approval node , set the Default Route to Rejected . Save the process definition. With the overall layout of the process definition complete, the routing logic implemented, and the I/O assignments defined, we can now implement the business rules of our automated approval decision.","title":"Exclusive Gateway"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#business-rules-and-decisions","text":"Our Order Management process contains a Business Rule Task , but we have not yet defined the Decision Model that will be used in the task. In this paragraph we will implement the automatic approval rules in the form of a DMN model.","title":"Business Rules and Decisions"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#creating-the-dmn-inputs-and-bkm","text":"In the main project page, the so called library view , click on the Add Asset button. In the next screen, set the drop-down filter to Decision . Select the DMN asset. Give it the name order-approval . In the DMN editor, open the property-panel on the right-side of the screen and set the Namespace property to: http://www.redhat.com/dmn/demo/order-management-dmn . First we need to import our data-model, so we can use it in our DMN decisions. In the DMN editor, click on the Data Types tab and click on the Import Data Object button at the right-hand side of the screen: Select both the OrderInfo and SupplierInfo objects and click on the Import button: \u200b With the 2 datatypes imported, we need to create a third type that will hold the possible values for the urgency field of our Order Information . Click on the blue Add button in the top-right corner. In the entry that opens, give the data type the Name Urgency and the Type string : Click on the Add Constraints button, select Enumeration as the constraint type , and set the values low and high`. Click on the blue checkmark button to save the type. Navigate back to the model via the Model tab. Add 2 Input nodes to the model and name them Order Information and Supplier Information Select the Order Information node. Open the properties panel on the right-hand side of the screen, and set the Data type to OrderInfo . Do the same for the Supplier Information node. Set the Data type to SupplierInfo . Create a new Business Knowledge Model node, name it Price Tolerance . Click on the node, and click on the Edit button to start editting the node: Click in the Edit parameters . An editor will open. Click on Add parameter . Name the parameter order information and set the type to OrderInfo . Right click in the empty white cell under the parameter definitions and select Clear . The text Select expression will appear in the cell. Click on the cell and select Decision Table . Add an input clause to the decision table. The name of the input clause is order information.urgency , which references the urgency attribute of the order information parameter. Set the type to Urgency , which references the Urgency enumeration we created earlier. Set the output clause data type to number . Leave the name empty. Click on the Price Tolerance cell (top cell of the table), and set the data type to number . Implement the rest of the decision table as shown below. And save the DMN model.","title":"Creating the DMN Inputs and BKM"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#writing-the-dmn-decision","text":"In this section we will complete the writing of the DMN decision. Navigate back to the model by clicking on the Back to order-approval link at the top-left of the editor. Create a new Decision Node and name it Approve . Connect the 2 input nodes and out Price Tolerance busines knowledge model node to the new decision node. Select the Approve decision node and click on the edit button. Click on _Select Expression, and set the logic type to Literal Expression . Enter the following expression: Supplier Information.offer < Price Tolerance(Order Information) * Order Information.targetPrice Click on the Approve cell (top cell of the table), and set the data type to boolean .","title":"Writing the DMN Decision"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#connecting-the-decision-to-the-process","text":"Navigate back to the model by clicking on the Back to order-approval link at the top-left of the editor. Our DMN model is now complete. Make sure to save your model. With our DMN model implemented, we can now revisit our Business Rules Task in our BPMN2 model. Open the order-management process definition and click on the Auto Approval Order node. Open the node\u2019s properties in the property-panel on the right side of the editor, open the Implementation/Execution section and set: Namespace: http://www.redhat.com/dmn/lab/order-approval-dmn Name: order-approval In the same properties panel, expand the Data Assignments section and open the Assignments editor Implement the following data input and output assignments. Our BPMN model is now complete. Make sure to save the model. Now, we should be able to create and implement our forms.","title":"Connecting the Decision to the Process"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#creating-forms","text":"In this section we are going to create the process start and user-task forms. We could simply generate these forms with the click of a button, which gives us some standard forms based on the process and task data. In this lab however, we will be creating these forms using the Form Modeler tool. This allows us to design these forms to our specific needs.","title":"Creating Forms"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#process-start-form","text":"Let\u2019s start with the process start form. We want to create the following form: In the project\u2019s library view, click on Add Asset . Filter on Form , click on the Form tile. Enter the details as shown in the screenshot below: On this form we want to specify the initial order. We therefore require fields from the orderInfo and supplierInfo process variable. When we expand the Model Fields section, we can see our 2 process variables ( orderInfo and supplierInfo ). These are both complex objects. To work with complex objects (as opposed to simple types like integers and booleans), we require a data-form for that specific object. We therefore first need to create a data-form for our OrderInfo and SupplierInfo objects. Go back to the project\u2019s library view, click again on Add Asset and create a new form. Use the following details: Using the Form Modeler constructs, create the following form: To create this form, drag both the item , urgency and targetPrice onto the canvas and configure them as follows. List Box: Radio Group: Decimal Box: Save the form and create another new form for our supplierInfo . Use the following details. . Using the Form Modeler constructs, create the following form: To create this form, drag the user field onto the canvas and configure it as follows. Save the form and open the OrderManagement form (the first form we created). Drag the orderInfo process variable onto the canvas. In the pop-up form, set the OrderManagement-Order form we just created as the Nested Form : Drag the supplierInfo process variable ontoo the canvas. In the pop-up form, set the OrderManagement-SupplierInfo form we just created as the Nested Form :","title":"Process Start Form"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#prepare-offer-form","text":"Next, we will create the form for the Prepare Offer User Task . Create a new form. Provide the following details: Our aim is to create a form that looks as such: As with the process start form, this user-task form operates on 2 variables, orderInfo and supplierInfo . And, as with the process start form, we need to create a data-object form for each of these variables. Technically, data-object forms for a certain data-object can be reused in multiple task-forms. However, creating a data-object form per task-form allows us to design these data-object forms aimed for that specific task. PrepareOffer-OrderInfo PrepareOffer-SupplierInfo Finally, we need to create the task form for the Approve task. Create a new form. Provide the following details. . Our aim is create a form that looks like this: As with the other forms, this user-task form operates on 2 variables, orderInfo , supplierInfo . And, as with the other forms, we need to create a data-object form for each of these variables. Approve-SupplierInfo Approve-OrderInfo Don\u2019t forget to save all your forms!!! The implementation of our process is complete. It\u2019s now time to deploy and test our application.","title":"Prepare Offer Form"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#deploying-the-process-service","text":"With our Order Management project\u2019s process, decisions and forms completed, we can now package our project in a Deployment Unit (KJAR) and deploy it on the Execution Server. To do this: Go back to our project\u2019s Library View (for example by clicking on the Order Management link in the breadcrumb navigation in the upper-left of the screen). Click on the Deploy button in the upper-right corner of the screen. This will package our project in a Deployment Unit (KJAR) and deploy it onto the Execution Server (KIE-Server). Go to the Execution Servers perspective by clicking on \"Menu \u2192 Deploy \u2192 Execution Servers\". You will see the Deployment Unit deployed on the Execution Server.","title":"Deploying the Process Service"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#execute-the-process","text":"In this section, you will execute the process deployed on the Process Execution Server via the Business Central workbench. Navigate to Menu \u2192 Manage \u2192 Process Definitions . If everything is correct, the order-management process will be listed. Click on the kebab icon of the order-management process and click on Start . In the form that opens, pick the Huawei P10 Phone as the item and set the urgency to low . Set the target price to 700 and set the supplier name to the name of your own Business Central user (e.g. bamAmdmin ). Click on Submit . In the process instance details screen that opens, click on the Diagram tab to open the process instance diagram, which shows the current state of the process. The process is in a wait state at the Prepare Offer task. Navigateto Menu \u2192 Track Task Inbox**. Click on the Prepare Offer task to open its task window. Click on the Start button to start working on the task. Because the task has been assigned to a single user (via #{supplierInfo.user}), you don\u2019t have to first claim the task. Select a random delivery date. Set the best offer to 900 . Click on Complete . The process will continue to the Auto Approve Order decision node. Because of the target prices set, and the offered price, the decision will evaluale to false . Hence, the process will continue to the Approve task. Go back to the Task Inbox and open the Approve task. Click on Claim and on Start . In this form we can approve or disapprove the order via the approved checkbox, and specify a rejection reason if we reject the order. Approve the task by checking the approved checkbox and clicking on Complete : Go back to the process instance view and observe that the process instance is gone. Enable the Completed checkbox in the State filter on the left-hand-side of the screen. Observe that we can see our process instance in the list. Open the process instance, open it\u2019s Diagram tab. Observe that the order has been accepted: Run a couple more process instances with different values to test, for example, the functionality of the Automated Approval Rules .","title":"Execute the process"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#correcting-problems-and-errors","text":"During process instance execution, a lot of things can go wrong. Users might fill in incorrect data, remote services are not available, etc. In an ideal world, the process definition takes a lot of these possible problems into account in its design. E.g. the process definition might contain exception handling logic via boundary catching error events and retry-loops. However, there are situations in which an operator or administrator would like to manually change the process to another statem for example, restart an already completed User Task . In the latest version of IBM Business Automation Open Edition 8.0 this is now possible via the Process Instance interface in Business Central. Start a new process instance of our Order Management process. Complete the Prepare Offer task in such a way that the order is not automatically approved and the process will hit the Approve User Task wait state. Go to the Process Instances view and select the process instance. Navigate to the Diagram tab. Observe that the process is waiting in the Approve User Task . Click on the Prepare Offer node to select it. In the Node Actions panel on the left-hand-side of the screen, verify that the Prepare Offer node is selected and click on Trigger . Observe that the Prepare Offer User Task has been activated. Although we have re-activated the Prepare Offer node, we have not yet de-activated the Approve task. Click on the active Approve task and expand the Node Instances section in the Node Actions panel. Click on the kebab icon of the active Approve instance and click on Cancel : 6. Open the Task Inbox . Observe that the Approve User Task is gone and that we have a new Prepare Offer task. Open the Prepare Offer task, set the price to a price which will trigger the rules to automatically approve the order, and complete the task. Go to the process instances view and observe that the process instance has been completed. Enable the Completed filter in the State filter panel on the left-hand-side of the screen. Open the completed process instance and open its Diagram tab.","title":"Correcting problems and errors"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#execute-the-process-via-apis","text":"The Execution Server provides a rich RESTful API that allows user to interact with the process engine and deployed processes via a REST. This powerful feature allows users to create modern user interface and applications in their technology of choice (e.g. Entando DXP, ReactJS/Redux, AngularJS, etc.) and integrate these applications with the process engine to create modern, process driven, enterprise applications. The Swagger interface provides the description and documentation of the Execution Server\u2019s RESTful API. At the same time, it allows the APIs to be called from the UI. This enables developers and users to quickly test a, in this case, a deployed business process. Navigate to the KIE Server Swagger Page Locate the Process instances section. The Process Instances API provides a vast array of operations to interact with the process engine. Locate the POST operation for the resource /server/containers/{containerId}/processes/{processId}/instances . This is the RESTful operation with which we can start a new process instance. Expand the operation: Click on the Try it out button. Set the containerId to order-management (in this case we use the alias of the container). Set the processId to order-management.OrderManagement . Set Parameter content type to application/json . Set the Response content type to application/json . Set the body to: { \"orderInfo\" : { \"com.myspace.order\\_management.OrderInfo\" : { \"item\" : \"Huawei P10\" , \"urgency\" : \"low\" , \"price\" : 0.0 , \"targetPrice\" : \"700.0\" } }, \"supplierInfo\" : { \"com.myspace.order\\_management.SupplierInfo\" : { \"user\" : \"bamAdmin\" } } } Click on the Execute button. If requested, provide the username and password of your Business Central and KIE-Server user (in this example we have been using u: bamAdmin , p: ibmpam1! ). \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the pamadmin:pamadm1n information Inspect the response. Note that the operation returns the process instance id of the started process. Go back to the Business Central workbench. Go the process instances view and inspect the process instance we have just started. The RESTful API provides many more operations. Let\u2019s use the API to fetch our Task List and complete the Request Offer task. In the Swagger API, navigate to the Process queries section. Find the GET operation for the resource /server/queries/tasks/instances/pot-owners . Expand the operation and click on the Try it out button. Make sure the *Response content type is set to application/json . Leave all the other fields set to their default values. Click on the Execute button. This will return all the tasks for our user (in the case of this example this is the bamAdmin user). We can see the Prepare Offer task that is available in our inbox. Let\u2019s complete this task. Go to the Task Instances section in the Swagger interface and locate the PUT operation of the /server/containers/{containerId}/tasks/{taskInstanceId}/states/completed resource. This is the operation with which we can complete a task. Set the containerId to order-management . Set the taskInstanceId to the id of the task instance you want to complete. The task instance id an be found in the list of task instances we got back from our previous REST operation. Set auto-progress to true . This controls the auto progression of the taks through the various states of the task lifecycle (i.e. claimed, started, etc.) Set the Parameter content type to application/json . Set the Response content type to application/json . Set the body to: { \"supplierInfo\" : { \"com.myspace.order\\_management.SupplierInfo\" : { \"user\" : \"bamAdmin\" , \"offer\" : \"900\" , \"deliveryDate\" : \"2020-03-11T12:00:00.000Z\" } } } \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. { \"supplierInfo\" : { \"com.myspace.order\\_management.SupplierInfo\" : { \"user\" : \"pamadmin\" , \"offer\" : \"900\" , \"deliveryDate\" : \"2020-03-11T12:00:00.000Z\" } } } Click on the Execute button. If you\u2019ve entered everything correctly, the task will be completed and the process will move to the next wait state, the Prepare Offer task. . Go back to the Business Central workbench. Go to the process instances view. Select the process instance of the task you\u2019ve just completed. Observe that the Prepare Offer task has been completed and that the process is now waiting on the Approve User Task . The rest of the tasks can be completed in the same way via the API.","title":"Execute the process via APIs"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#using-the-kie-server-client","text":"IBM Business Automation Open Edition 8.0 provides a KIE-Server Client API that allows the user to interact with the KIE-Server from a Java client using a higher level API. It abstracts the data marshalling and unmarshalling and the creation and execution of the RESTful commands from the developer, allowing him/her to focus on developing business logic. In this section we will create a simple Java client for our Order Management process. Create a new Maven Java JAR project in your favourite IDE (e.g. IntelliJ, Eclipse, Visual Studio Code). Add the following dependency to your project: <dependency> <groupid> org.kie.server </groupid> <artifactId> kie-server-client </artifactId> <version> 7.67.2.Final-redhat-00017 </version> <scope> compile </scope> </dependency> Create a Java package in your src/main/java folder with the name com.myspace.order_management . Download the OrderInfo.java file from this location and add it to the package you\u2019ve just created. Download the SupplierInfo.java file from this location and add it to the package. Create a new Java class called Main . Add a public static void main(String[] args) method to your main class. Before we implement our method, we first define a number of constants that we will need when implementing our method (note that the values of your constants can be different depending on your environment, model namespace, etc.): private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"order-management\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; private static final String PROCESS_ID = \"order-management.OrderManagement\" ; \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"order-management\" ; private static final String USERNAME = \"pamadmin\" ; private static final String PASSWORD = \"pamadm1n\" ; private static final String PROCESS_ID = \"order-management.OrderManagement\" ; KIE-Server client API classes can mostly be retrieved from the KieServicesFactory class. We first need to create a KieServicesConfiguration instance that will hold our credentials and defines how we want our client to communicate with the server: KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , new EnteredCredentialsProvider ( USERNAME , PASSWORD )); To allow the KIE-Server Client\u2019s marshaller to marshall and unmarshall instances of our domain model, we need to add our domain model classes to the KieServicesConfiguration . Set < Class <?>> extraClasses = new HashSet <> (); extraClasses . add ( OrderInfo . class ); extraClasses . add ( SupplierInfo . class ); kieServicesConfig . addExtraClasses ( extraClasses ); Next, we create the KieServicesClient : ~~~java KieServicesClient kieServicesClient = KieServicesFactory.newKieServicesClient(kieServicesConfig); ~~ From this client we retrieve our ProcessServicesClient : ProcessServicesClient processServicesClient = kieServicesClient . getServicesClient ( ProcessServicesClient . class ); We now create a Map which we will use to pass the process input variables. We create a new OrderInfo instance and SupplierInfo instance and put them in the Map . Map < String , Object > inputData = new HashMap <> (); OrderInfo orderInfo = new OrderInfo (); orderInfo . setItem ( \"Huawei P10\" ); orderInfo . setUrgency ( \"low\" ); inputData . put ( \"orderInfo\" , orderInfo ); SupplierInfo supplierInfo = new SupplierInfo (); supplierInfo . setUser ( \"bamAdmin\" ); inputData . put ( \"supplierInfo\" , supplierInfo ); \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use pamadmin : pamadm1n as the username password Map < String , Object > inputData = new HashMap <> (); OrderInfo orderInfo = new OrderInfo (); orderInfo . setItem ( \"Huawei P10\" ); orderInfo . setUrgency ( \"low\" ); inputData . put ( \"orderInfo\" , orderInfo ); SupplierInfo supplierInfo = new SupplierInfo (); supplierInfo . setUser ( \"pamadmin\" ); inputData . put ( \"supplierInfo\" , supplierInfo ); We can now start a new process instance via the ProcessServicesClient . Long processInstanceId = processServicesClient . startProcess ( CONTAINER_ID , PROCESS_ID , inputData ); Finally, we can print the process instance id to System.out . System . out . println ( \"New *Order Management* process instance started with instance-id: \" + processInstanceId ); Compile your project and run it. Observe the output in the console, which should say: New Order Management process instance started with instance-id The complete project can be found here: https://github.com/timwuthenow/rhpam7-order-management-demo-repo The KIE-Server Client provides more services to interact with the Execution Server: UserTaskServicesClient : provides functionality to interact with the UserTask services, for example to claim, start and complete a User Task . CaseServicesClient : provides functionality to interact with the Case Management features of the Execution Server. ProcessAdminServicesClient : provides the administration API for processes. etc. We leave as an exercise to the reader to try to complete a User Task , of the process instance we\u2019ve just created, using the UserTaskServicesClient .","title":"Using the KIE-Server Client"},{"location":"guided_exercises/05_bam_kafka/","text":"BAMOE + Kafka Workshop A set of guided labs to get you up started on how to: Running locally Have docker or podman running locally: ~~~shell docker run -it --rm -p 8080:8080 -v $(pwd):/app-data -e CONTENT_URL_PREFIX=\"file:///app-data\" -e WORKSHOPS_URLS=\"file:///app-data/_bam_kafka_workshop.yml\" -e LOG_TO_STDOUT=true quay.io/osevg/workshopper ~~~ Running on ocp Login on ocp and: ~~~shell oc create -f support/ocp-provisioning.yml ~~~","title":"BAMOE + Kafka Workshop"},{"location":"guided_exercises/05_bam_kafka/#bamoe-kafka-workshop","text":"","title":"BAMOE + Kafka Workshop"},{"location":"guided_exercises/05_bam_kafka/#a-set-of-guided-labs-to-get-you-up-started-on-how-to","text":"","title":"A set of guided labs to get you up started on how to:"},{"location":"guided_exercises/05_bam_kafka/#running-locally","text":"Have docker or podman running locally: ~~~shell docker run -it --rm -p 8080:8080 -v $(pwd):/app-data -e CONTENT_URL_PREFIX=\"file:///app-data\" -e WORKSHOPS_URLS=\"file:///app-data/_bam_kafka_workshop.yml\" -e LOG_TO_STDOUT=true quay.io/osevg/workshopper ~~~","title":"Running locally"},{"location":"guided_exercises/05_bam_kafka/#running-on-ocp","text":"Login on ocp and: ~~~shell oc create -f support/ocp-provisioning.yml ~~~","title":"Running on ocp"},{"location":"guided_exercises/05_bam_kafka/00_introduction/","text":"Introduction In this guided lab let\u2019s see in practice how we can use process automation applications that fits within event-driven architectures. We can list at least three ways to adjust our business application fit within EDA: We can build processes that can react to events that happen in the ecosystem; From within the process, we can emit events to notify the ecosystem about key activities in the business process and interact with external services via events; We can track every transaction committed either for business processes, cases (case management), or human tasks by publishing events for. The alignment of tech evolution and business standards like BPMN When providing an implementation for a specification, each provider has the opportunity to deliver the solution of choice. It is not different for the BPMN specification. It allows different implementations for its diagram elements, and this is how BAMOE delivers the most recent tech concepts by still allowing business users to use the modeling notation they are used to. In BAMOE (a.k.a. jBPM), it is possible to make use of message events (starting, intermediate or ending) to interact via events. In this case, the KIE Server Kafka extension makes sure the communication occurs effectively with the event streaming brokers. In the upcoming labs we will learn how to model the processes and when and how to add configurations to the business project and KIE Server.","title":"Introduction"},{"location":"guided_exercises/05_bam_kafka/00_introduction/#introduction","text":"In this guided lab let\u2019s see in practice how we can use process automation applications that fits within event-driven architectures. We can list at least three ways to adjust our business application fit within EDA: We can build processes that can react to events that happen in the ecosystem; From within the process, we can emit events to notify the ecosystem about key activities in the business process and interact with external services via events; We can track every transaction committed either for business processes, cases (case management), or human tasks by publishing events for.","title":"Introduction"},{"location":"guided_exercises/05_bam_kafka/00_introduction/#the-alignment-of-tech-evolution-and-business-standards-like-bpmn","text":"When providing an implementation for a specification, each provider has the opportunity to deliver the solution of choice. It is not different for the BPMN specification. It allows different implementations for its diagram elements, and this is how BAMOE delivers the most recent tech concepts by still allowing business users to use the modeling notation they are used to. In BAMOE (a.k.a. jBPM), it is possible to make use of message events (starting, intermediate or ending) to interact via events. In this case, the KIE Server Kafka extension makes sure the communication occurs effectively with the event streaming brokers. In the upcoming labs we will learn how to model the processes and when and how to add configurations to the business project and KIE Server.","title":"The alignment of tech evolution and business standards like BPMN"},{"location":"guided_exercises/05_bam_kafka/01_lab-one-setup/","text":"BAMOE 9.2.0 Kafka extension In order to be able to start processes based on new events, we will need to configure the BAMOEKafka extension. The BAMOE Kafka extension allows the KIE Server (process and decision engine) to react to events and publish events to kafka topics. \ud83d\udcd8 INFO: There are several options in BAMOE to customize the Kafka address, topic names, etc. In our case, we\u2019re using the default Kafka address, which is, localhost:9092. More customization information can be found in the official Red Hat product documentation: Configuring a KIE Server to send and receive Kafka messages from the process. In this setup steps, we will configure BAMOE only in the server level - we are not yet configuring the business project . We will see how to configure the project as we move forward on the labs. Enabling the Kafka extension We can configure the engine to support different capabilities. In order to enable processes to be started through eventing, we only need to enable the extension via system property. With BAMOE up and running, execute the following, where $JBOSS_HOME is the installation directory of the JBoss EAP instance you're running KIE Server from: $JBOSS_HOME /bin/jboss-cli.sh -c [ standalone@localhost:9990 / ] /system-property = org.kie.kafka.server.ext.disabled:add ( value = false ) [ standalone@localhost:9990 / ] :shutdown ( restart = true ) The first command will enable the Kafka extension. Next, we're restarting EAP so that the new configuration is active. You can check EAP logs to confirm it is restarting. The following output will show up in BAMOE logs: INFO [org.kie.server.services.impl.KieServerImpl] (ServerService Thread Pool -- 74) Kafka KIE Server extension has been successfully registered as server extension This is the only configuration you will need in a server level to be able to start processes using events.","title":"Setup - Starting Processes with Events"},{"location":"guided_exercises/05_bam_kafka/01_lab-one-setup/#bamoe-920-kafka-extension","text":"In order to be able to start processes based on new events, we will need to configure the BAMOEKafka extension. The BAMOE Kafka extension allows the KIE Server (process and decision engine) to react to events and publish events to kafka topics. \ud83d\udcd8 INFO: There are several options in BAMOE to customize the Kafka address, topic names, etc. In our case, we\u2019re using the default Kafka address, which is, localhost:9092. More customization information can be found in the official Red Hat product documentation: Configuring a KIE Server to send and receive Kafka messages from the process. In this setup steps, we will configure BAMOE only in the server level - we are not yet configuring the business project . We will see how to configure the project as we move forward on the labs.","title":"BAMOE 9.2.0 Kafka extension"},{"location":"guided_exercises/05_bam_kafka/01_lab-one-setup/#enabling-the-kafka-extension","text":"We can configure the engine to support different capabilities. In order to enable processes to be started through eventing, we only need to enable the extension via system property. With BAMOE up and running, execute the following, where $JBOSS_HOME is the installation directory of the JBoss EAP instance you're running KIE Server from: $JBOSS_HOME /bin/jboss-cli.sh -c [ standalone@localhost:9990 / ] /system-property = org.kie.kafka.server.ext.disabled:add ( value = false ) [ standalone@localhost:9990 / ] :shutdown ( restart = true ) The first command will enable the Kafka extension. Next, we're restarting EAP so that the new configuration is active. You can check EAP logs to confirm it is restarting. The following output will show up in BAMOE logs: INFO [org.kie.server.services.impl.KieServerImpl] (ServerService Thread Pool -- 74) Kafka KIE Server extension has been successfully registered as server extension This is the only configuration you will need in a server level to be able to start processes using events.","title":"Enabling the Kafka extension"},{"location":"guided_exercises/05_bam_kafka/02_start-event/","text":"The Credit Card Raise Approval Project In this use case we would like to handle the automation of a credit limit increase approval process. Most card issuers allow customers to request an increased credit limit through various entry points such as: websites, their mobile applications or over the phone with customer service. Let\u2019s consider we need to deliver this automation for a bank that wants to achieve a similar use case within an event-driven architecture. The existing process is started via REST. It has a step for automatic request validation using DMN, and if the request not approved, it goes to a manual review queue. If approved, the service responsible for updating the cc limit is invoked via REST (the diagram only represents this REST call with a script task since this is not relevant for this lab's scenario). Finally, the process ends either with an approved or denied request. Now, with the architecture shift, the service responsible for increasing the credit card limit should not be invoked via REST anymore. The external service now listens to the topic \u201crequest-approved\u201d in order to track when to execute the limit raise. The business process should get started based on events, and whenever the process finishes, it should post a message to a specific topic depending on whether the request was approved or not. Process v2. Whenever a new event happens in a topic, a new instance will be triggered. Depending on how this process ends, an event is published in a different topic, therefore, different services can react based on the approval status . In this strategy we have a resilient way of communication between services where the broker is responsible for storing and providing the events. Adding to that, the tech team can evolve the solutions by using the features available in Kafka itself, like the possibility to replay all the events that happened in a specific time, in chronological order. Importing the project Let's import the existing project so we can start implementing the eventing capabilities. Access Business Central, and import the following project: https://github.com/kmacedovarela/cc-limit-approval-app-step1 Let's check the existing project. Open the cc-limit-raise-approval process. Notice that currently it is a traditional process, with a standard start and stop node. Processes like this can be started either via REST or JMS. Reacting to events The first task we'll do, is to enable the existing process to react to events that are published in a specific topic. Whenever a new event is published, a new process instance should be created. To allow this process definition to be started with events, the first step is to change the start event to a Start Message Event by clicking the node in the editor and selecting the envelope icon to \"Convert into Start Message\": Whenever a customer make a new request (independently of the channel used) an event should be published on the incoming-requests Kafka topic . This way as new channels are ready to be added, they just need to point to this request instead of modifying for the process end point. With that, a new process instance should be started whenever a new event is published in this topic. Let's configure the Start Message Event with the incoming-requests topic: \ud83d\udea7 WARN: we need to receive the data that is the event data. The KIE Server provides automatic marshalling to help us mapping the input directly to a Data Object (a POJO). This project has an object named LimitRaiseRequest.java which we will use to receive the incoming data and feed it in the process. On the properties panel of the Start Message Event , configure the input data: Name: request Data Type: LimitRaiseRequest Target: request Save the process. Your process should now look like this: Deploying the project Now, let's deploy and test the project. On the breadcrumb, click on \"cc-limit-approval-app-step1\" to go back to the Project Explorer view. Click on the \"Deploy\" button. Testing the project Let's publish a new event in the incoming-requests topic using the Kafka producer CLI tool. Open a new tab in your terminal and access the strimzi-all-in-one project folder. cd ~/enablement/amq-examples/strimzi-all-in-one Next, use the Kafka producer to publish new messages on the topic incoming-requests . docker-compose exec kafka bin/kafka-console-producer.sh --topic incoming-requests --bootstrap-server localhost:9092 You can send the following data, and press enter: { \"customerId\" : 1 , \"customerScore\" : 250 , \"requestedValue\" : 1500 } Back to the browser, open Business Central. On the top menu, go to Menu -> Manage -> Process Instances . On the left column, filter by \"Completed\" State. You should see as many instances as the number of events you published on Kafka: Select a process instance, and next, select the tab Diagram . You should see something like:","title":"Starting Processes with Events"},{"location":"guided_exercises/05_bam_kafka/02_start-event/#the-credit-card-raise-approval-project","text":"In this use case we would like to handle the automation of a credit limit increase approval process. Most card issuers allow customers to request an increased credit limit through various entry points such as: websites, their mobile applications or over the phone with customer service. Let\u2019s consider we need to deliver this automation for a bank that wants to achieve a similar use case within an event-driven architecture. The existing process is started via REST. It has a step for automatic request validation using DMN, and if the request not approved, it goes to a manual review queue. If approved, the service responsible for updating the cc limit is invoked via REST (the diagram only represents this REST call with a script task since this is not relevant for this lab's scenario). Finally, the process ends either with an approved or denied request. Now, with the architecture shift, the service responsible for increasing the credit card limit should not be invoked via REST anymore. The external service now listens to the topic \u201crequest-approved\u201d in order to track when to execute the limit raise. The business process should get started based on events, and whenever the process finishes, it should post a message to a specific topic depending on whether the request was approved or not. Process v2. Whenever a new event happens in a topic, a new instance will be triggered. Depending on how this process ends, an event is published in a different topic, therefore, different services can react based on the approval status . In this strategy we have a resilient way of communication between services where the broker is responsible for storing and providing the events. Adding to that, the tech team can evolve the solutions by using the features available in Kafka itself, like the possibility to replay all the events that happened in a specific time, in chronological order.","title":"The Credit Card Raise Approval Project"},{"location":"guided_exercises/05_bam_kafka/02_start-event/#importing-the-project","text":"Let's import the existing project so we can start implementing the eventing capabilities. Access Business Central, and import the following project: https://github.com/kmacedovarela/cc-limit-approval-app-step1 Let's check the existing project. Open the cc-limit-raise-approval process. Notice that currently it is a traditional process, with a standard start and stop node. Processes like this can be started either via REST or JMS.","title":"Importing the project"},{"location":"guided_exercises/05_bam_kafka/02_start-event/#reacting-to-events","text":"The first task we'll do, is to enable the existing process to react to events that are published in a specific topic. Whenever a new event is published, a new process instance should be created. To allow this process definition to be started with events, the first step is to change the start event to a Start Message Event by clicking the node in the editor and selecting the envelope icon to \"Convert into Start Message\": Whenever a customer make a new request (independently of the channel used) an event should be published on the incoming-requests Kafka topic . This way as new channels are ready to be added, they just need to point to this request instead of modifying for the process end point. With that, a new process instance should be started whenever a new event is published in this topic. Let's configure the Start Message Event with the incoming-requests topic: \ud83d\udea7 WARN: we need to receive the data that is the event data. The KIE Server provides automatic marshalling to help us mapping the input directly to a Data Object (a POJO). This project has an object named LimitRaiseRequest.java which we will use to receive the incoming data and feed it in the process. On the properties panel of the Start Message Event , configure the input data: Name: request Data Type: LimitRaiseRequest Target: request Save the process. Your process should now look like this:","title":"Reacting to events"},{"location":"guided_exercises/05_bam_kafka/02_start-event/#deploying-the-project","text":"Now, let's deploy and test the project. On the breadcrumb, click on \"cc-limit-approval-app-step1\" to go back to the Project Explorer view. Click on the \"Deploy\" button.","title":"Deploying the project"},{"location":"guided_exercises/05_bam_kafka/02_start-event/#testing-the-project","text":"Let's publish a new event in the incoming-requests topic using the Kafka producer CLI tool. Open a new tab in your terminal and access the strimzi-all-in-one project folder. cd ~/enablement/amq-examples/strimzi-all-in-one Next, use the Kafka producer to publish new messages on the topic incoming-requests . docker-compose exec kafka bin/kafka-console-producer.sh --topic incoming-requests --bootstrap-server localhost:9092 You can send the following data, and press enter: { \"customerId\" : 1 , \"customerScore\" : 250 , \"requestedValue\" : 1500 } Back to the browser, open Business Central. On the top menu, go to Menu -> Manage -> Process Instances . On the left column, filter by \"Completed\" State. You should see as many instances as the number of events you published on Kafka: Select a process instance, and next, select the tab Diagram . You should see something like:","title":"Testing the project"},{"location":"guided_exercises/05_bam_kafka/03_emitting-events/","text":"Emmitting Events from Business Processes In order to be able to finish processes based on new events, we will need to set up our environment. In this setup we will: Check the required configuration in the business project Add bpmn components to emit messages Emitting events in a business process Now, we need to End Message Events instead of two End Events . In Business Central, open the cc-limit-approval-app process. Convert the two End Events to End Message Events . It should look like this: Next, configure the Kafka topic name in the message name for both nodes as following: Raise Approved message name: requests-approved Raise Denied message name: requests-denied See below one of the nodes, the Raise Denied node configuration: Save the process definition. On the breadcrumb, click on \"cc-limit-approval-app-step1\" to go back to the Project Explorer view. Click on the \"Deploy\" button. Configuring the business application In Business Central, navigate to the Project Settings -> Deployments -> Work Item handlers : Observe that there is a task configured named Send Task . In BAMOE {{ version }} you need this configuration to be able to use any Message Events (ending and throwing) that would emit events. Consuming the events from Kafka topic using Kafka Consumer CLI In order to validate if our process is emitting processes as we expect, we need to listen to the Kafka topics requests-approved and requests-denied to validate if the messages were emitted correctly. Open a new terminal tab, and navigate to the Kafka project folder. cd ~/enablement/amq-examples/strimzi-all-in-one/ Start the Kafka command line tool that allows us to consume events that happen in a topic, and therefore, will allow us to know if BAMOE published the events when the process ended. The tool is kafka-console-consumer.sh . Let's check if the process emitted events on the topic requests-approved . docker-compose exec kafka bin/kafka-console-consumer.sh --topic requests-approved --from-beginning --bootstrap-server localhost:9092 Testing the solution To test the solution, we will start a new process instance that will start, be automatically approved, and end without any human interaction. A new process instance should get started whenever you publish a new event on the incoming-requests topic, and, when there is an automatic approval, the process will end and publish an event to the requests-approved topic. Let's see this in action: Like we did on the first lab, let's start a new process instance by publishing a message in the incoming-requests topic. If you canceled the execution of the kafka producer, here's how you can start it: docker-compose exec kafka bin/kafka-console-producer.sh --topic incoming-requests --bootstrap-server localhost:9092 You can use the following data in your event: {\"data\" : {\"customerId\": 1, \"customerScore\": 250, \"requestedValue\":1500}} When you hit return, the data is published to the incoming-requests topic Kafka reads the event from the incoming-requests and automatically instantiates a new process is with this data. Now check the terminal where you are consuming the messages in the requests-approved topic. You should see a new event published by your process. The event will look like this (though not on multiple lines): { \"specversion\" : \"1.0\" , \"time\" : \"2021-04-14T18:04:42.532-0300\" , \"id\" : \"25ba2dd0-a8d0-4cfc-9ba4-d2e556ffb4d0\" , \"type\" : \"empty\" , \"source\" : \"/process/cc-limit-approval-app.cc-limit-raise-approval/5\" , \"data\" : null } Identify the process ID on the event above. In this example, the process instance that emitted this event was process of ID 5 . Let's check this same process instance in Business Central. In Business Central, open the Menu -> Manage -> Process Instances . On the left column, filter by \"Completed\" State. You should see as many instances as the number of events you published on Kafka. Identify your process instance ID. In this example, instance with id 5 . Select the process instance. Next, select the tab Diagram . You should see something like:","title":"Emitting Events in Processes"},{"location":"guided_exercises/05_bam_kafka/03_emitting-events/#emmitting-events-from-business-processes","text":"In order to be able to finish processes based on new events, we will need to set up our environment. In this setup we will: Check the required configuration in the business project Add bpmn components to emit messages","title":"Emmitting Events from Business Processes"},{"location":"guided_exercises/05_bam_kafka/03_emitting-events/#emitting-events-in-a-business-process","text":"Now, we need to End Message Events instead of two End Events . In Business Central, open the cc-limit-approval-app process. Convert the two End Events to End Message Events . It should look like this: Next, configure the Kafka topic name in the message name for both nodes as following: Raise Approved message name: requests-approved Raise Denied message name: requests-denied See below one of the nodes, the Raise Denied node configuration: Save the process definition. On the breadcrumb, click on \"cc-limit-approval-app-step1\" to go back to the Project Explorer view. Click on the \"Deploy\" button.","title":"Emitting events in a business process"},{"location":"guided_exercises/05_bam_kafka/03_emitting-events/#configuring-the-business-application","text":"In Business Central, navigate to the Project Settings -> Deployments -> Work Item handlers : Observe that there is a task configured named Send Task . In BAMOE {{ version }} you need this configuration to be able to use any Message Events (ending and throwing) that would emit events.","title":"Configuring the business application"},{"location":"guided_exercises/05_bam_kafka/03_emitting-events/#consuming-the-events-from-kafka-topic-using-kafka-consumer-cli","text":"In order to validate if our process is emitting processes as we expect, we need to listen to the Kafka topics requests-approved and requests-denied to validate if the messages were emitted correctly. Open a new terminal tab, and navigate to the Kafka project folder. cd ~/enablement/amq-examples/strimzi-all-in-one/ Start the Kafka command line tool that allows us to consume events that happen in a topic, and therefore, will allow us to know if BAMOE published the events when the process ended. The tool is kafka-console-consumer.sh . Let's check if the process emitted events on the topic requests-approved . docker-compose exec kafka bin/kafka-console-consumer.sh --topic requests-approved --from-beginning --bootstrap-server localhost:9092","title":"Consuming the events from Kafka topic using Kafka Consumer CLI"},{"location":"guided_exercises/05_bam_kafka/03_emitting-events/#testing-the-solution","text":"To test the solution, we will start a new process instance that will start, be automatically approved, and end without any human interaction. A new process instance should get started whenever you publish a new event on the incoming-requests topic, and, when there is an automatic approval, the process will end and publish an event to the requests-approved topic. Let's see this in action: Like we did on the first lab, let's start a new process instance by publishing a message in the incoming-requests topic. If you canceled the execution of the kafka producer, here's how you can start it: docker-compose exec kafka bin/kafka-console-producer.sh --topic incoming-requests --bootstrap-server localhost:9092 You can use the following data in your event: {\"data\" : {\"customerId\": 1, \"customerScore\": 250, \"requestedValue\":1500}} When you hit return, the data is published to the incoming-requests topic Kafka reads the event from the incoming-requests and automatically instantiates a new process is with this data. Now check the terminal where you are consuming the messages in the requests-approved topic. You should see a new event published by your process. The event will look like this (though not on multiple lines): { \"specversion\" : \"1.0\" , \"time\" : \"2021-04-14T18:04:42.532-0300\" , \"id\" : \"25ba2dd0-a8d0-4cfc-9ba4-d2e556ffb4d0\" , \"type\" : \"empty\" , \"source\" : \"/process/cc-limit-approval-app.cc-limit-raise-approval/5\" , \"data\" : null } Identify the process ID on the event above. In this example, the process instance that emitted this event was process of ID 5 . Let's check this same process instance in Business Central. In Business Central, open the Menu -> Manage -> Process Instances . On the left column, filter by \"Completed\" State. You should see as many instances as the number of events you published on Kafka. Identify your process instance ID. In this example, instance with id 5 . Select the process instance. Next, select the tab Diagram . You should see something like:","title":"Testing the solution"},{"location":"guided_exercises/05_bam_kafka/04_auditing-events/","text":"Auditing with Kafka When using the Kafka extension in IBM Business Automation Open Edition 8.0, every transaction for processes, cases and tasks execution can be tracked via events. For each of these categories, we'll have an event emitted to a Kafka topic, in other words, we'll have three topics here: jbpm-processes-events , jbpm-tasks-events , jbpm-cases-events . To enable this feature, you need to add the jbpm-event-emitters-kafka library to the engine, KIE Server. This can either be downloaded in the community repository for jBPM or via the Red Hat customer portal: rhpam-7.10.0-maven-repository.zip . The maven repository have ~1.5GB. In order to facilitate the execution of this lab, you can download the jbpm-event-emmiters-kafka for BAMOE {{ version }} here ; FIGURE OUT THE REPLACEMENT Stop BAMOE. Download the jbpm-event-emitters-kafka . It's name will be similar to jbpm-event-emitters-kafka-7.67.2.Final-redhat-00017.jar . Since this is a behavior only needed by the engine, place the library inside the kie-server.war folder, inside the WEB-INF directory. TIP: If you downloaded the maven repository zip file in the Red Hat Customer Portal, you can find the jar inside the folder maven-repository/org/jbpm/jbpm-event-emitters-kafka/7.67.2.Final-redhat-00017/jbpm-event-emitters-kafka-7.67.2.Final-redhat-00017.jar cp jbpm-event-emitters-kafka-7.67.2.Final-redhat-00017.jar $JBOSS_EAP /standalone/deployments/kie-server.war/WEB-INF/lib/ Next,startBAMOE server. Let's check the auditing behavior. Testing the feature To check the auditing capabilities you can start new processes, interact with human tasks and track the events that are being published on the jbpm-tasks-events and jbpm-processes-events topics. The event tracking are active also for processes that doesn't use message events elements. In this example we will check the behavior for our event driven business application. Start a new process by emitting an event. Let's start a process that will not be automatically approved. In this way, we will also have a human task created. You can emit the following event to the incoming-requests topic: { \"customerId\" : 1 , \"customerScore\" : 100 , \"requestedValue\" : 1200 } You should be able to see a new process instance can be seen in Business Central in the following status: You can use the kafka consumer CLI script to check the messages that were emitted on the topics: jbpm-processes-events and jbpm-tasks-events . You should be able to see an event like this published on the jbpm-process-events : { \"specversion\" : \"1.0\" , \"time\" : \"2022-09-15T10:00:05.609-0300\" , \"id\" : \"28e13bc0-1c92-42fd-8909-b48a206325d3\" , \"type\" : \"process\" , \"source\" : \"/process/cc-limit-approval-app.cc-limit-raise-approval-with-end-events/2\" , \"data\" :{ \"compositeId\" : \"default-kieserver_2\" , \"id\" : 2 , \"processId\" : \"cc-limit-approval-app.cc-limit-raise-approval-with-end-events\" , \"processName\" : \"cc-limit-raise-approval-with-events\" , \"processVersion\" : \"1.0\" , \"state\" : 1 , \"containerId\" : \"cc-limit-approval-app_1.0.0-SNAPSHOT\" , \"initiator\" : \"unknown\" , \"date\" : \"2021-04-15T10:00:05.608-0300\" , \"processInstanceDescription\" : \"cc-limit-raise-approval-with-events\" , \"correlationKey\" : \"2\" , \"parentId\" : -1 , \"variables\" :{ \"request\" :{ \"customerId\" : 1 , \"requestedValue\" : 1200 , \"customerScore\" : 100 , \"denyReason\" : null }, \"approval\" : false , \"initiator\" : \"unknown\" }}} You should be able to see an event like this published on the jbpm-tasks-events : { \"specversion\" : \"1.0\" , \"time\" : \"2022-09-15T10:00:05.612-0300\" , \"id\" : \"2ac83d91-40d7-49f3-a114-2b72816a20a4\" , \"type\" : \"task\" , \"source\" : \"/process/cc-limit-approval-app.cc-limit-raise-approval-with-end-events/2\" , \"data\" :{ \"compositeId\" : \"default-kieserver_2\" , \"id\" : 2 , \"priority\" : 0 , \"name\" : \"Analyst validation\" , \"subject\" : \"\" , \"description\" : \"\" , \"taskType\" : null , \"formName\" : \"Task\" , \"status\" : \"Ready\" , \"actualOwner\" : null , \"createdBy\" : null , \"createdOn\" : \"2021-04-15T10:00:05.590-0300\" , \"activationTime\" : \"2021-04-15T10:00:05.590-0300\" , \"expirationDate\" : null , \"skipable\" : false , \"workItemId\" : 2 , \"processInstanceId\" : 2 , \"parentId\" : -1 , \"processId\" : \"cc-limit-approval-app.cc-limit-raise-approval-with-end-events\" , \"containerId\" : \"cc-limit-approval-app_1.0.0-SNAPSHOT\" , \"potentialOwners\" :[ \"kie-server\" ], \"excludedOwners\" :[], \"businessAdmins\" :[ \"Administrator\" , \"Administrators\" ], \"inputData\" :{ \"Skippable\" : \"false\" , \"request\" :{ \"customerId\" : 1 , \"requestedValue\" : 1200 , \"customerScore\" : 100 , \"denyReason\" : null }, \"TaskName\" : \"Task\" , \"NodeName\" : \"Analyst validation\" , \"GroupId\" : \"kie-server\" }, \"outputData\" : null }} Using Business Central, tnteract with the human task Analyst Validation , and check the events emitted on the jbpm-tasks-events . You should be able to see at every task change, a new event in the jbpm-tasks-events . Also, for every transaction commited for the process, you should see new events on the jbpm-process-events . By now, you have an event-driven process, that can be integrated within an event driven architecture, and furthermore, can be tracked and monitored in an asyncronous way by the usage of events. The complete project can be found at: https://github.com/timwuthenow/cc-limit-approval-app","title":"Auditing through events"},{"location":"guided_exercises/05_bam_kafka/04_auditing-events/#auditing-with-kafka","text":"When using the Kafka extension in IBM Business Automation Open Edition 8.0, every transaction for processes, cases and tasks execution can be tracked via events. For each of these categories, we'll have an event emitted to a Kafka topic, in other words, we'll have three topics here: jbpm-processes-events , jbpm-tasks-events , jbpm-cases-events . To enable this feature, you need to add the jbpm-event-emitters-kafka library to the engine, KIE Server. This can either be downloaded in the community repository for jBPM or via the Red Hat customer portal: rhpam-7.10.0-maven-repository.zip . The maven repository have ~1.5GB. In order to facilitate the execution of this lab, you can download the jbpm-event-emmiters-kafka for BAMOE {{ version }} here ; FIGURE OUT THE REPLACEMENT Stop BAMOE. Download the jbpm-event-emitters-kafka . It's name will be similar to jbpm-event-emitters-kafka-7.67.2.Final-redhat-00017.jar . Since this is a behavior only needed by the engine, place the library inside the kie-server.war folder, inside the WEB-INF directory. TIP: If you downloaded the maven repository zip file in the Red Hat Customer Portal, you can find the jar inside the folder maven-repository/org/jbpm/jbpm-event-emitters-kafka/7.67.2.Final-redhat-00017/jbpm-event-emitters-kafka-7.67.2.Final-redhat-00017.jar cp jbpm-event-emitters-kafka-7.67.2.Final-redhat-00017.jar $JBOSS_EAP /standalone/deployments/kie-server.war/WEB-INF/lib/ Next,startBAMOE server. Let's check the auditing behavior.","title":"Auditing with Kafka"},{"location":"guided_exercises/05_bam_kafka/04_auditing-events/#testing-the-feature","text":"To check the auditing capabilities you can start new processes, interact with human tasks and track the events that are being published on the jbpm-tasks-events and jbpm-processes-events topics. The event tracking are active also for processes that doesn't use message events elements. In this example we will check the behavior for our event driven business application. Start a new process by emitting an event. Let's start a process that will not be automatically approved. In this way, we will also have a human task created. You can emit the following event to the incoming-requests topic: { \"customerId\" : 1 , \"customerScore\" : 100 , \"requestedValue\" : 1200 } You should be able to see a new process instance can be seen in Business Central in the following status: You can use the kafka consumer CLI script to check the messages that were emitted on the topics: jbpm-processes-events and jbpm-tasks-events . You should be able to see an event like this published on the jbpm-process-events : { \"specversion\" : \"1.0\" , \"time\" : \"2022-09-15T10:00:05.609-0300\" , \"id\" : \"28e13bc0-1c92-42fd-8909-b48a206325d3\" , \"type\" : \"process\" , \"source\" : \"/process/cc-limit-approval-app.cc-limit-raise-approval-with-end-events/2\" , \"data\" :{ \"compositeId\" : \"default-kieserver_2\" , \"id\" : 2 , \"processId\" : \"cc-limit-approval-app.cc-limit-raise-approval-with-end-events\" , \"processName\" : \"cc-limit-raise-approval-with-events\" , \"processVersion\" : \"1.0\" , \"state\" : 1 , \"containerId\" : \"cc-limit-approval-app_1.0.0-SNAPSHOT\" , \"initiator\" : \"unknown\" , \"date\" : \"2021-04-15T10:00:05.608-0300\" , \"processInstanceDescription\" : \"cc-limit-raise-approval-with-events\" , \"correlationKey\" : \"2\" , \"parentId\" : -1 , \"variables\" :{ \"request\" :{ \"customerId\" : 1 , \"requestedValue\" : 1200 , \"customerScore\" : 100 , \"denyReason\" : null }, \"approval\" : false , \"initiator\" : \"unknown\" }}} You should be able to see an event like this published on the jbpm-tasks-events : { \"specversion\" : \"1.0\" , \"time\" : \"2022-09-15T10:00:05.612-0300\" , \"id\" : \"2ac83d91-40d7-49f3-a114-2b72816a20a4\" , \"type\" : \"task\" , \"source\" : \"/process/cc-limit-approval-app.cc-limit-raise-approval-with-end-events/2\" , \"data\" :{ \"compositeId\" : \"default-kieserver_2\" , \"id\" : 2 , \"priority\" : 0 , \"name\" : \"Analyst validation\" , \"subject\" : \"\" , \"description\" : \"\" , \"taskType\" : null , \"formName\" : \"Task\" , \"status\" : \"Ready\" , \"actualOwner\" : null , \"createdBy\" : null , \"createdOn\" : \"2021-04-15T10:00:05.590-0300\" , \"activationTime\" : \"2021-04-15T10:00:05.590-0300\" , \"expirationDate\" : null , \"skipable\" : false , \"workItemId\" : 2 , \"processInstanceId\" : 2 , \"parentId\" : -1 , \"processId\" : \"cc-limit-approval-app.cc-limit-raise-approval-with-end-events\" , \"containerId\" : \"cc-limit-approval-app_1.0.0-SNAPSHOT\" , \"potentialOwners\" :[ \"kie-server\" ], \"excludedOwners\" :[], \"businessAdmins\" :[ \"Administrator\" , \"Administrators\" ], \"inputData\" :{ \"Skippable\" : \"false\" , \"request\" :{ \"customerId\" : 1 , \"requestedValue\" : 1200 , \"customerScore\" : 100 , \"denyReason\" : null }, \"TaskName\" : \"Task\" , \"NodeName\" : \"Analyst validation\" , \"GroupId\" : \"kie-server\" }, \"outputData\" : null }} Using Business Central, tnteract with the human task Analyst Validation , and check the events emitted on the jbpm-tasks-events . You should be able to see at every task change, a new event in the jbpm-tasks-events . Also, for every transaction commited for the process, you should see new events on the jbpm-process-events . By now, you have an event-driven process, that can be integrated within an event driven architecture, and furthermore, can be tracked and monitored in an asyncronous way by the usage of events. The complete project can be found at: https://github.com/timwuthenow/cc-limit-approval-app","title":"Testing the feature"},{"location":"guided_exercises/05_techXchange_2023/introduction/","text":"IBM Business Automation Open Edition 9.2.x labs at IBM TechXchange 2024 Welcome to the Session 1717, at IBM TechXchange 2024. In this session you'll have a hands-on experience with cloud-native decision automation using IBM Business Automation Open Edition 9.2.x. As part of our learning journey, we'll explore many capabilities of BAMOE, along with having a better understanding of its architecture, how its components work together and ultimately, allows to create cloud-native decision services that can be deployed on cloud platforms such as Red Hat OpenShift Container Platform (OCP). To create our decision models, we'll use a new tool made available in 9.0, IBM Business Automation Manager Canvas, and we'll combine it with the developer joy brought by Quarkus, and how we can quickly deploy our automation solutions to a cloud environment. We will focus predominantly on the BAMOE 9.0 release, exploring how to automate a decision using Open Standards such as DMN, and running it with a Kogito-based Decision Service. Note If you want to go a step further, you'll also have available to you, labs focused on the 8.0.3 version of Open Editions, specially, on the Process Automation capabilities. Aligned with the evolution of the technology landscape and the future of this solution, the process automation capabilities are currently shifting towards a cloud-native architecture as we'll learn today. We hope to see you around in TechXchange 2024, to experience together all the exciting innovation we're building for this IBM's open source business automation solution! Now, let's get started! Create your first Decision Service using IBM Decision Manager Open Edition Intro to DMN Explore DMN Deeper Dive into DMN Go one step further (Optional) Explore process automation with IBM Business Automation Open Edition 8.0 Getting started with Process Event-driven processes More Exercises First Kogito Project Setup from Scratch Implement a On-push CI/CD","title":"Introduction"},{"location":"guided_exercises/05_techXchange_2023/introduction/#ibm-business-automation-open-edition-92x-labs-at-ibm-techxchange-2024","text":"Welcome to the Session 1717, at IBM TechXchange 2024. In this session you'll have a hands-on experience with cloud-native decision automation using IBM Business Automation Open Edition 9.2.x. As part of our learning journey, we'll explore many capabilities of BAMOE, along with having a better understanding of its architecture, how its components work together and ultimately, allows to create cloud-native decision services that can be deployed on cloud platforms such as Red Hat OpenShift Container Platform (OCP). To create our decision models, we'll use a new tool made available in 9.0, IBM Business Automation Manager Canvas, and we'll combine it with the developer joy brought by Quarkus, and how we can quickly deploy our automation solutions to a cloud environment. We will focus predominantly on the BAMOE 9.0 release, exploring how to automate a decision using Open Standards such as DMN, and running it with a Kogito-based Decision Service. Note If you want to go a step further, you'll also have available to you, labs focused on the 8.0.3 version of Open Editions, specially, on the Process Automation capabilities. Aligned with the evolution of the technology landscape and the future of this solution, the process automation capabilities are currently shifting towards a cloud-native architecture as we'll learn today. We hope to see you around in TechXchange 2024, to experience together all the exciting innovation we're building for this IBM's open source business automation solution! Now, let's get started!","title":"IBM Business Automation Open Edition 9.2.x labs at IBM TechXchange 2024"},{"location":"guided_exercises/05_techXchange_2023/introduction/#create-your-first-decision-service-using-ibm-decision-manager-open-edition","text":"Intro to DMN Explore DMN Deeper Dive into DMN","title":"Create your first Decision Service using IBM Decision Manager Open Edition"},{"location":"guided_exercises/05_techXchange_2023/introduction/#go-one-step-further-optional","text":"","title":"Go one step further (Optional)"},{"location":"guided_exercises/05_techXchange_2023/introduction/#explore-process-automation-with-ibm-business-automation-open-edition-80","text":"Getting started with Process Event-driven processes","title":"Explore process automation with IBM Business Automation Open Edition 8.0"},{"location":"guided_exercises/05_techXchange_2023/introduction/#more-exercises","text":"First Kogito Project Setup from Scratch Implement a On-push CI/CD","title":"More Exercises"},{"location":"guided_exercises/05_techXchange_2023/techxchange-lab/","text":"TESTING :markdown","title":"Techxchange lab"},{"location":"guided_exercises/05_techXchange_2023/001-canvas-lab/canvas/","text":"","title":"Canvas"},{"location":"guided_exercises/06_KIE_BC/getting-started-bc/","text":"Importing a DMN in Business Central If you want to try this in Business Central, you can go through the following steps, but it is not required. From the GitHub web page, click Clone or download on the right and then select Download ZIP : Using your favorite file system navigation tool, locate the downloaded ZIP file and unzip it to a directory in your file system. From this point forward, this location is referred to as $PROJECT_HOME . Log in to Business Central. You can use either {{ bamAdmin }}:{{ bamPass }} to do so or whatever login you have created on your instance. Create a project in Business Central called policy-price . In the empty project library view for the policy-price project, click Import Asset . In the Create new Uploaded file dialog, enter insurance-pricing.dmn in the Uploaded file field: Using the browse button at the far right of the field labeled Please select a file to upload , navigate with the file browser to the $PROJECT_HOME directory where the unzipped Git repository is located. Select the $PROJECT_HOME\\policy-price\\insurance-pricing.dmn file. Click Ok to import the DMN asset. The diagram will open and you will be able to see the DRD. Explore the diagram nodes to check the decision policies of this diagram. Close the diagram. You should now be on the library view for the policy-price project. You should see the insurance-pricing asset is added to your project assets: From the policy-price project\u2019s library view, click Build , then Deploy to deploy the project to the execution server. After receiving the build confirmation, navigate to the container deployment list by clicking the \" View deployment details \" link in the confirmation pop-up, or by selecting Menu \u2192 Deploy \u2192 Execution Servers . Verify that policy-price_2.0.0 shows a green status: Testing the Decision Service on KIE Server In this section, you test the DMN solution using the REST endpoints available in the Decision Server (a.k.a. KIE Server). Open your Decision Server (a.k.a KIE Server) on the url \"/docs\". You should see something like this: Next, under DMN Models , click on the POST /server/containers/{containerId}/dmn\" and select \"Try it out\": Now use the following data: Container ID: policy-price Body (dmn context): {\"dmn-context\": {\"Age\": 20, \"had previous incidents\": false}} Parameter content type: application/json Click on the execute button. You should see the server response 200 and the results of the decision. Try out the Decision with different values for the age and accident history, and compare the results with the decision table:","title":"Getting started bc"},{"location":"guided_exercises/06_KIE_BC/getting-started-bc/#importing-a-dmn-in-business-central","text":"If you want to try this in Business Central, you can go through the following steps, but it is not required. From the GitHub web page, click Clone or download on the right and then select Download ZIP : Using your favorite file system navigation tool, locate the downloaded ZIP file and unzip it to a directory in your file system. From this point forward, this location is referred to as $PROJECT_HOME . Log in to Business Central. You can use either {{ bamAdmin }}:{{ bamPass }} to do so or whatever login you have created on your instance. Create a project in Business Central called policy-price . In the empty project library view for the policy-price project, click Import Asset . In the Create new Uploaded file dialog, enter insurance-pricing.dmn in the Uploaded file field: Using the browse button at the far right of the field labeled Please select a file to upload , navigate with the file browser to the $PROJECT_HOME directory where the unzipped Git repository is located. Select the $PROJECT_HOME\\policy-price\\insurance-pricing.dmn file. Click Ok to import the DMN asset. The diagram will open and you will be able to see the DRD. Explore the diagram nodes to check the decision policies of this diagram. Close the diagram. You should now be on the library view for the policy-price project. You should see the insurance-pricing asset is added to your project assets: From the policy-price project\u2019s library view, click Build , then Deploy to deploy the project to the execution server. After receiving the build confirmation, navigate to the container deployment list by clicking the \" View deployment details \" link in the confirmation pop-up, or by selecting Menu \u2192 Deploy \u2192 Execution Servers . Verify that policy-price_2.0.0 shows a green status:","title":"Importing a DMN in Business Central"},{"location":"guided_exercises/06_KIE_BC/getting-started-bc/#testing-the-decision-service-on-kie-server","text":"In this section, you test the DMN solution using the REST endpoints available in the Decision Server (a.k.a. KIE Server). Open your Decision Server (a.k.a KIE Server) on the url \"/docs\". You should see something like this: Next, under DMN Models , click on the POST /server/containers/{containerId}/dmn\" and select \"Try it out\": Now use the following data: Container ID: policy-price Body (dmn context): {\"dmn-context\": {\"Age\": 20, \"had previous incidents\": false}} Parameter content type: application/json Click on the execute button. You should see the server response 200 and the results of the decision. Try out the Decision with different values for the age and accident history, and compare the results with the decision table:","title":"Testing the Decision Service on KIE Server"},{"location":"guided_exercises/operator/configmaps-deleteapp/","text":"ConfigMaps The Operator stores its configuration in a number of ConfigurationMaps . These ConfigurationMaps can be used to change more advanced configurations that can not be configured in the KieApp YAML. In the case the Operator upgrades the version of your BAMOE environment, the Operator is aware that one of the ConfigMaps has changed and will make a backup of it during the upgrade. Viewing and editing ConfigMaps A powerful feature of OpenShift are ConfigMaps which provide mechanisms to inject containers with configuration data while keeping containers agnostic of OpenShift Container Platform. A ConfigMap can be used to store fine-grained information like individual properties or coarse-grained information like entire configuration files or JSON blobs. In this section, we will modify some of the default health properties that are different than the defaults provided with BAMOE and we want them to roll out to every container that gets created with the operator. In the OpenShift Console, open Workloads \u2192 Config Maps . Note that the Operator keeps the current ConfigMaps, and the ones of the last 2 versions. Click on the kieconfigs-7.10.1 ConfigMap and open the YAML tab. Explore the configuration options. Set the initialDelaySeconds of the livenessProbe of the Business Central console from 180 to 240. Click the Save button to save the configuration. Go to \"Workloads \u2192 Deployment Configs\", open the rhpam-trial-rhpamcentr Deployment Config and open the YAML tab. Find the LivenessProbe initialDelaySeconds configuration and notice that it\u2019s still set to 180. Delete the DeploymentConfig. This will have the Operator reconciliation recreate the DC. Open the YAML configuation of this recreated DeploymentConfig. Find the LivenessProbe initialDelaySeconds configuration and note that this time it has been set to 240, the value set in the ConfigMap. Deleting an application Apart from provisioning an BAMOE application, the Operator also allows us to easily delete an application. Navigate to Operators \u2192 Installed Operators \u2192 Business Automation \u2192 KieApp . Click on the kebab icon of the rhpam-trial KieApp and click Delete . Navigate back to Workloads \u2192 Deployment Configs and note that the BAMOE Deployment Configs have been removed.","title":"ConfigMaps and Deleteting Projects"},{"location":"guided_exercises/operator/configmaps-deleteapp/#configmaps","text":"The Operator stores its configuration in a number of ConfigurationMaps . These ConfigurationMaps can be used to change more advanced configurations that can not be configured in the KieApp YAML. In the case the Operator upgrades the version of your BAMOE environment, the Operator is aware that one of the ConfigMaps has changed and will make a backup of it during the upgrade.","title":"ConfigMaps"},{"location":"guided_exercises/operator/configmaps-deleteapp/#viewing-and-editing-configmaps","text":"A powerful feature of OpenShift are ConfigMaps which provide mechanisms to inject containers with configuration data while keeping containers agnostic of OpenShift Container Platform. A ConfigMap can be used to store fine-grained information like individual properties or coarse-grained information like entire configuration files or JSON blobs. In this section, we will modify some of the default health properties that are different than the defaults provided with BAMOE and we want them to roll out to every container that gets created with the operator. In the OpenShift Console, open Workloads \u2192 Config Maps . Note that the Operator keeps the current ConfigMaps, and the ones of the last 2 versions. Click on the kieconfigs-7.10.1 ConfigMap and open the YAML tab. Explore the configuration options. Set the initialDelaySeconds of the livenessProbe of the Business Central console from 180 to 240. Click the Save button to save the configuration. Go to \"Workloads \u2192 Deployment Configs\", open the rhpam-trial-rhpamcentr Deployment Config and open the YAML tab. Find the LivenessProbe initialDelaySeconds configuration and notice that it\u2019s still set to 180. Delete the DeploymentConfig. This will have the Operator reconciliation recreate the DC. Open the YAML configuation of this recreated DeploymentConfig. Find the LivenessProbe initialDelaySeconds configuration and note that this time it has been set to 240, the value set in the ConfigMap.","title":"Viewing and editing ConfigMaps"},{"location":"guided_exercises/operator/configmaps-deleteapp/#deleting-an-application","text":"Apart from provisioning an BAMOE application, the Operator also allows us to easily delete an application. Navigate to Operators \u2192 Installed Operators \u2192 Business Automation \u2192 KieApp . Click on the kebab icon of the rhpam-trial KieApp and click Delete . Navigate back to Workloads \u2192 Deployment Configs and note that the BAMOE Deployment Configs have been removed.","title":"Deleting an application"},{"location":"guided_exercises/operator/configuration/","text":"KIE App Configuration The definition of the expected state of KIE-App environment is defined in the YAML definition the KIE-App. In this section we will slightly change this configuration to see how the Operator applies changes in the configuration of your IBM Business Automation Open Edition 9.2.x environment. Changing Credentials Go back to the YAML definition of your rhpam-trial KieApp. Add a commonConfig section, with the adminUser to the value bamAdmin , and the adminPassword to ibmpam1! . Click on the Save button. spec : commonConfig : adminPassword : bamAdmin adminUser : ibmpam1! Click the Reload button to reload the YAML view. Click on the Overview tab. Notice the deployments re-deploying. Click on the Business/Central Central URL to open the Business Central console. Log in with the new username and password: bamAdmin / ibmpam1! . Adding a KIE-Server Apart from changing some configuration parameters, we can also change the topology our deployment in the KieApp YAML file. Go back to the YAML definition of your rhpam-trial KieApp. Add a servers section and set the replicas parameter of the rhpam-trial-kieserver to 2 . objects : servers : - deployments : 1 name : rhpam-trial-kieserver replicas : 2 Click the Save button. Go to Workloads \u2192 Deployment Configs . Note that there are now 2 KIE-Server Deployment Configs. Go back to the YAML definition of your rhpam-trial KieApp. Navigate to the servers section and add the property deployments with the value 2 . objects : servers : - deployments : 2 name : rhpam-trial-kieserver replicas : 2 Click the Save button.","title":"Kie App Configuration"},{"location":"guided_exercises/operator/configuration/#kie-app-configuration","text":"The definition of the expected state of KIE-App environment is defined in the YAML definition the KIE-App. In this section we will slightly change this configuration to see how the Operator applies changes in the configuration of your IBM Business Automation Open Edition 9.2.x environment.","title":"KIE App Configuration"},{"location":"guided_exercises/operator/configuration/#changing-credentials","text":"Go back to the YAML definition of your rhpam-trial KieApp. Add a commonConfig section, with the adminUser to the value bamAdmin , and the adminPassword to ibmpam1! . Click on the Save button. spec : commonConfig : adminPassword : bamAdmin adminUser : ibmpam1! Click the Reload button to reload the YAML view. Click on the Overview tab. Notice the deployments re-deploying. Click on the Business/Central Central URL to open the Business Central console. Log in with the new username and password: bamAdmin / ibmpam1! .","title":"Changing Credentials"},{"location":"guided_exercises/operator/configuration/#adding-a-kie-server","text":"Apart from changing some configuration parameters, we can also change the topology our deployment in the KieApp YAML file. Go back to the YAML definition of your rhpam-trial KieApp. Add a servers section and set the replicas parameter of the rhpam-trial-kieserver to 2 . objects : servers : - deployments : 1 name : rhpam-trial-kieserver replicas : 2 Click the Save button. Go to Workloads \u2192 Deployment Configs . Note that there are now 2 KIE-Server Deployment Configs. Go back to the YAML definition of your rhpam-trial KieApp. Navigate to the servers section and add the property deployments with the value 2 . objects : servers : - deployments : 2 name : rhpam-trial-kieserver replicas : 2 Click the Save button.","title":"Adding a KIE-Server"},{"location":"guided_exercises/operator/installer/","text":"Operator Wizard Walkthrough The Business Automation Operator contains an Operator Installer Console . This console gives you a wizard experience to deploy IBM Business Automation Open Edition 9.2.x environments. Wizard walk through Go the Business Automation Operator and click in Installer link. Login with Openshift. A page will show up asking for authorization. Select all options and click on \"Allow selected permissions\". Give the application the name my-ibamoe-authoring . Select the rhpam-authoring for the Enviroment. Check the Enable Upgrades checkbox. Scroll down and set the Username and Password to bamAdmin : ibmpam1 . Click the Next button. Don\u2019t change any values in the Security section. Click on Next . Go through the Components section of the installer and observe the possible options. Don\u2019t change any values for now. Keep clicking next until you reach the Confirmation screen and click Deploy . Go back to the OpenShift Console. Navigate to Workloads \u2192 Deployment Configs and observe that a new IBM Business Automation Open Edition 9.2.x production environment has been deployed. Note that this environment has a PostgreSQL database deployed. Also note that both the Business Central and KIE-Server Deployment Configs have their ReplicationController set to 3 pods. Go back to the Operators \u2192 Installed Operators \u2192 Business Automation \u2192 KieApp . Delete the my-rhpam-authoring we\u2019ve just deployed with the Installer. How to KIE Server with more replicas We will now deploy a new production environment using the installer, but this time we will configure our KIE-Server in the wizard and set the replications of the KIE-Server to 2 instead of 3. Go back to the Business Automation Operator, and open the Wizard. Create a new BAMOE Production Environment. Continue until you reach the KIE Servers screen. Click Add new KIE Server and use the following configuration for your KIE-Server. Click through the rest of the screens until you can press the Deploy button to deploy the environment. Navigate to the Workloads \u2192 Deployment Configs screen to see your BAMOE production environment, including the KIE-Server you configured. Conclusion This concludes the lab on the Business Automation Operator. If you have time left, feel free to explore more features of the operator.","title":"Operator Wizard"},{"location":"guided_exercises/operator/installer/#operator-wizard-walkthrough","text":"The Business Automation Operator contains an Operator Installer Console . This console gives you a wizard experience to deploy IBM Business Automation Open Edition 9.2.x environments.","title":"Operator Wizard Walkthrough"},{"location":"guided_exercises/operator/installer/#wizard-walk-through","text":"Go the Business Automation Operator and click in Installer link. Login with Openshift. A page will show up asking for authorization. Select all options and click on \"Allow selected permissions\". Give the application the name my-ibamoe-authoring . Select the rhpam-authoring for the Enviroment. Check the Enable Upgrades checkbox. Scroll down and set the Username and Password to bamAdmin : ibmpam1 . Click the Next button. Don\u2019t change any values in the Security section. Click on Next . Go through the Components section of the installer and observe the possible options. Don\u2019t change any values for now. Keep clicking next until you reach the Confirmation screen and click Deploy . Go back to the OpenShift Console. Navigate to Workloads \u2192 Deployment Configs and observe that a new IBM Business Automation Open Edition 9.2.x production environment has been deployed. Note that this environment has a PostgreSQL database deployed. Also note that both the Business Central and KIE-Server Deployment Configs have their ReplicationController set to 3 pods. Go back to the Operators \u2192 Installed Operators \u2192 Business Automation \u2192 KieApp . Delete the my-rhpam-authoring we\u2019ve just deployed with the Installer.","title":"Wizard walk through"},{"location":"guided_exercises/operator/installer/#how-to-kie-server-with-more-replicas","text":"We will now deploy a new production environment using the installer, but this time we will configure our KIE-Server in the wizard and set the replications of the KIE-Server to 2 instead of 3. Go back to the Business Automation Operator, and open the Wizard. Create a new BAMOE Production Environment. Continue until you reach the KIE Servers screen. Click Add new KIE Server and use the following configuration for your KIE-Server. Click through the rest of the screens until you can press the Deploy button to deploy the environment. Navigate to the Workloads \u2192 Deployment Configs screen to see your BAMOE production environment, including the KIE-Server you configured.","title":"How to KIE Server with more replicas"},{"location":"guided_exercises/operator/installer/#conclusion","text":"This concludes the lab on the Business Automation Operator. If you have time left, feel free to explore more features of the operator.","title":"Conclusion"},{"location":"guided_exercises/operator/introduction/","text":"Red Hat PAM Operator on OpenShift 4 In this lab we will use the enhanced Business Automation Operator 7.10+ to deploy a number of IBM Business Automation Open Edition 9.2.x environments on OpenShift 4. Goal Install the Business Automation Operator on OCP 4. Use the Business Automation Operator 8.0.7 to deploy a number of IBM Business Automation Open Edition 9.2.x environments. Change the KIE-App deployment CRDs to show reconciliation. Change Operator ConfigMaps to make advanced configuration changes to the KIE-App. Problem Statement In this lab, the goal is to provision and manage various IBM Business Automation Open Edition 9.2.x architectures using the Business Automation Operator on OpenShift 4. We deploy an BAMOE Trial environment, which is a basic ephemeral environment that does not require any form of storage (e.g. persistent volume, database). We explore Operator reconciliation features by removing provisioned resources like Services and Deployment Configs. We alter the deployment through the Operator to show how the provisioned environment changes. We change a KIE configuration parameter in the Business Automation Operator ConfigMap to demonstrate advanced configuration changes. We provision a more sophisticated Production environment, to show creation of PVCs, deployment of databases and integration with BAMOE Smart Router. We use the Operator Installer console to install a new KIE-App deployment. First steps If you are using your own OpenShift environment , follow the steps below to create a project and install the operator. If you are trying this lab in an environment provisioned by the Red Hat team, skip to the section Inspect the Lab environment . Create a new project in OpenShift. We suggest the name ibamoe-install . Navigate to Operators , Operator Hub , and search for Business Automation : Click on the Business Automation and then, click Install . You can select the following options, and click on Submit : Once subscribed, you should wait for the operator to get provisioned. Then you can proceed with the lab.","title":"Using the Business Automation Operator"},{"location":"guided_exercises/operator/introduction/#red-hat-pam-operator-on-openshift-4","text":"In this lab we will use the enhanced Business Automation Operator 7.10+ to deploy a number of IBM Business Automation Open Edition 9.2.x environments on OpenShift 4.","title":"Red Hat PAM Operator on OpenShift 4"},{"location":"guided_exercises/operator/introduction/#goal","text":"Install the Business Automation Operator on OCP 4. Use the Business Automation Operator 8.0.7 to deploy a number of IBM Business Automation Open Edition 9.2.x environments. Change the KIE-App deployment CRDs to show reconciliation. Change Operator ConfigMaps to make advanced configuration changes to the KIE-App.","title":"Goal"},{"location":"guided_exercises/operator/introduction/#problem-statement","text":"In this lab, the goal is to provision and manage various IBM Business Automation Open Edition 9.2.x architectures using the Business Automation Operator on OpenShift 4. We deploy an BAMOE Trial environment, which is a basic ephemeral environment that does not require any form of storage (e.g. persistent volume, database). We explore Operator reconciliation features by removing provisioned resources like Services and Deployment Configs. We alter the deployment through the Operator to show how the provisioned environment changes. We change a KIE configuration parameter in the Business Automation Operator ConfigMap to demonstrate advanced configuration changes. We provision a more sophisticated Production environment, to show creation of PVCs, deployment of databases and integration with BAMOE Smart Router. We use the Operator Installer console to install a new KIE-App deployment.","title":"Problem Statement"},{"location":"guided_exercises/operator/introduction/#first-steps","text":"If you are using your own OpenShift environment , follow the steps below to create a project and install the operator. If you are trying this lab in an environment provisioned by the Red Hat team, skip to the section Inspect the Lab environment . Create a new project in OpenShift. We suggest the name ibamoe-install . Navigate to Operators , Operator Hub , and search for Business Automation : Click on the Business Automation and then, click Install . You can select the following options, and click on Submit : Once subscribed, you should wait for the operator to get provisioned. Then you can proceed with the lab.","title":"First steps"},{"location":"guided_exercises/operator/reconciliation/","text":"Reconciliation The OpenShift Operators provide functionality to reconciliate an existing environment in order to bring it back to its expected state. We will now test this feature by removing one of the required resources from our deployment. Open the Resources tab. This will show all the resources of the application deployed and managed by the Operator. On the fourth row, we can see the rhpam-trial-kieserver Service resource. In the left menu, go to Networking \u2192 Services . Open rhpam-trial-kieserver . Delete the Service by clicking on the Actions button at the upper right of the screen and clicking on Delete . Notice the Service disappearing and immediately reappearing. This is the Operators reconciliation logic at work, bringing the environment back in its expected state.","title":"Automatic reconciliation"},{"location":"guided_exercises/operator/reconciliation/#reconciliation","text":"The OpenShift Operators provide functionality to reconciliate an existing environment in order to bring it back to its expected state. We will now test this feature by removing one of the required resources from our deployment. Open the Resources tab. This will show all the resources of the application deployed and managed by the Operator. On the fourth row, we can see the rhpam-trial-kieserver Service resource. In the left menu, go to Networking \u2192 Services . Open rhpam-trial-kieserver . Delete the Service by clicking on the Actions button at the upper right of the screen and clicking on Delete . Notice the Service disappearing and immediately reappearing. This is the Operators reconciliation logic at work, bringing the environment back in its expected state.","title":"Reconciliation"},{"location":"guided_exercises/operator/trial-environment/","text":"Deploying an BAMOE Trial Environment From the Business Automation page in your OpenShift Console, open the KieApp tab and click on Create KieApp . A form will be displayed for you to choose which instalation option you want to have. notice the environment field. In this field we define the type of the environment we want to provision. In this case we want to provision the Trial environment, so we accept the default values. TIP: You also have the YAML definition option if you want to do customizations that are not available in the form above. Click on the Create button at the bottom of the page. In the KieApp tab, we can see our new rhpam-trial environment being listed. Expand the Workloads menu on the left side of the screen. Click on Deployment Configs . Observe that the Operator has created 2 Deployment Configs, one for Business Central and one for KIE-Server. Open the Developer Console by clicking on the link in the dropdown box at the top left of the screen. Click on the Topology link to show a graphical representation of the topology of our namespace, which includes an Operator DC, a Business Central DC, and a KIE-Server DC. Go back to the Adminstrator console. Open Networking \u2192 Routes menu to see all the available routes to our KIE application deployed in this namespace. Identify the Business/Decision Central URL link to navigate to the BAMOE Business Central workbench. It should be named rhpam-trial-rhpamcentr-http for the http option, or rhpam-trial-rhpamcentr for https. As the Operator is responsible for deployment and configuration of the BAMOE environment, we can find the details if this deployment in the KieApp instance details screen. Open your KieApp in Operators \u2192 Installed Operators \u2192 Business Automation \u2192 KieApp \u2192 rhpam-trial , and click on the YAML tab. We can see in the YAML description that the adminPassword has been set to RedHat . Navigate back to the Business Central workbench and login with u: adminUser p: RedHat . Explore the Business Central application. In particular, go to Menu \u2192 Deploy \u2192 Execution Servers to see the Execution Server connected to the workbench.","title":"Deploying a trial environment"},{"location":"guided_exercises/operator/trial-environment/#deploying-an-bamoe-trial-environment","text":"From the Business Automation page in your OpenShift Console, open the KieApp tab and click on Create KieApp . A form will be displayed for you to choose which instalation option you want to have. notice the environment field. In this field we define the type of the environment we want to provision. In this case we want to provision the Trial environment, so we accept the default values. TIP: You also have the YAML definition option if you want to do customizations that are not available in the form above. Click on the Create button at the bottom of the page. In the KieApp tab, we can see our new rhpam-trial environment being listed. Expand the Workloads menu on the left side of the screen. Click on Deployment Configs . Observe that the Operator has created 2 Deployment Configs, one for Business Central and one for KIE-Server. Open the Developer Console by clicking on the link in the dropdown box at the top left of the screen. Click on the Topology link to show a graphical representation of the topology of our namespace, which includes an Operator DC, a Business Central DC, and a KIE-Server DC. Go back to the Adminstrator console. Open Networking \u2192 Routes menu to see all the available routes to our KIE application deployed in this namespace. Identify the Business/Decision Central URL link to navigate to the BAMOE Business Central workbench. It should be named rhpam-trial-rhpamcentr-http for the http option, or rhpam-trial-rhpamcentr for https. As the Operator is responsible for deployment and configuration of the BAMOE environment, we can find the details if this deployment in the KieApp instance details screen. Open your KieApp in Operators \u2192 Installed Operators \u2192 Business Automation \u2192 KieApp \u2192 rhpam-trial , and click on the YAML tab. We can see in the YAML description that the adminPassword has been set to RedHat . Navigate back to the Business Central workbench and login with u: adminUser p: RedHat . Explore the Business Central application. In particular, go to Menu \u2192 Deploy \u2192 Execution Servers to see the Execution Server connected to the workbench.","title":"Deploying an BAMOE Trial Environment"},{"location":"guided_exercises/operator/version-upgrade/","text":"Version Upgrades The Operator of BAMOE is also capable of doing both patch and minor upgrades. This means that, for example, the Operation can upgrade an BAMOE environment from 7.10.0 to 7.10.1 or from 7.10.1 to 7.11.0. When creating a new KieApp, you can find the option to enable the version updates. If both the Enable Upgrades and Include minor version upgrades settings are set to true, the KieApp YAML configuration will include the following spec: With this configuration the version upgrade mechanism of the Operator should be enabled for the given KieApp.","title":"Versions and the Operator - Starting Processes with Events"},{"location":"guided_exercises/operator/version-upgrade/#version-upgrades","text":"The Operator of BAMOE is also capable of doing both patch and minor upgrades. This means that, for example, the Operation can upgrade an BAMOE environment from 7.10.0 to 7.10.1 or from 7.10.1 to 7.11.0. When creating a new KieApp, you can find the option to enable the version updates. If both the Enable Upgrades and Include minor version upgrades settings are set to true, the KieApp YAML configuration will include the following spec: With this configuration the version upgrade mechanism of the Operator should be enabled for the given KieApp.","title":"Version Upgrades"},{"location":"guided_exercises/operator/support/readme/","text":"To provision this guide in ocp: 1. Login to the cluster on your terminal 2. run oc create -f ocp-provisioning.yml","title":"Readme"},{"location":"guided_exercises/tools/authoring-decisions/","text":"Authoring a Decision Let's author a simple decision and test it. The use case we'll try out is the automation of a repeated decision for requests approval. Create a new Decision In the project we've just created: Select the folder where you want to create the new file. Click on resources . Next, click on the new folder icon: Name the file automated-request-approval.dmn and press enter. The file should open in the DMN Editor. Create the following DMN: This DRD contains: A decision node Approval of type boolean ; Two inputs: A request type , which is string A request price , that is a number . Implement the following decision table, in the decision node: Save the diagram Testing the decision Before deploying the decision service in KIE Server, let's do some unit testing using the Test Scenario Simulation tooling. Configuring the project In order to use test scenarios, you need to add at least three dependencies to your project: junit:junit org.drools:drools-scenario-simulation-backend org.drools:drools-scenario-simulation-api Open the pom.xml file and add the following dependencies: <dependencies> <dependency> <groupId>org.drools</groupId> <artifactId>drools-scenario-simulation-api</artifactId> <version>${version.org.kie}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.drools</groupId> <artifactId>drools-scenario-simulation-backend</artifactId> <version>${version.org.kie}</version> <scope>test</scope> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.12</version> <scope>test</scope> </dependency> </dependencies> Adding the JUnit Runner In this scenario, the version.org.kie should be compatible with the product version you want to use. In this scenario, we are using BAMOE 7.10, which would be <version.org.kie>7.67.2.Final-redhat-00017</version.org.kie> . Create a new folder testscenario: /src/test/java/testscenario In the folder you just created, add a file and name it ScenarioJunitActivatorTest.java In this class, you should add a the Scenario Activator. This class allows the test scenarios to run along with the junit tests. package testscenario; /** * Do not remove this file */ @org.junit.runner.RunWith(org.drools.scenariosimulation.backend.runner.ScenarioJunitActivator.class) public class ScenarioJunitActivatorTest { } It should look like this: Creating the test scenario On the folder src/test/resources/org/kie/businessapp create a new file named ValidateAutomaticDecision.scesim . The editor should open up with the option to choose the Source type . This is the type of rule you want to test. Select DMN, next, choose your DMN file and click on the create button. The tool will already bring the inputs and expected result columns based on your DMN. Now, implement the following test: Runing the tests You can run the tests in two ways: using maven or the JUnit activator class. To run the test with maven you can for example run: mvn test If you want to run the tests using the activator class: Right click the ScenarioJunitActivatorTest.java file and select Run Java : The execution results should show up: Try changing the line one expected result from true to false . Click on the re-run button to see the results. Finally, adjust the tests and make sure that your project can compile when you run: mvn clean install Next Steps Now it's time to deploy our project to KIE Server and test it out.","title":"Authoring and testing decisions"},{"location":"guided_exercises/tools/authoring-decisions/#authoring-a-decision","text":"Let's author a simple decision and test it. The use case we'll try out is the automation of a repeated decision for requests approval.","title":"Authoring a Decision"},{"location":"guided_exercises/tools/authoring-decisions/#create-a-new-decision","text":"In the project we've just created: Select the folder where you want to create the new file. Click on resources . Next, click on the new folder icon: Name the file automated-request-approval.dmn and press enter. The file should open in the DMN Editor. Create the following DMN: This DRD contains: A decision node Approval of type boolean ; Two inputs: A request type , which is string A request price , that is a number . Implement the following decision table, in the decision node: Save the diagram","title":"Create a new Decision"},{"location":"guided_exercises/tools/authoring-decisions/#testing-the-decision","text":"Before deploying the decision service in KIE Server, let's do some unit testing using the Test Scenario Simulation tooling.","title":"Testing the decision"},{"location":"guided_exercises/tools/authoring-decisions/#configuring-the-project","text":"In order to use test scenarios, you need to add at least three dependencies to your project: junit:junit org.drools:drools-scenario-simulation-backend org.drools:drools-scenario-simulation-api Open the pom.xml file and add the following dependencies: <dependencies> <dependency> <groupId>org.drools</groupId> <artifactId>drools-scenario-simulation-api</artifactId> <version>${version.org.kie}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.drools</groupId> <artifactId>drools-scenario-simulation-backend</artifactId> <version>${version.org.kie}</version> <scope>test</scope> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.12</version> <scope>test</scope> </dependency> </dependencies>","title":"Configuring the project"},{"location":"guided_exercises/tools/authoring-decisions/#adding-the-junit-runner","text":"In this scenario, the version.org.kie should be compatible with the product version you want to use. In this scenario, we are using BAMOE 7.10, which would be <version.org.kie>7.67.2.Final-redhat-00017</version.org.kie> . Create a new folder testscenario: /src/test/java/testscenario In the folder you just created, add a file and name it ScenarioJunitActivatorTest.java In this class, you should add a the Scenario Activator. This class allows the test scenarios to run along with the junit tests. package testscenario; /** * Do not remove this file */ @org.junit.runner.RunWith(org.drools.scenariosimulation.backend.runner.ScenarioJunitActivator.class) public class ScenarioJunitActivatorTest { } It should look like this:","title":"Adding the JUnit Runner"},{"location":"guided_exercises/tools/authoring-decisions/#creating-the-test-scenario","text":"On the folder src/test/resources/org/kie/businessapp create a new file named ValidateAutomaticDecision.scesim . The editor should open up with the option to choose the Source type . This is the type of rule you want to test. Select DMN, next, choose your DMN file and click on the create button. The tool will already bring the inputs and expected result columns based on your DMN. Now, implement the following test:","title":"Creating the test scenario"},{"location":"guided_exercises/tools/authoring-decisions/#runing-the-tests","text":"You can run the tests in two ways: using maven or the JUnit activator class. To run the test with maven you can for example run: mvn test If you want to run the tests using the activator class: Right click the ScenarioJunitActivatorTest.java file and select Run Java : The execution results should show up: Try changing the line one expected result from true to false . Click on the re-run button to see the results. Finally, adjust the tests and make sure that your project can compile when you run: mvn clean install","title":"Runing the tests"},{"location":"guided_exercises/tools/authoring-decisions/#next-steps","text":"Now it's time to deploy our project to KIE Server and test it out.","title":"Next Steps"},{"location":"guided_exercises/tools/deploying-project/","text":"Deploying the project in KIE Server It's time to deploy our business application in KIE Server. Deployment We can deploy the project directly in KIE Server without the need to use Business Central. To do so, we can use the available REST API. Open KIE Server REST API. (i.e. http://localhost:8080/kie-server/docs) Under \u201cKIE Server and KIE container\" category select the following: PUT /server/containers/{containerId} Creates a new KIE container in the KIE Server with a specified KIE container ID Click on \"Try it out\" Insert your project details. The GAV can be found for example, in your pom.xml . See an example: containerId : mybusinessapp body : {\"container-id\" : \"mybusinessapp\", \"release-id\" : { \"group-id\" : \"org.kie.businessapp\", \"artifact-id\" : \"mybusinessapp\", \"version\" : \"1.0\" } } Click on the blue button \"Execute\". You should get a 201 result as follows: Testing the Automated Approval Decision Now, using the KIE server REST API, we'll consume the decision we've just deployed. Under the section DMN Models locate: POST /server/containers/{containerId}/dmn Evaluates decisions for given input Click on try it out Use the following data: ContainerID : mybusinessapp Body: { \"dmn-context\": { \"request type\": \"urgent\", \"request price\": \"250\" } } Extra Lab: Business Central Finally, you can import this project in Business Central. In order to do so, this needs to be a git-based project and Business Central needs to have access to the git repository where the project is stored. The following steps consider a local environment scenario. Access your application folder in the terminal. Initialize the git repository and do the first commit git init git add -A git commit -m \"first commit\" With this you can already import the project in Business Central. Open Business Central and select the import the project option. In the pop-up, in the Repository URL field, you should insert the git repository. If it is on your local machine you can inform something like: /$PROJECT_DIR/tooling-labs/mybusinessapp . Confirm the operation. You should see the project. Select it and click the Ok button. Feel free to explore the project and validate the test scenario and deployment through Business Central.","title":"Deploying and consuming services"},{"location":"guided_exercises/tools/deploying-project/#deploying-the-project-in-kie-server","text":"It's time to deploy our business application in KIE Server.","title":"Deploying the project in KIE Server"},{"location":"guided_exercises/tools/deploying-project/#deployment","text":"We can deploy the project directly in KIE Server without the need to use Business Central. To do so, we can use the available REST API. Open KIE Server REST API. (i.e. http://localhost:8080/kie-server/docs) Under \u201cKIE Server and KIE container\" category select the following: PUT /server/containers/{containerId} Creates a new KIE container in the KIE Server with a specified KIE container ID Click on \"Try it out\" Insert your project details. The GAV can be found for example, in your pom.xml . See an example: containerId : mybusinessapp body : {\"container-id\" : \"mybusinessapp\", \"release-id\" : { \"group-id\" : \"org.kie.businessapp\", \"artifact-id\" : \"mybusinessapp\", \"version\" : \"1.0\" } } Click on the blue button \"Execute\". You should get a 201 result as follows:","title":"Deployment"},{"location":"guided_exercises/tools/deploying-project/#testing-the-automated-approval-decision","text":"Now, using the KIE server REST API, we'll consume the decision we've just deployed. Under the section DMN Models locate: POST /server/containers/{containerId}/dmn Evaluates decisions for given input Click on try it out Use the following data: ContainerID : mybusinessapp Body: { \"dmn-context\": { \"request type\": \"urgent\", \"request price\": \"250\" } }","title":"Testing the Automated Approval Decision"},{"location":"guided_exercises/tools/deploying-project/#extra-lab-business-central","text":"Finally, you can import this project in Business Central. In order to do so, this needs to be a git-based project and Business Central needs to have access to the git repository where the project is stored. The following steps consider a local environment scenario. Access your application folder in the terminal. Initialize the git repository and do the first commit git init git add -A git commit -m \"first commit\" With this you can already import the project in Business Central. Open Business Central and select the import the project option. In the pop-up, in the Repository URL field, you should insert the git repository. If it is on your local machine you can inform something like: /$PROJECT_DIR/tooling-labs/mybusinessapp . Confirm the operation. You should see the project. Select it and click the Ok button. Feel free to explore the project and validate the test scenario and deployment through Business Central.","title":"Extra Lab: Business Central"},{"location":"guided_exercises/tools/getting-started/","text":"Business Automation projects in VSCode To start working with business automation projects in VSCode, you'll need to install the VSCode Extension that allows you to work with BPMN, DMN and Test Scenarios through graphical editors. Installing the VSCode extension To install the extension in VSCode, open the extensions menu, and search for Business Automation. You should find the Red Hat Business Automation Bundle. Click on install. If this is the first time you are using VSCode, it would be interesting to also install the code command in path, so that you can open projects directly from the terminal. To do so, press cmd+shift+p (or ctrl+shift+p ) to launch VSCode Quick Open menu. And next, search for Instal code command in PATH : Create new a project Let's create a new project using the maven archetype. This project should contain the structure and files that Business Central expects, so this project should be editable and authored in both VScode and Business Central. Now we will use the terminal. You can either use your terminal or use the built-in terminal In VScode. To use the terminal in VSCode you can press cmd+shift+p (or ctrl+shift+p ) to launch VSCode Quick Open menu. And next, open a new intergrated terminal : Next, in the terminal navigate to the directory where you would like to create the new project. Let's call it $PROJECT_DIR from now on. Create a new folder named tooling-labs . $ cd $PROJECT_DIR $ mkdir tooling-labs $ cd tooling-labs Now, use the maven archetype to create a new project in the tooling-labs directory: mvn archetype:generate \\ -DarchetypeGroupId=org.kie \\ -DarchetypeArtifactId=kie-kjar-archetype \\ -DarchetypeVersion=7.67.2.Final-redhat-00017 TIP: If you need to create a case project, you can use the parameter -DcaseProject=true . Maven will download the libraries, and once it finishes, it will confirm if you want to create the project using the default GAV (group:artifact:version). Type \"Y\" and press enter. You should get a new project named mybusinessapp . If you are in VSCode built-in terminal, you can open the project with: $ code -r mybusinessapp/ In VSCode, navigate through the project structure and confirm that it has a kie-deployment-descriptor.xml and a kmodule.xml . These are the files that Business Central needs to understand that this is a business project that should be packaged in a kjar. These files are also needed by KIE Server. Next Steps Now, let's author a DMN file, test it and deploy it to KIE Server.","title":"Getting Started"},{"location":"guided_exercises/tools/getting-started/#business-automation-projects-in-vscode","text":"To start working with business automation projects in VSCode, you'll need to install the VSCode Extension that allows you to work with BPMN, DMN and Test Scenarios through graphical editors.","title":"Business Automation projects in VSCode"},{"location":"guided_exercises/tools/getting-started/#installing-the-vscode-extension","text":"To install the extension in VSCode, open the extensions menu, and search for Business Automation. You should find the Red Hat Business Automation Bundle. Click on install. If this is the first time you are using VSCode, it would be interesting to also install the code command in path, so that you can open projects directly from the terminal. To do so, press cmd+shift+p (or ctrl+shift+p ) to launch VSCode Quick Open menu. And next, search for Instal code command in PATH :","title":"Installing the VSCode extension"},{"location":"guided_exercises/tools/getting-started/#create-new-a-project","text":"Let's create a new project using the maven archetype. This project should contain the structure and files that Business Central expects, so this project should be editable and authored in both VScode and Business Central. Now we will use the terminal. You can either use your terminal or use the built-in terminal In VScode. To use the terminal in VSCode you can press cmd+shift+p (or ctrl+shift+p ) to launch VSCode Quick Open menu. And next, open a new intergrated terminal : Next, in the terminal navigate to the directory where you would like to create the new project. Let's call it $PROJECT_DIR from now on. Create a new folder named tooling-labs . $ cd $PROJECT_DIR $ mkdir tooling-labs $ cd tooling-labs Now, use the maven archetype to create a new project in the tooling-labs directory: mvn archetype:generate \\ -DarchetypeGroupId=org.kie \\ -DarchetypeArtifactId=kie-kjar-archetype \\ -DarchetypeVersion=7.67.2.Final-redhat-00017 TIP: If you need to create a case project, you can use the parameter -DcaseProject=true . Maven will download the libraries, and once it finishes, it will confirm if you want to create the project using the default GAV (group:artifact:version). Type \"Y\" and press enter. You should get a new project named mybusinessapp . If you are in VSCode built-in terminal, you can open the project with: $ code -r mybusinessapp/ In VSCode, navigate through the project structure and confirm that it has a kie-deployment-descriptor.xml and a kmodule.xml . These are the files that Business Central needs to understand that this is a business project that should be packaged in a kjar. These files are also needed by KIE Server.","title":"Create new a project"},{"location":"guided_exercises/tools/getting-started/#next-steps","text":"Now, let's author a DMN file, test it and deploy it to KIE Server.","title":"Next Steps"},{"location":"guided_exercises/tools/introduction/","text":"Introduction This is a series of guided exercises that will allow you to experiment the authoring tools in VSCode and deployment in BAMOE engine, KIE Server. Tooling Set In Red Hat PAM and DM you can author decisions using: Business Central (in BAMOE) or Decision Central (in RHDM) A more business friendly UI; Business Automation VSCode Extension A developer IDE ( Visual Studio Code ) extension that allows the visualization and editing of BPMN, DMN and Test Scenarios inside VSCode. There is also a set of community tooling that's also available for use. All the tools below are backed by IBM and Red Hat: DMN FEEL Handbook A handbook for the FEEL expression language from the DMN specification, as implemented by the Drools DMN open source engine. Learn DMN in 15 minutes A guided tour in a website through the elements of DMN GitHub Chrome Extension A browser extension that allows you to visualize and edit BPMN, DMN and Test Scenario files directly in GitHub. Online Editors BPMN.new - A free online editor for business processes; DMN.new - A free online editor for decision models; PMML.new - A free online editor for scorecards; Business Modeler Hub Allows for the download of the: VSCode extension, GitHub Chrome Extension, and Desktop App","title":"Introduction"},{"location":"guided_exercises/tools/introduction/#introduction","text":"This is a series of guided exercises that will allow you to experiment the authoring tools in VSCode and deployment in BAMOE engine, KIE Server.","title":"Introduction"},{"location":"guided_exercises/tools/introduction/#tooling-set","text":"In Red Hat PAM and DM you can author decisions using: Business Central (in BAMOE) or Decision Central (in RHDM) A more business friendly UI; Business Automation VSCode Extension A developer IDE ( Visual Studio Code ) extension that allows the visualization and editing of BPMN, DMN and Test Scenarios inside VSCode. There is also a set of community tooling that's also available for use. All the tools below are backed by IBM and Red Hat: DMN FEEL Handbook A handbook for the FEEL expression language from the DMN specification, as implemented by the Drools DMN open source engine. Learn DMN in 15 minutes A guided tour in a website through the elements of DMN GitHub Chrome Extension A browser extension that allows you to visualize and edit BPMN, DMN and Test Scenario files directly in GitHub. Online Editors BPMN.new - A free online editor for business processes; DMN.new - A free online editor for decision models; PMML.new - A free online editor for scorecards; Business Modeler Hub Allows for the download of the: VSCode extension, GitHub Chrome Extension, and Desktop App","title":"Tooling Set"}]}